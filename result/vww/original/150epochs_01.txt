Number of dataloader workers: 32
Batch size: 256

loading annotations into memory...
Done (t=1.73s)
creating index...
index created!
Len of trainloader: 451

loading annotations into memory...
Done (t=0.18s)
creating index...
index created!
Len of testloader: 32

use 4 GPUs!

------Epoch: 0------
[batch_idx--0] train_loss: 0.002710737055167556, acc: 0.4921875, lr: 0.05
[batch_idx--50] train_loss: 0.0026589930468403243, acc: 0.578890931372549, lr: 0.04999993260712715
[batch_idx--100] train_loss: 0.0026452065546913902, acc: 0.5843904702970297, lr: 0.04999973042887193
[batch_idx--150] train_loss: 0.0026283847994559646, acc: 0.5940604304635762, lr: 0.049999393466324364
[batch_idx--200] train_loss: 0.0025963238359368707, acc: 0.6045942164179104, lr: 0.04999892172130118
[batch_idx--250] train_loss: 0.0025688753105269605, acc: 0.6153822211155379, lr: 0.04999831519634575
[batch_idx--300] train_loss: 0.0025509410449597626, acc: 0.6211456602990033, lr: 0.0499975738947281
[batch_idx--350] train_loss: 0.002534291775966132, acc: 0.6266359508547008, lr: 0.04999669782044491
[batch_idx--400] train_loss: 0.002518818886750877, acc: 0.6312149314214464, lr: 0.049995686978219496
[batch_idx--450] train_loss: 0.0025144832044323617, acc: 0.6359044676641095, lr: 0.04999454137350172
train_loss:  0.0025144832044323617  acc:  0.6359044676641095
->>lr:0.049995
test_loss:  0.004073883651696004  test_acc:  0.5284774785953592
best acc:  0
Saving..
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(

------Epoch: 1------
[batch_idx--0] train_loss: 0.0025928253307938576, acc: 0.66015625, lr: 0.04999451708687114
[batch_idx--50] train_loss: 0.002441341107638151, acc: 0.6623774509803921, lr: 0.04999323403077913
[batch_idx--100] train_loss: 0.0023823997171798553, acc: 0.6759746287128713, lr: 0.04999181622541965
[batch_idx--150] train_loss: 0.002371492755556146, acc: 0.6789890314569537, lr: 0.0499902636784367
[batch_idx--200] train_loss: 0.0023453376679889748, acc: 0.6849541355721394, lr: 0.04998857639820074
[batch_idx--250] train_loss: 0.002330016358699872, acc: 0.6884026394422311, lr: 0.04998675439380862
[batch_idx--300] train_loss: 0.002307713129619551, acc: 0.6925093438538206, lr: 0.04998479767508354
[batch_idx--350] train_loss: 0.0022911015056216945, acc: 0.6958800747863247, lr: 0.04998270625257502
[batch_idx--400] train_loss: 0.002274291987874617, acc: 0.698936253117207, lr: 0.04998048013755882
[batch_idx--450] train_loss: 0.002262561454488075, acc: 0.7018953726524803, lr: 0.049978119342036866
train_loss:  0.002262561454488075  acc:  0.7018953726524803
->>lr:0.049978
test_loss:  0.00242219918388663  test_acc:  0.6892914753691525
best acc:  52.84774785953592
Saving..

------Epoch: 2------
[batch_idx--0] train_loss: 0.00217228545807302, acc: 0.73046875, lr: 0.04997807075247146
[batch_idx--50] train_loss: 0.0021815109066665173, acc: 0.7218137254901961, lr: 0.04997557259595109
[batch_idx--100] train_loss: 0.0021611866886994922, acc: 0.724319306930693, lr: 0.04997293978538365
[batch_idx--150] train_loss: 0.0021749154559725166, acc: 0.7215180049668874, lr: 0.04997017233496374
[batch_idx--200] train_loss: 0.002159462847274297, acc: 0.7237834266169154, lr: 0.04996727025961189
[batch_idx--250] train_loss: 0.0021460450145850086, acc: 0.7257376743027888, lr: 0.04996423357497442
[batch_idx--300] train_loss: 0.0021358781387453953, acc: 0.727782392026578, lr: 0.04996106229742341
[batch_idx--350] train_loss: 0.002127044466708885, acc: 0.7290331196581197, lr: 0.04995775644405658
[batch_idx--400] train_loss: 0.0021154551698299045, acc: 0.7304297849127181, lr: 0.049954316032697205
[batch_idx--450] train_loss: 0.0021129776823420104, acc: 0.7308640261047662, lr: 0.049950741081894026
train_loss:  0.0021129776823420104  acc:  0.7308640261047662
->>lr:0.049951
test_loss:  0.003049878857244938  test_acc:  0.6807296190594366
best acc:  68.92914753691525

------Epoch: 3------
[batch_idx--0] train_loss: 0.0018735914491117, acc: 0.765625, lr: 0.04995066821070679
[batch_idx--50] train_loss: 0.002005370331508126, acc: 0.7452512254901961, lr: 0.04994695604953209
[batch_idx--100] train_loss: 0.0019986187476701664, acc: 0.7476021039603961, lr: 0.0499431093885944
[batch_idx--150] train_loss: 0.001993368372833048, acc: 0.7491980546357616, lr: 0.04993912824863275
[batch_idx--200] train_loss: 0.0019827188827810966, acc: 0.7515741604477612, lr: 0.049935012651111166
[batch_idx--250] train_loss: 0.00198326265231606, acc: 0.7499533117529881, lr: 0.0499307626182186
[batch_idx--300] train_loss: 0.0019805043490226087, acc: 0.7502984842192691, lr: 0.0499263781728688
[batch_idx--350] train_loss: 0.001975892619773788, acc: 0.7503783831908832, lr: 0.04992185933870021
[batch_idx--400] train_loss: 0.001975117656843565, acc: 0.7508085255610972, lr: 0.049917206140075804
[batch_idx--450] train_loss: 0.0019768384372335136, acc: 0.751831152150519, lr: 0.04991241860208297
train_loss:  0.0019768384372335136  acc:  0.751831152150519
->>lr:0.049912
test_loss:  0.002496675257795812  test_acc:  0.6616205484551433
best acc:  68.92914753691525

------Epoch: 4------
[batch_idx--0] train_loss: 0.001948893186636269, acc: 0.76953125, lr: 0.049912321481237615
[batch_idx--50] train_loss: 0.001938646419576424, acc: 0.7585018382352942, lr: 0.04990739694368515
[batch_idx--100] train_loss: 0.001944175917922639, acc: 0.7590887995049505, lr: 0.04990233811964985
[batch_idx--150] train_loss: 0.0019477067734207341, acc: 0.7567518625827815, lr: 0.049897145036406014
[batch_idx--200] train_loss: 0.0019351562985510968, acc: 0.7590562810945274, lr: 0.04989181772195179
[batch_idx--250] train_loss: 0.0019302269424389736, acc: 0.759788969123506, lr: 0.04988635620500901
[batch_idx--300] train_loss: 0.0019281746593746434, acc: 0.7598499792358804, lr: 0.049880760515023076
[batch_idx--350] train_loss: 0.0019228955361981508, acc: 0.7603944088319088, lr: 0.04987503068216274
[batch_idx--400] train_loss: 0.0019241911141396944, acc: 0.7603549719451371, lr: 0.04986916673732
[batch_idx--450] train_loss: 0.001925922741952275, acc: 0.7606744888395182, lr: 0.049863168712109905
train_loss:  0.001925922741952275  acc:  0.7606744888395182
->>lr:0.049863
test_loss:  0.002267529583107465  test_acc:  0.7306117384290854
best acc:  68.92914753691525
Saving..

------Epoch: 5------
[batch_idx--0] train_loss: 0.0018799112876877189, acc: 0.74609375, lr: 0.049863047384206834
[batch_idx--50] train_loss: 0.0019129588332611556, acc: 0.7611060049019608, lr: 0.04985691263034154
[batch_idx--100] train_loss: 0.0018917683207678913, acc: 0.7675974628712872, lr: 0.049850643862176046
[batch_idx--150] train_loss: 0.0018892889797675195, acc: 0.7685482201986755, lr: 0.04984424111350797
[batch_idx--200] train_loss: 0.0018754618344318807, acc: 0.7702891791044776, lr: 0.0498377044188573
[batch_idx--250] train_loss: 0.0018752565383874029, acc: 0.7699981324701195, lr: 0.04983103381346615
[batch_idx--300] train_loss: 0.0018782076136552516, acc: 0.7699724875415282, lr: 0.049824229333298636
[batch_idx--350] train_loss: 0.0018795256640900065, acc: 0.7693309294871795, lr: 0.049817291015040614
[batch_idx--400] train_loss: 0.0018801206207260527, acc: 0.768946773690773, lr: 0.04981021889609956
[batch_idx--450] train_loss: 0.0018789076595514631, acc: 0.7695698962057833, lr: 0.0498030130146043
train_loss:  0.0018789076595514631  acc:  0.7695698962057833
->>lr:0.049803
test_loss:  0.0019510026893125808  test_acc:  0.7646109939198412
best acc:  73.06117384290854
Saving..

------Epoch: 6------
[batch_idx--0] train_loss: 0.0017508957535028458, acc: 0.78515625, lr: 0.04980286753286195
[batch_idx--50] train_loss: 0.0018535655256652949, acc: 0.7706801470588235, lr: 0.04979552525358963
[batch_idx--100] train_loss: 0.0018526738587914423, acc: 0.7711169554455446, lr: 0.04978804929098283
[batch_idx--150] train_loss: 0.0018549475892884841, acc: 0.7710057947019867, lr: 0.049780439685347705
[batch_idx--200] train_loss: 0.0018532633322721986, acc: 0.771610696517413, lr: 0.0497726964777109
[batch_idx--250] train_loss: 0.0018488564612009967, acc: 0.772457046812749, lr: 0.049764819709819365
[batch_idx--300] train_loss: 0.0018484631918395476, acc: 0.7735672757475083, lr: 0.04975680942414015
[batch_idx--350] train_loss: 0.0018476677714897442, acc: 0.7734152421652422, lr: 0.04974866566386016
[batch_idx--400] train_loss: 0.001843659982359283, acc: 0.774021976309227, lr: 0.049740388472885894
[batch_idx--450] train_loss: 0.001847489779875793, acc: 0.7742041864824522, lr: 0.04973197789584324
train_loss:  0.001847489779875793  acc:  0.7742041864824522
->>lr:0.049732
test_loss:  0.0027012078752291675  test_acc:  0.6896637299913141
best acc:  76.46109939198412

------Epoch: 7------
[batch_idx--0] train_loss: 0.001692941295914352, acc: 0.7890625, lr: 0.049731808324074714
[batch_idx--50] train_loss: 0.0018231459304361658, acc: 0.7729013480392157, lr: 0.04972326173996171
[batch_idx--100] train_loss: 0.0018290439078879387, acc: 0.7736695544554455, lr: 0.049714581862117906
[batch_idx--150] train_loss: 0.0018136148558411476, acc: 0.7756622516556292, lr: 0.049705768737340256
[batch_idx--200] train_loss: 0.001807581119016925, acc: 0.7778296019900498, lr: 0.04969682241314409
[batch_idx--250] train_loss: 0.0018062352938429114, acc: 0.778324203187251, lr: 0.0496877429377629
[batch_idx--300] train_loss: 0.0018095493102384317, acc: 0.7781483596345515, lr: 0.04967853036014805
[batch_idx--350] train_loss: 0.0018150314825992936, acc: 0.7768429487179487, lr: 0.04966918472996849
[batch_idx--400] train_loss: 0.0018197157117712341, acc: 0.7768761689526185, lr: 0.04965970609761053
[batch_idx--450] train_loss: 0.001819064639389492, acc: 0.777961953691811, lr: 0.04965009451417756
train_loss:  0.001819064639389492  acc:  0.777961953691811
->>lr:0.049650
test_loss:  0.001922291356024131  test_acc:  0.7720560863630723
best acc:  76.46109939198412
Saving..

------Epoch: 8------
[batch_idx--0] train_loss: 0.0017902094405144453, acc: 0.80859375, lr: 0.04964990092676263
[batch_idx--50] train_loss: 0.0017776889247162378, acc: 0.7856158088235294, lr: 0.0496401537866232
[batch_idx--100] train_loss: 0.001788143588386771, acc: 0.7840733292079208, lr: 0.04963027380082368
[batch_idx--150] train_loss: 0.001786462991923982, acc: 0.7828797599337748, lr: 0.04962026102263131
[batch_idx--200] train_loss: 0.0017871875391198108, acc: 0.7818524564676617, lr: 0.04961011550602926
[batch_idx--250] train_loss: 0.0017821221089220616, acc: 0.7829930278884463, lr: 0.04959983730571641
[batch_idx--300] train_loss: 0.0017831090905787144, acc: 0.7832485465116279, lr: 0.04958942647710693
[batch_idx--350] train_loss: 0.0017859455034869094, acc: 0.7831196581196581, lr: 0.04957888307633007
[batch_idx--400] train_loss: 0.0017824045410310093, acc: 0.7838216957605985, lr: 0.04956820716022985
[batch_idx--450] train_loss: 0.0017866023226506336, acc: 0.7832384489880931, lr: 0.049557398786364705
train_loss:  0.0017866023226506336  acc:  0.7832384489880931
->>lr:0.049557
test_loss:  0.001757871821376641  test_acc:  0.79935475865492
best acc:  77.20560863630723
Saving..

------Epoch: 9------
[batch_idx--0] train_loss: 0.0019169264705851674, acc: 0.78125, lr: 0.049557181268217225
[batch_idx--50] train_loss: 0.0017774926987020117, acc: 0.7853860294117647, lr: 0.04954623784746919
[batch_idx--100] train_loss: 0.0017869891783240997, acc: 0.7838799504950495, lr: 0.04953516208740226
[batch_idx--150] train_loss: 0.001780879491051992, acc: 0.7846906043046358, lr: 0.049523954047730606
[batch_idx--200] train_loss: 0.0017734862335804682, acc: 0.7861279539800995, lr: 0.04951261378888158
[batch_idx--250] train_loss: 0.0017730040318959678, acc: 0.7866502739043825, lr: 0.049501141371995405
[batch_idx--300] train_loss: 0.001775400910721442, acc: 0.786155523255814, lr: 0.0494895368589248
[batch_idx--350] train_loss: 0.0017749151593721617, acc: 0.7864472044159544, lr: 0.0494778003122347
[batch_idx--400] train_loss: 0.0017774742104791272, acc: 0.7855653834164589, lr: 0.049465931795201847
[batch_idx--450] train_loss: 0.0017789185425278198, acc: 0.7861977991460409, lr: 0.049453931371814544
train_loss:  0.0017789185425278198  acc:  0.7861977991460409
->>lr:0.049454
test_loss:  0.0018626968401184536  test_acc:  0.7804938577987344
best acc:  79.935475865492

------Epoch: 10------
[batch_idx--0] train_loss: 0.0016253456706181169, acc: 0.8046875, lr: 0.049453690018345146
[batch_idx--50] train_loss: 0.0017594926963176798, acc: 0.7879136029411765, lr: 0.04944155511713453
[batch_idx--100] train_loss: 0.0017379755511566406, acc: 0.7930074257425742, lr: 0.04942928844099462
[batch_idx--150] train_loss: 0.0017524792192910878, acc: 0.7899161837748344, lr: 0.0494168900560603
[batch_idx--200] train_loss: 0.0017506727225967307, acc: 0.7891402363184079, lr: 0.04940436002917662
[batch_idx--250] train_loss: 0.001761027799169737, acc: 0.7874284113545816, lr: 0.04939169842789834
[batch_idx--300] train_loss: 0.001759275248308242, acc: 0.7872326619601329, lr: 0.04937890532048959
[batch_idx--350] train_loss: 0.001752437195512453, acc: 0.7878828347578347, lr: 0.04936598077592351
[batch_idx--400] train_loss: 0.00174957821453385, acc: 0.788692331670823, lr: 0.049352924863881875
[batch_idx--450] train_loss: 0.001751089144589797, acc: 0.7893567535668414, lr: 0.04933973765475472
train_loss:  0.001751089144589797  acc:  0.7893567535668414
->>lr:0.049340
test_loss:  0.0018740882240730415  test_acc:  0.7791289241841419
best acc:  79.935475865492

------Epoch: 11------
[batch_idx--0] train_loss: 0.0015949922380968928, acc: 0.80078125, lr: 0.04933947257182901
[batch_idx--50] train_loss: 0.0017347796340746915, acc: 0.7883731617647058, lr: 0.04932615151292455
[batch_idx--100] train_loss: 0.0017332198999656162, acc: 0.789371905940594, lr: 0.04931269930128121
[batch_idx--150] train_loss: 0.0017276616013047613, acc: 0.7918563741721855, lr: 0.04929911600942565
[batch_idx--200] train_loss: 0.0017336692923995021, acc: 0.791064210199005, lr: 0.04928540171059123
[batch_idx--250] train_loss: 0.001733768509192563, acc: 0.7907121513944223, lr: 0.04927155647871764
[batch_idx--300] train_loss: 0.0017284880619669377, acc: 0.7912427325581395, lr: 0.04925758038845046
[batch_idx--350] train_loss: 0.0017309051684960936, acc: 0.7908208689458689, lr: 0.04924347351514081
[batch_idx--400] train_loss: 0.0017264450283632834, acc: 0.7911958385286783, lr: 0.049229235934844906
[batch_idx--450] train_loss: 0.0017297013152947954, acc: 0.7914742944423231, lr: 0.04921486772432365
train_loss:  0.0017297013152947954  acc:  0.7914742944423231
->>lr:0.049215
test_loss:  0.0017577721154309633  test_acc:  0.7878148653679117
best acc:  79.935475865492

------Epoch: 12------
[batch_idx--0] train_loss: 0.002182117197662592, acc: 0.7265625, lr: 0.04921457902821578
[batch_idx--50] train_loss: 0.0017197607815557835, acc: 0.7945006127450981, lr: 0.04920007765467417
[batch_idx--100] train_loss: 0.0017210838343431748, acc: 0.7941290222772277, lr: 0.049185445808112044
[batch_idx--150] train_loss: 0.001721022373053017, acc: 0.7935637417218543, lr: 0.049170683567415975
[batch_idx--200] train_loss: 0.00172568178580453, acc: 0.7922302549751243, lr: 0.04915579101217554
[batch_idx--250] train_loss: 0.0017209182450718494, acc: 0.7924551792828686, lr: 0.04914076822268292
[batch_idx--300] train_loss: 0.0017211518913315628, acc: 0.7926962209302325, lr: 0.04912561527993242
[batch_idx--350] train_loss: 0.0017210214420028051, acc: 0.7929576210826211, lr: 0.049110332265620074
[batch_idx--400] train_loss: 0.0017193004072636848, acc: 0.7930466801745636, lr: 0.049094919262143176
[batch_idx--450] train_loss: 0.0017203866817828131, acc: 0.7939389731662443, lr: 0.049079376352599846
train_loss:  0.0017203866817828131  acc:  0.7939389731662443
->>lr:0.049079
test_loss:  0.0017563543066461916  test_acc:  0.7894279687306117
best acc:  79.935475865492

------Epoch: 13------
[batch_idx--0] train_loss: 0.0015412467764690518, acc: 0.83203125, lr: 0.049079064169941455
[batch_idx--50] train_loss: 0.001682103565875806, acc: 0.7965686274509803, lr: 0.04906338884254441
[batch_idx--100] train_loss: 0.0016789791418589874, acc: 0.799737004950495, lr: 0.04904758377907498
[batch_idx--150] train_loss: 0.0016881104224108605, acc: 0.7979615066225165, lr: 0.04903164906474507
[batch_idx--200] train_loss: 0.0016857319574024696, acc: 0.7985657649253731, lr: 0.04901558478546556
[batch_idx--250] train_loss: 0.0016920965608298423, acc: 0.7980266434262948, lr: 0.04899939102784589
[batch_idx--300] train_loss: 0.0016936222532225704, acc: 0.7979002284053156, lr: 0.04898306787919357
[batch_idx--350] train_loss: 0.0016958028667784187, acc: 0.7975872507122507, lr: 0.04896661542751371
[batch_idx--400] train_loss: 0.0016953124736721378, acc: 0.7973718048628429, lr: 0.04895003376150854
[batch_idx--450] train_loss: 0.0017014276049429852, acc: 0.7973062102961086, lr: 0.04893332297057697
train_loss:  0.0017014276049429852  acc:  0.7973062102961086
->>lr:0.048933
test_loss:  0.002073276548087412  test_acc:  0.7539396947512098
best acc:  79.935475865492

------Epoch: 14------
[batch_idx--0] train_loss: 0.0018343685660511255, acc: 0.8046875, lr: 0.048932987438301684
[batch_idx--50] train_loss: 0.0016904216483417972, acc: 0.7971813725490197, lr: 0.04891614503276587
[batch_idx--100] train_loss: 0.0016896860595644995, acc: 0.7981512995049505, lr: 0.04889917368501234
[batch_idx--150] train_loss: 0.0017038225304236672, acc: 0.7959437086092715, lr: 0.04888207348654094
[batch_idx--200] train_loss: 0.0016989149031604626, acc: 0.796836131840796, lr: 0.048864844529546175
[batch_idx--250] train_loss: 0.0016941235134511651, acc: 0.7979177041832669, lr: 0.04884748690691675
[batch_idx--300] train_loss: 0.001693239246426998, acc: 0.7979261835548173, lr: 0.048830000712235096
[batch_idx--350] train_loss: 0.001686349931037706, acc: 0.7995904558404558, lr: 0.048812386039776785
[batch_idx--400] train_loss: 0.0016824944706011256, acc: 0.7998363466334164, lr: 0.0487946429845101
[batch_idx--450] train_loss: 0.0016859377361504286, acc: 0.7998056028048738, lr: 0.048776771642095464
train_loss:  0.0016859377361504286  acc:  0.7998056028048738
->>lr:0.048777
test_loss:  0.0020069637699744913  test_acc:  0.7550564586176945
best acc:  79.935475865492

------Epoch: 15------
[batch_idx--0] train_loss: 0.0015516651328653097, acc: 0.8125, lr: 0.048776412907378844
[batch_idx--50] train_loss: 0.001653186689295313, acc: 0.8043811274509803, lr: 0.04875841081133997
[batch_idx--100] train_loss: 0.0016642177159353943, acc: 0.8019415222772277, lr: 0.04874028062349638
[batch_idx--150] train_loss: 0.001661792927921213, acc: 0.8026179635761589, lr: 0.0487220224415957
[batch_idx--200] train_loss: 0.001668983870023741, acc: 0.800742381840796, lr: 0.04870363636407563
[batch_idx--250] train_loss: 0.0016663988859233153, acc: 0.801761703187251, lr: 0.04868512249006343
[batch_idx--300] train_loss: 0.0016592815804582993, acc: 0.802922549833887, lr: 0.048666480919375345
[batch_idx--350] train_loss: 0.0016614446904711913, acc: 0.8028957443019943, lr: 0.0486477117525161
[batch_idx--400] train_loss: 0.0016619016911644034, acc: 0.8028171758104738, lr: 0.04862881509067834
[batch_idx--450] train_loss: 0.0016676877798994225, acc: 0.8028170236400874, lr: 0.04860979103574209
train_loss:  0.0016676877798994225  acc:  0.8028170236400874
->>lr:0.048610
test_loss:  0.0017515973545419532  test_acc:  0.7952599578111428
best acc:  79.935475865492

------Epoch: 16------
[batch_idx--0] train_loss: 0.0016315510729327798, acc: 0.8046875, lr: 0.04860940925593703
[batch_idx--50] train_loss: 0.0016805747656297742, acc: 0.7997855392156863, lr: 0.048590255365709406
[batch_idx--100] train_loss: 0.0016874818679978056, acc: 0.7993889232673267, lr: 0.04857097429027534
[batch_idx--150] train_loss: 0.001686993691860544, acc: 0.7991514900662252, lr: 0.0485515661335874
[batch_idx--200] train_loss: 0.0016726087784598494, acc: 0.800897854477612, lr: 0.048532031000283304
[batch_idx--250] train_loss: 0.00166670208475905, acc: 0.8021974601593626, lr: 0.048512368995685354
[batch_idx--300] train_loss: 0.0016643151345405963, acc: 0.8027538413621262, lr: 0.04849258022579986
[batch_idx--350] train_loss: 0.001662531619204309, acc: 0.8031405804843305, lr: 0.04847266479731658
[batch_idx--400] train_loss: 0.001661988688827936, acc: 0.8033139806733167, lr: 0.048452622817608176
[batch_idx--450] train_loss: 0.0016622616896738748, acc: 0.8035546915680216, lr: 0.04843245439472954
train_loss:  0.0016622616896738748  acc:  0.8035546915680216
->>lr:0.048432
test_loss:  0.001683553793335955  test_acc:  0.806055341853828
best acc:  79.935475865492
Saving..

------Epoch: 17------
[batch_idx--0] train_loss: 0.0014736250741407275, acc: 0.83203125, lr: 0.04843204973729729
[batch_idx--50] train_loss: 0.001673958885173003, acc: 0.8017003676470589, lr: 0.048411752454410206
[batch_idx--100] train_loss: 0.001670974228383586, acc: 0.8016707920792079, lr: 0.0483913289487026
[batch_idx--150] train_loss: 0.0016729646664581552, acc: 0.8011692880794702, lr: 0.04837077933028638
[batch_idx--200] train_loss: 0.0016587735113192034, acc: 0.8024137126865671, lr: 0.04835010370995336
[batch_idx--250] train_loss: 0.0016546821930404916, acc: 0.8030378486055777, lr: 0.0483293021991747
[batch_idx--300] train_loss: 0.0016517573387433227, acc: 0.8037141818936877, lr: 0.04830837491010029
[batch_idx--350] train_loss: 0.0016494378403752532, acc: 0.8039752492877493, lr: 0.04828732195555814
[batch_idx--400] train_loss: 0.0016516173610282583, acc: 0.8038594918952618, lr: 0.04826614344905377
[batch_idx--450] train_loss: 0.0016560136372338655, acc: 0.8037195820460304, lr: 0.04824483950476961
train_loss:  0.0016560136372338655  acc:  0.8037195820460304
->>lr:0.048245
test_loss:  0.0017637692346571692  test_acc:  0.7904206477230425
best acc:  80.60553418538281

------Epoch: 18------
[batch_idx--0] train_loss: 0.001450582523830235, acc: 0.83203125, lr: 0.04824441214720629
[batch_idx--50] train_loss: 0.0016343091641936231, acc: 0.8059895833333334, lr: 0.04822298037471885
[batch_idx--100] train_loss: 0.0016274222548553112, acc: 0.8061571782178217, lr: 0.04820142339716232
[batch_idx--150] train_loss: 0.0016287643329813979, acc: 0.8065500827814569, lr: 0.04817974133075961
[batch_idx--200] train_loss: 0.0016247583680614754, acc: 0.8074860074626866, lr: 0.04815793429240808
[batch_idx--250] train_loss: 0.001626991261950735, acc: 0.8069129731075697, lr: 0.04813600239967885
[batch_idx--300] train_loss: 0.0016255617111556207, acc: 0.8082174003322259, lr: 0.04811394577081616
[batch_idx--350] train_loss: 0.001632060486149372, acc: 0.8070023148148148, lr: 0.04809176452473679
[batch_idx--400] train_loss: 0.0016356860165556246, acc: 0.8067526496259352, lr: 0.048069458781029376
[batch_idx--450] train_loss: 0.0016401783420719532, acc: 0.8066615753115562, lr: 0.048047028659953764
train_loss:  0.0016401783420719532  acc:  0.8066615753115562
->>lr:0.048047
test_loss:  0.001655469643801935  test_acc:  0.8132522645489515
best acc:  80.60553418538281
Saving..

------Epoch: 19------
[batch_idx--0] train_loss: 0.0014528634492307901, acc: 0.84765625, lr: 0.04804657878971252
[batch_idx--50] train_loss: 0.001633936696795418, acc: 0.8072916666666666, lr: 0.04802402192830846
[batch_idx--100] train_loss: 0.001632514067390694, acc: 0.8077815594059405, lr: 0.04800134093450581
[batch_idx--150] train_loss: 0.0016299218667165333, acc: 0.8084126655629139, lr: 0.04797853593058756
[batch_idx--200] train_loss: 0.0016346429492027235, acc: 0.8074665733830846, lr: 0.04795560703950527
[batch_idx--250] train_loss: 0.0016297815697809378, acc: 0.8073487300796812, lr: 0.047932554384878465
[batch_idx--300] train_loss: 0.0016282198276135612, acc: 0.807750207641196, lr: 0.0479093780909939
[batch_idx--350] train_loss: 0.001625043951954024, acc: 0.808059561965812, lr: 0.047886078282804945
[batch_idx--400] train_loss: 0.001623228151975966, acc: 0.8086327150872819, lr: 0.047862655085930884
[batch_idx--450] train_loss: 0.0016277537658175919, acc: 0.8091088971430556, lr: 0.04783910862665624
train_loss:  0.0016277537658175919  acc:  0.8091088971430556
->>lr:0.047839
test_loss:  0.0016652414810274386  test_acc:  0.8049385779873434
best acc:  81.32522645489514

------Epoch: 20------
[batch_idx--0] train_loss: 0.0017671009991317987, acc: 0.78515625, lr: 0.04783863644106502
[batch_idx--50] train_loss: 0.0016586191486567259, acc: 0.8038449754901961, lr: 0.047814964384929284
[batch_idx--100] train_loss: 0.0016243965468729043, acc: 0.8094446163366337, lr: 0.047791169323514016
[batch_idx--150] train_loss: 0.0016224234124386546, acc: 0.8099648178807947, lr: 0.047767251385108636
[batch_idx--200] train_loss: 0.0016232070383333152, acc: 0.8094877176616916, lr: 0.04774321069866503
[batch_idx--250] train_loss: 0.001627779699791505, acc: 0.808624875498008, lr: 0.04771904739379687
[batch_idx--300] train_loss: 0.0016286313267690794, acc: 0.8082563330564784, lr: 0.0476947616007789
[batch_idx--350] train_loss: 0.0016271675282829253, acc: 0.8087829415954416, lr: 0.0476703534505463
[batch_idx--400] train_loss: 0.001624087199506041, acc: 0.8095581359102244, lr: 0.047645823074693894
[batch_idx--450] train_loss: 0.0016244790094664425, acc: 0.8095948901308709, lr: 0.047621170605475466
train_loss:  0.0016244790094664425  acc:  0.8095948901308709
->>lr:0.047621
test_loss:  0.0016063768169338227  test_acc:  0.8166025561484055
best acc:  81.32522645489514
Saving..

------Epoch: 21------
[batch_idx--0] train_loss: 0.0014904403360560536, acc: 0.8125, lr: 0.04762067631165049
[batch_idx--50] train_loss: 0.001625922784738827, acc: 0.8092064950980392, lr: 0.047595899444129244
[batch_idx--100] train_loss: 0.0016131230853590193, acc: 0.8109142945544554, lr: 0.04757100075240174
[batch_idx--150] train_loss: 0.0016239374337890587, acc: 0.8097578642384106, lr: 0.04754598037070751
[batch_idx--200] train_loss: 0.0016183450184438137, acc: 0.8095071517412935, lr: 0.047520838433942204
[batch_idx--250] train_loss: 0.0016145612780130896, acc: 0.8102745268924303, lr: 0.04749557507765681
[batch_idx--300] train_loss: 0.0016123032587212185, acc: 0.8108648255813954, lr: 0.047470190438056926
[batch_idx--350] train_loss: 0.001612484211001301, acc: 0.8108863069800569, lr: 0.047444684652002064
[batch_idx--400] train_loss: 0.001610477951394761, acc: 0.8113310473815462, lr: 0.04741905785700488
[batch_idx--450] train_loss: 0.0016117849700729896, acc: 0.8115388620821328, lr: 0.04739331019123044
train_loss:  0.0016117849700729896  acc:  0.8115388620821328
->>lr:0.047393
test_loss:  0.0015795308154042593  test_acc:  0.8252884973321752
best acc:  81.66025561484055
Saving..

------Epoch: 22------
[batch_idx--0] train_loss: 0.0017261095345020294, acc: 0.796875, lr: 0.047392794005985325
[batch_idx--50] train_loss: 0.0016274290734573321, acc: 0.8082873774509803, lr: 0.04736692319503155
[batch_idx--100] train_loss: 0.0016018039302550034, acc: 0.8118811881188119, lr: 0.047340931794380904
[batch_idx--150] train_loss: 0.0016011371499309872, acc: 0.8127845612582781, lr: 0.04731481994416418
[batch_idx--200] train_loss: 0.0015939981694013548, acc: 0.8140158582089553, lr: 0.04728858778516159
[batch_idx--250] train_loss: 0.0015949179989794098, acc: 0.813527141434263, lr: 0.04726223545880198
[batch_idx--300] train_loss: 0.0015934889678196654, acc: 0.8134603405315615, lr: 0.04723576310716207
[batch_idx--350] train_loss: 0.001587719533478369, acc: 0.8142361111111112, lr: 0.047209170872965694
[batch_idx--400] train_loss: 0.0015909312240096586, acc: 0.8136981764339152, lr: 0.047182458899583
[batch_idx--450] train_loss: 0.0015906196014016333, acc: 0.8141684312840629, lr: 0.04715562733102973
train_loss:  0.0015906196014016333  acc:  0.8141684312840629
->>lr:0.047156
test_loss:  0.0021845732674230073  test_acc:  0.7530711006328329
best acc:  82.52884973321753

------Epoch: 23------
[batch_idx--0] train_loss: 0.0016610502498224378, acc: 0.80859375, lr: 0.04715508948078037
[batch_idx--50] train_loss: 0.0016252223525525017, acc: 0.8107383578431373, lr: 0.047128136074186804
[batch_idx--100] train_loss: 0.0016160550997546405, acc: 0.8109916460396039, lr: 0.04710106336530035
[batch_idx--150] train_loss: 0.0016006433277002332, acc: 0.8131467301324503, lr: 0.04707387150008163
[batch_idx--200] train_loss: 0.0015974244037967416, acc: 0.8128886815920398, lr: 0.04704656062513368
[batch_idx--250] train_loss: 0.0015964873679598192, acc: 0.8122976842629482, lr: 0.04701913088770116
[batch_idx--300] train_loss: 0.0015910886803586221, acc: 0.8129412375415282, lr: 0.04699158243566958
[batch_idx--350] train_loss: 0.0015817215706638334, acc: 0.8144920762108262, lr: 0.04696391541756448
[batch_idx--400] train_loss: 0.001582011378409708, acc: 0.8146625623441397, lr: 0.046936129982550645
[batch_idx--450] train_loss: 0.0015854267422557933, acc: 0.8141771097302739, lr: 0.0469082262804313
train_loss:  0.0015854267422557933  acc:  0.8141771097302739
->>lr:0.046908
test_loss:  0.0020166075818369147  test_acc:  0.7556768829879638
best acc:  82.52884973321753

------Epoch: 24------
[batch_idx--0] train_loss: 0.0014701078180223703, acc: 0.81640625, lr: 0.04690766700109659
[batch_idx--50] train_loss: 0.0015925456559760314, acc: 0.8143382352941176, lr: 0.04687964282151817
[batch_idx--100] train_loss: 0.001582015653850861, acc: 0.8145111386138614, lr: 0.046851500679380806
[batch_idx--150] train_loss: 0.0015776903747846158, acc: 0.8149834437086093, lr: 0.046823240726410885
[batch_idx--200] train_loss: 0.0015746551792864775, acc: 0.816289645522388, lr: 0.046794863114969956
[batch_idx--250] train_loss: 0.0015759774552383627, acc: 0.8162039342629482, lr: 0.04676636799805392
[batch_idx--300] train_loss: 0.001567822867506292, acc: 0.8170681063122923, lr: 0.046737755529292206
[batch_idx--350] train_loss: 0.001569217523000073, acc: 0.8170628561253561, lr: 0.046709025862946923
[batch_idx--400] train_loss: 0.0015703652886107117, acc: 0.8173706359102244, lr: 0.046680179153912066
[batch_idx--450] train_loss: 0.001570777581231921, acc: 0.8180563751865866, lr: 0.04665121555771262
train_loss:  0.001570777581231921  acc:  0.8180563751865866
->>lr:0.046651
test_loss:  0.0014969958407305831  test_acc:  0.8333540141456757
best acc:  82.52884973321753
Saving..

------Epoch: 25------
[batch_idx--0] train_loss: 0.001654467429034412, acc: 0.80078125, lr: 0.046650635094610975
[batch_idx--50] train_loss: 0.0015454067798404425, acc: 0.8206188725490197, lr: 0.04662155243437907
[batch_idx--100] train_loss: 0.0015462260695616944, acc: 0.8201577970297029, lr: 0.04659235320306443
[batch_idx--150] train_loss: 0.0015540746686708749, acc: 0.8198209850993378, lr: 0.046563037558092685
[batch_idx--200] train_loss: 0.0015584506402579854, acc: 0.8190687189054726, lr: 0.04653360565751704
[batch_idx--250] train_loss: 0.0015616976601290572, acc: 0.8182270916334662, lr: 0.04650405766001754
[batch_idx--300] train_loss: 0.0015643851686906428, acc: 0.8174574335548173, lr: 0.04647439372490014
[batch_idx--350] train_loss: 0.0015638357942440995, acc: 0.8172631766381766, lr: 0.046444614012095875
[batch_idx--400] train_loss: 0.0015628414438744807, acc: 0.817838216957606, lr: 0.046414718682159954
[batch_idx--450] train_loss: 0.0015636774505273486, acc: 0.8179348769396327, lr: 0.04638470789627097
train_loss:  0.0015636774505273486  acc:  0.8179348769396327
->>lr:0.046385
test_loss:  0.0017859009665995558  test_acc:  0.7922819208338504
best acc:  83.33540141456757

------Epoch: 26------
[batch_idx--0] train_loss: 0.0016041219932958484, acc: 0.8203125, lr: 0.04638410650401267
[batch_idx--50] train_loss: 0.0015273378892600828, acc: 0.8219209558823529, lr: 0.04635397811974326
[batch_idx--100] train_loss: 0.001546712890774528, acc: 0.8190362004950495, lr: 0.04632373460699926
[batch_idx--150] train_loss: 0.0015480606986320769, acc: 0.8194846854304636, lr: 0.04629337612883644
[batch_idx--200] train_loss: 0.0015610894964387019, acc: 0.8177666355721394, lr: 0.046262902848930414
[batch_idx--250] train_loss: 0.0015577296860909379, acc: 0.8179313994023905, lr: 0.04623231493157573
[batch_idx--300] train_loss: 0.0015589409191439862, acc: 0.8180803571428571, lr: 0.046201612541685
[batch_idx--350] train_loss: 0.0015540199544668155, acc: 0.8183871972934473, lr: 0.046170795844788
[batch_idx--400] train_loss: 0.0015542156337655876, acc: 0.818120713840399, lr: 0.04613986500703078
[batch_idx--450] train_loss: 0.0015581966977090975, acc: 0.8181171243100636, lr: 0.0461088201951748
train_loss:  0.0015581966977090975  acc:  0.8181171243100636
->>lr:0.046109
test_loss:  0.0015308629240198604  test_acc:  0.8252884973321752
best acc:  83.33540141456757

------Epoch: 27------
[batch_idx--0] train_loss: 0.0016698481049388647, acc: 0.8125, lr: 0.04610819813755038
[batch_idx--50] train_loss: 0.0015327898310680016, acc: 0.819546568627451, lr: 0.04607703724454855
[batch_idx--100] train_loss: 0.0015638089775670283, acc: 0.8164836014851485, lr: 0.046045762716179425
[batch_idx--150] train_loss: 0.0015560051916623549, acc: 0.8174668874172185, lr: 0.04601437472105743
[batch_idx--200] train_loss: 0.0015599355582305375, acc: 0.8167754975124378, lr: 0.04598287342840873
[batch_idx--250] train_loss: 0.0015520267819325287, acc: 0.8184449701195219, lr: 0.04595125900807036
[batch_idx--300] train_loss: 0.0015533185135449385, acc: 0.818171200166113, lr: 0.045919531630489216
[batch_idx--350] train_loss: 0.0015463661664481304, acc: 0.8191884793447294, lr: 0.045887691466721246
[batch_idx--400] train_loss: 0.0015470631208103567, acc: 0.8191435473815462, lr: 0.04585573868843045
[batch_idx--450] train_loss: 0.0015491511955038428, acc: 0.8196879230742528, lr: 0.04582367346788801
train_loss:  0.0015491511955038428  acc:  0.8196879230742528
->>lr:0.045824
test_loss:  0.0025801791537117167  test_acc:  0.7059188484923687
best acc:  83.33540141456757

------Epoch: 28------
[batch_idx--0] train_loss: 0.0014197075506672263, acc: 0.83984375, lr: 0.04582303101775249
[batch_idx--50] train_loss: 0.0015548823958299325, acc: 0.8176317401960784, lr: 0.04579085128421578
[batch_idx--100] train_loss: 0.001541508698264266, acc: 0.821511448019802, lr: 0.04575855945826332
[batch_idx--150] train_loss: 0.0015464925379893243, acc: 0.8199244619205298, lr: 0.04572615571399419
[batch_idx--200] train_loss: 0.0015469386968622678, acc: 0.8194379664179104, lr: 0.045693640226110915
[batch_idx--250] train_loss: 0.001542090522255229, acc: 0.8201568725099602, lr: 0.045661013169918455
[batch_idx--300] train_loss: 0.0015402744203942063, acc: 0.820390365448505, lr: 0.04562827472132332
[batch_idx--350] train_loss: 0.0015417138058851417, acc: 0.8203792735042735, lr: 0.045595425056832534
[batch_idx--400] train_loss: 0.0015375737557791526, acc: 0.8207411159600998, lr: 0.04556246435355277
[batch_idx--450] train_loss: 0.0015424585257160191, acc: 0.8198441351060506, lr: 0.04552939278918935
train_loss:  0.0015424585257160191  acc:  0.8198441351060506
->>lr:0.045529
test_loss:  0.001586423701692092  test_acc:  0.8168507258965132
best acc:  83.33540141456757

------Epoch: 29------
[batch_idx--0] train_loss: 0.001560463453643024, acc: 0.8125, lr: 0.045528730228342605
[batch_idx--50] train_loss: 0.0015542290022815851, acc: 0.8186274509803921, lr: 0.04549554576936571
[batch_idx--100] train_loss: 0.0015418612199955353, acc: 0.8197323638613861, lr: 0.04546225081009201
[batch_idx--150] train_loss: 0.00153360138755193, acc: 0.8213990066225165, lr: 0.04542884553002893
[batch_idx--200] train_loss: 0.0015331790220705949, acc: 0.8217117537313433, lr: 0.04539533010927871
[batch_idx--250] train_loss: 0.0015276011137061742, acc: 0.8227091633466136, lr: 0.04536170472853737
[batch_idx--300] train_loss: 0.0015301312002016115, acc: 0.8226484634551495, lr: 0.045327969569093796
[batch_idx--350] train_loss: 0.0015344831646762344, acc: 0.8218928062678063, lr: 0.04529412481282874
[batch_idx--400] train_loss: 0.0015326340490037069, acc: 0.8219490336658354, lr: 0.04526017064221382
[batch_idx--450] train_loss: 0.001533669678386159, acc: 0.8224216336307147, lr: 0.04522610724031057
train_loss:  0.001533669678386159  acc:  0.8224216336307147
->>lr:0.045226
test_loss:  0.0015619356744060713  test_acc:  0.8199528477478595
best acc:  83.33540141456757

------Epoch: 30------
[batch_idx--0] train_loss: 0.001428810996003449, acc: 0.83203125, lr: 0.04522542485937369
[batch_idx--50] train_loss: 0.0015085582899879298, acc: 0.8254442401960784, lr: 0.04519125023075707
[batch_idx--100] train_loss: 0.0015346949267671397, acc: 0.8225943688118812, lr: 0.045156966742431685
[batch_idx--150] train_loss: 0.0015293805904698786, acc: 0.8223561672185431, lr: 0.04512257457923455
[batch_idx--200] train_loss: 0.0015307627919486802, acc: 0.8225279850746269, lr: 0.04508807392658861
[batch_idx--250] train_loss: 0.0015282890902260742, acc: 0.822242280876494, lr: 0.0450534649705017
[batch_idx--300] train_loss: 0.0015185226386945163, acc: 0.8232713870431894, lr: 0.04501874789756559
[batch_idx--350] train_loss: 0.0015208129591953296, acc: 0.8231392450142451, lr: 0.04498392289495493
[batch_idx--400] train_loss: 0.001520225070730465, acc: 0.8230790211970075, lr: 0.04494899015042629
[batch_idx--450] train_loss: 0.0015231390794463095, acc: 0.8231506231124379, lr: 0.04491394985231711
train_loss:  0.0015231390794463095  acc:  0.8231506231124379
->>lr:0.044914
test_loss:  0.0017392350635097819  test_acc:  0.7922819208338504
best acc:  83.33540141456757

------Epoch: 31------
[batch_idx--0] train_loss: 0.001447862247005105, acc: 0.8203125, lr: 0.04491324795060491
[batch_idx--50] train_loss: 0.0015564988157255393, acc: 0.8154871323529411, lr: 0.04487809814247016
[batch_idx--100] train_loss: 0.0015452561443579373, acc: 0.818649443069307, lr: 0.044842841162964144
[batch_idx--150] train_loss: 0.001546420943337363, acc: 0.8185792632450332, lr: 0.04480747720217241
[batch_idx--200] train_loss: 0.001532377002167687, acc: 0.8210509950248757, lr: 0.04477200645075726
[batch_idx--250] train_loss: 0.0015241526896542167, acc: 0.8226780378486056, lr: 0.04473642909995676
[batch_idx--300] train_loss: 0.0015227648261717933, acc: 0.823530938538206, lr: 0.044700745341583706
[batch_idx--350] train_loss: 0.0015240280104249163, acc: 0.8234953703703703, lr: 0.044664955368024575
[batch_idx--400] train_loss: 0.0015225576819210985, acc: 0.8237024625935162, lr: 0.044629059372238496
[batch_idx--450] train_loss: 0.0015255594510005333, acc: 0.8237320790085743, lr: 0.044593057547756214
train_loss:  0.0015255594510005333  acc:  0.8237320790085743
->>lr:0.044593
test_loss:  0.0015433980455941724  test_acc:  0.8306241469164909
best acc:  83.33540141456757

------Epoch: 32------
[batch_idx--0] train_loss: 0.0014162156730890274, acc: 0.84765625, lr: 0.044592336433146
[batch_idx--50] train_loss: 0.0015285033321775058, acc: 0.8203890931372549, lr: 0.04455622686336067
[batch_idx--100] train_loss: 0.0015292524009721704, acc: 0.8203898514851485, lr: 0.0445200118575505
[batch_idx--150] train_loss: 0.0015240840194438467, acc: 0.8218905215231788, lr: 0.04448369161096615
[batch_idx--200] train_loss: 0.0015256680543557625, acc: 0.822158737562189, lr: 0.044447266319425686
[batch_idx--250] train_loss: 0.0015237968159000949, acc: 0.8224134711155379, lr: 0.044410736179313494
[batch_idx--300] train_loss: 0.0015203502710654936, acc: 0.8229599252491694, lr: 0.04437410138757928
[batch_idx--350] train_loss: 0.0015207404095647682, acc: 0.8231281160968661, lr: 0.04433736214173695
[batch_idx--400] train_loss: 0.0015194696627390057, acc: 0.8233030704488778, lr: 0.04430051863986356
[batch_idx--450] train_loss: 0.0015242275095149007, acc: 0.8235845454229874, lr: 0.04426357108059828
train_loss:  0.0015242275095149007  acc:  0.8235845454229874
->>lr:0.044264
test_loss:  0.0015032141790568704  test_acc:  0.8298796376721678
best acc:  83.33540141456757

------Epoch: 33------
[batch_idx--0] train_loss: 0.0014497886877506971, acc: 0.83984375, lr: 0.044262831069394735
[batch_idx--50] train_loss: 0.0014975831236726806, acc: 0.8269761029411765, lr: 0.04422577757680957
[batch_idx--100] train_loss: 0.001497891445708747, acc: 0.8265779702970297, lr: 0.04418862042979371
[batch_idx--150] train_loss: 0.0014968596881452874, acc: 0.8271419701986755, lr: 0.044151359828677295
[batch_idx--200] train_loss: 0.0014928771363599087, acc: 0.8272115982587065, lr: 0.04411399597434825
[batch_idx--250] train_loss: 0.0014952668485109252, acc: 0.8268955428286853, lr: 0.04407652906825116
[batch_idx--300] train_loss: 0.00149658964358988, acc: 0.8269570182724253, lr: 0.04403895931238623
[batch_idx--350] train_loss: 0.0014986495438968483, acc: 0.8263888888888888, lr: 0.044001286909308164
[batch_idx--400] train_loss: 0.0015023497625386477, acc: 0.8257189058603491, lr: 0.04396351206212508
[batch_idx--450] train_loss: 0.0015073231963483645, acc: 0.82579754920679, lr: 0.043925634974497405
train_loss:  0.0015073231963483645  acc:  0.82579754920679
->>lr:0.043926
test_loss:  0.0016471100202608468  test_acc:  0.8111428216900359
best acc:  83.33540141456757

------Epoch: 34------
[batch_idx--0] train_loss: 0.0014226359780877829, acc: 0.8515625, lr: 0.043924876391293916
[batch_idx--50] train_loss: 0.0014947742821794807, acc: 0.8260569852941176, lr: 0.0438868952287954
[batch_idx--100] train_loss: 0.0015051862533840507, acc: 0.8256884282178217, lr: 0.04384881223892658
[batch_idx--150] train_loss: 0.0015002382041837108, acc: 0.8268056705298014, lr: 0.04381062762700922
[batch_idx--200] train_loss: 0.0014921933092844131, acc: 0.8271144278606966, lr: 0.043772341598912995
[batch_idx--250] train_loss: 0.001500040468522366, acc: 0.8258839641434262, lr: 0.04373395436105431
[batch_idx--300] train_loss: 0.0014997306202485712, acc: 0.825750103820598, lr: 0.04369546612039528
[batch_idx--350] train_loss: 0.0015024085070783977, acc: 0.825431801994302, lr: 0.04365687708444255
[batch_idx--400] train_loss: 0.0015004867399831365, acc: 0.8259721789276808, lr: 0.04361818746124621
[batch_idx--450] train_loss: 0.0014987241620447542, acc: 0.8264831464574582, lr: 0.04357939745939863
train_loss:  0.0014987241620447542  acc:  0.8264831464574582
->>lr:0.043579
test_loss:  0.0015233999143863882  test_acc:  0.82801836456136
best acc:  83.33540141456757

------Epoch: 35------
[batch_idx--0] train_loss: 0.0014915295178070664, acc: 0.83984375, lr: 0.04357862063693486
[batch_idx--50] train_loss: 0.0015127168712662715, acc: 0.8242953431372549, lr: 0.04353972846431615
[batch_idx--100] train_loss: 0.0014973038480165276, acc: 0.828434405940594, lr: 0.043500736336052385
[batch_idx--150] train_loss: 0.001494469359325899, acc: 0.828150869205298, lr: 0.04346164446236689
[batch_idx--200] train_loss: 0.0014932061915068112, acc: 0.828008395522388, lr: 0.04342245305402075
[batch_idx--250] train_loss: 0.0014910163595322594, acc: 0.8278604332669323, lr: 0.04338316232231171
[batch_idx--300] train_loss: 0.0014886485460675336, acc: 0.8287609011627907, lr: 0.043343772479072985
[batch_idx--350] train_loss: 0.0014930678872481795, acc: 0.8282362891737892, lr: 0.043304283736672146
[batch_idx--400] train_loss: 0.0014889953556955974, acc: 0.8286510286783042, lr: 0.04326469630800999
[batch_idx--450] train_loss: 0.0014922499649435387, acc: 0.8284184399625091, lr: 0.04322501040651934
train_loss:  0.0014922499649435387  acc:  0.8284184399625091
->>lr:0.043225
test_loss:  0.0015392700892729002  test_acc:  0.8272738553170369
best acc:  83.33540141456757

------Epoch: 36------
[batch_idx--0] train_loss: 0.0015239118365570903, acc: 0.828125, lr: 0.04322421568553529
[batch_idx--50] train_loss: 0.0014545661945115117, acc: 0.8331035539215687, lr: 0.04318442956218869
[batch_idx--100] train_loss: 0.0014726956119060074, acc: 0.8311417079207921, lr: 0.04314454539876614
[batch_idx--150] train_loss: 0.001469909447855902, acc: 0.8313069122516556, lr: 0.043104563410300306
[batch_idx--200] train_loss: 0.0014794923588679872, acc: 0.8298546330845771, lr: 0.043064483812351256
[batch_idx--250] train_loss: 0.0014766628466234858, acc: 0.8294011454183267, lr: 0.04302430682100536
[batch_idx--300] train_loss: 0.0014749781419662294, acc: 0.8293189368770764, lr: 0.042984032652874024
[batch_idx--350] train_loss: 0.0014721466327202839, acc: 0.8300614316239316, lr: 0.04294366152509261
[batch_idx--400] train_loss: 0.0014708839865526801, acc: 0.8306577306733167, lr: 0.04290319365531922
[batch_idx--450] train_loss: 0.001476998974588184, acc: 0.8301020585274412, lr: 0.04286262926173353
train_loss:  0.001476998974588184  acc:  0.8301020585274412
->>lr:0.042863
test_loss:  0.0014824383510876448  test_acc:  0.8352152872564834
best acc:  83.33540141456757
Saving..

------Epoch: 37------
[batch_idx--0] train_loss: 0.0012428545160219073, acc: 0.84765625, lr: 0.04286181699082009
[batch_idx--50] train_loss: 0.001496737608320865, acc: 0.8270526960784313, lr: 0.04282115436825425
[batch_idx--100] train_loss: 0.0014914360743403287, acc: 0.8297107054455446, lr: 0.04278039566418517
[batch_idx--150] train_loss: 0.0014852179632219948, acc: 0.8300393211920529, lr: 0.04273954109836054
[batch_idx--200] train_loss: 0.0014846907252102943, acc: 0.8296214241293532, lr: 0.0426985908910449
[batch_idx--250] train_loss: 0.0014792699876994607, acc: 0.8303037848605578, lr: 0.042657545263018404
[batch_idx--300] train_loss: 0.0014815144534795189, acc: 0.8298380398671097, lr: 0.042616404435575676
[batch_idx--350] train_loss: 0.0014768921268674044, acc: 0.8303285256410257, lr: 0.04257516863052461
[batch_idx--400] train_loss: 0.0014736373526185863, acc: 0.8305700592269327, lr: 0.04253383807018514
[batch_idx--450] train_loss: 0.0014817922439430192, acc: 0.8300413094039644, lr: 0.042492412977388094
train_loss:  0.0014817922439430192  acc:  0.8300413094039644
->>lr:0.042492
test_loss:  0.001503138347511656  test_acc:  0.8283906191835215
best acc:  83.52152872564834

------Epoch: 38------
[batch_idx--0] train_loss: 0.0011399327777326107, acc: 0.875, lr: 0.042491583512834136
[batch_idx--50] train_loss: 0.001465356821997785, acc: 0.8283547794117647, lr: 0.04245006222701923
[batch_idx--100] train_loss: 0.0014750312686445984, acc: 0.8292465965346535, lr: 0.04240844686041831
[batch_idx--150] train_loss: 0.001466348468147169, acc: 0.8309188741721855, lr: 0.042366737637397726
[batch_idx--200] train_loss: 0.0014637526001929496, acc: 0.8307291666666666, lr: 0.04232493478282982
[batch_idx--250] train_loss: 0.0014682817365603144, acc: 0.8299769671314741, lr: 0.042283038522091754
[batch_idx--300] train_loss: 0.0014700104085654268, acc: 0.8297471968438538, lr: 0.04224104908106427
[batch_idx--350] train_loss: 0.001471915387779347, acc: 0.8294715990028491, lr: 0.0421989666861305
[batch_idx--400] train_loss: 0.0014718037120129291, acc: 0.8294985193266833, lr: 0.04215679156417471
[batch_idx--450] train_loss: 0.0014778241395151644, acc: 0.8292515707987642, lr: 0.04211452394258114
train_loss:  0.0014778241395151644  acc:  0.8292515707987642
->>lr:0.042115
test_loss:  0.0016065294304147284  test_acc:  0.8152376225338132
best acc:  83.52152872564834

------Epoch: 39------
[batch_idx--0] train_loss: 0.0017947062151506543, acc: 0.76953125, lr: 0.04211367764821722
[batch_idx--50] train_loss: 0.0015222712319490372, acc: 0.8252910539215687, lr: 0.04207131591176148
[batch_idx--100] train_loss: 0.0015038955993581526, acc: 0.8279316212871287, lr: 0.042028862136503926
[batch_idx--150] train_loss: 0.0014843004840481657, acc: 0.8312551738410596, lr: 0.0419863165513311
[batch_idx--200] train_loss: 0.0014770615212294964, acc: 0.831914645522388, lr: 0.041943679385624544
[batch_idx--250] train_loss: 0.0014677254234503703, acc: 0.8322802539840638, lr: 0.04190095086925955
[batch_idx--300] train_loss: 0.0014628579416946566, acc: 0.8329656353820598, lr: 0.041858131232603915
[batch_idx--350] train_loss: 0.0014634337574721146, acc: 0.8328547898860399, lr: 0.04181522070651669
[batch_idx--400] train_loss: 0.0014673277347852613, acc: 0.8322163341645885, lr: 0.04177221952234698
[batch_idx--450] train_loss: 0.00147231301679376, acc: 0.8315253237060437, lr: 0.041729127911932645
train_loss:  0.00147231301679376  acc:  0.8315253237060437
->>lr:0.041729
test_loss:  0.0017066948103807097  test_acc:  0.7937709393224965
best acc:  83.52152872564834

------Epoch: 40------
[batch_idx--0] train_loss: 0.0013550741132348776, acc: 0.86328125, lr: 0.04172826515897146
[batch_idx--50] train_loss: 0.0014101873133696762, acc: 0.8391544117647058, lr: 0.04168508155313258
[batch_idx--100] train_loss: 0.0014257357365885142, acc: 0.8381420173267327, lr: 0.04164180799084732
[batch_idx--150] train_loss: 0.001431952925269424, acc: 0.8364290149006622, lr: 0.04159844470542206
[batch_idx--200] train_loss: 0.0014420499404271443, acc: 0.8342273009950248, lr: 0.041554991930646906
[batch_idx--250] train_loss: 0.0014476316398477173, acc: 0.8336653386454184, lr: 0.041511449900794445
[batch_idx--300] train_loss: 0.0014522074956904811, acc: 0.8323037790697675, lr: 0.04146781885061848
[batch_idx--350] train_loss: 0.001453707930618669, acc: 0.8318754451566952, lr: 0.04142409901535277
[batch_idx--400] train_loss: 0.0014538923278450966, acc: 0.8317097880299252, lr: 0.04138029063070971
[batch_idx--450] train_loss: 0.0014559004273335821, acc: 0.8322369562953449, lr: 0.041336393932879134
train_loss:  0.0014559004273335821  acc:  0.8322369562953449
->>lr:0.041336
test_loss:  0.0015461497366583692  test_acc:  0.8200769326219134
best acc:  83.52152872564834

------Epoch: 41------
[batch_idx--0] train_loss: 0.001256144605576992, acc: 0.8671875, lr: 0.04133551509975264
[batch_idx--50] train_loss: 0.0014577896891198323, acc: 0.8309589460784313, lr: 0.041291528566287365
[batch_idx--100] train_loss: 0.001445407341787647, acc: 0.8325727103960396, lr: 0.041247454198189024
[batch_idx--150] train_loss: 0.0014558689852263655, acc: 0.8310482201986755, lr: 0.04120329223308148
[batch_idx--200] train_loss: 0.0014484932400582737, acc: 0.8330029539800995, lr: 0.041159042909060875
[batch_idx--250] train_loss: 0.001447398709021954, acc: 0.8335097111553785, lr: 0.04111470646469433
[batch_idx--300] train_loss: 0.0014429372500170308, acc: 0.8349122715946844, lr: 0.04107028313901867
[batch_idx--350] train_loss: 0.001440170277745869, acc: 0.8351250890313391, lr: 0.04102577317153916
[batch_idx--400] train_loss: 0.0014417730283428755, acc: 0.8347490648379052, lr: 0.04098117680222813
[batch_idx--450] train_loss: 0.0014488832032510432, acc: 0.8342243204776617, lr: 0.04093649427152381
train_loss:  0.0014488832032510432  acc:  0.8342243204776617
->>lr:0.040936
test_loss:  0.0014445306253663983  test_acc:  0.8404268519667453
best acc:  83.52152872564834
Saving..

------Epoch: 42------
[batch_idx--0] train_loss: 0.0015059721190482378, acc: 0.8125, lr: 0.040935599743717244
[batch_idx--50] train_loss: 0.0014640043727030942, acc: 0.8362438725490197, lr: 0.04089082957657292
[batch_idx--100] train_loss: 0.0014425336433998724, acc: 0.8391862623762376, lr: 0.04084597373513602
[batch_idx--150] train_loss: 0.0014351962191293274, acc: 0.8396626655629139, lr: 0.04080103246124366
[batch_idx--200] train_loss: 0.0014402088575964024, acc: 0.8385027985074627, lr: 0.04075600599719357
[batch_idx--250] train_loss: 0.0014387521196149318, acc: 0.837960657370518, lr: 0.04071089458574278
[batch_idx--300] train_loss: 0.0014390962865927638, acc: 0.8372871677740864, lr: 0.04066569847010627
[batch_idx--350] train_loss: 0.0014383821692476925, acc: 0.8372173254985755, lr: 0.040620417893955756
[batch_idx--400] train_loss: 0.001439215849383625, acc: 0.836697319201995, lr: 0.04057505310141827
[batch_idx--450] train_loss: 0.0014442677417292906, acc: 0.8362377199986114, lr: 0.04052960433707492
train_loss:  0.0014442677417292906  acc:  0.8362377199986114
->>lr:0.040530
test_loss:  0.0014134770972747367  test_acc:  0.844025313314307
best acc:  84.04268519667453
Saving..

------Epoch: 43------
[batch_idx--0] train_loss: 0.0011978254187852144, acc: 0.86328125, lr: 0.040528694506957764
[batch_idx--50] train_loss: 0.0014975562796690593, acc: 0.8263633578431373, lr: 0.04048316034380937
[batch_idx--100] train_loss: 0.001458682388408423, acc: 0.8326500618811881, lr: 0.040437542704288454
[batch_idx--150] train_loss: 0.0014501524339769257, acc: 0.8334023178807947, lr: 0.04039184183433934
[batch_idx--200] train_loss: 0.0014529489467281903, acc: 0.8334888059701493, lr: 0.04034605798035504
[batch_idx--250] train_loss: 0.001446113699074998, acc: 0.833976593625498, lr: 0.04030019138917598
[batch_idx--300] train_loss: 0.0014395497258183271, acc: 0.8347824958471761, lr: 0.040254242308088696
[batch_idx--350] train_loss: 0.0014366817148535, acc: 0.8354589565527065, lr: 0.040208210984824425
[batch_idx--400] train_loss: 0.0014418486669483105, acc: 0.8345347568578554, lr: 0.04016209766755781
[batch_idx--450] train_loss: 0.0014463882586450087, acc: 0.8347363488041101, lr: 0.040115902604905565
train_loss:  0.0014463882586450087  acc:  0.8347363488041101
->>lr:0.040116
test_loss:  0.001861474116544656  test_acc:  0.787566695619804
best acc:  84.4025313314307

------Epoch: 44------
[batch_idx--0] train_loss: 0.0012092371471226215, acc: 0.87890625, lr: 0.040114977871559376
[batch_idx--50] train_loss: 0.0014819930074736476, acc: 0.8301930147058824, lr: 0.04006869968519579
[batch_idx--100] train_loss: 0.0014374886828447037, acc: 0.8367883663366337, lr: 0.04002234025699524
[batch_idx--150] train_loss: 0.0014432130716062637, acc: 0.836040976821192, lr: 0.039975899836901335
[batch_idx--200] train_loss: 0.0014370230926589957, acc: 0.8372978855721394, lr: 0.03992937867529435
[batch_idx--250] train_loss: 0.0014401879810924786, acc: 0.8363888197211156, lr: 0.03988277702298985
[batch_idx--300] train_loss: 0.0014395794430531043, acc: 0.8359504775747508, lr: 0.03983609513123738
[batch_idx--350] train_loss: 0.0014319569645237253, acc: 0.8370170049857549, lr: 0.03978933325171908
[batch_idx--400] train_loss: 0.0014284235965855365, acc: 0.8373012780548629, lr: 0.03974249163654834
[batch_idx--450] train_loss: 0.0014333587354384992, acc: 0.8371229215121325, lr: 0.03969557053826845
train_loss:  0.0014333587354384992  acc:  0.8371229215121325
->>lr:0.039696
test_loss:  0.0017492319287933092  test_acc:  0.8053108326095049
best acc:  84.4025313314307

------Epoch: 45------
[batch_idx--0] train_loss: 0.0013067481340840459, acc: 0.8671875, lr: 0.03969463130731183
[batch_idx--50] train_loss: 0.0014115737856128345, acc: 0.8396905637254902, lr: 0.0396476293968751
[batch_idx--100] train_loss: 0.0014013750236573639, acc: 0.8410040222772277, lr: 0.03960054851477232
[batch_idx--150] train_loss: 0.0014104987456099382, acc: 0.839817880794702, lr: 0.039553388914836764
[batch_idx--200] train_loss: 0.0014107932228900825, acc: 0.839902052238806, lr: 0.039506150851326104
[batch_idx--250] train_loss: 0.0014145841072517563, acc: 0.8393768675298805, lr: 0.039458834578921055
[batch_idx--300] train_loss: 0.001413696110975678, acc: 0.8394674003322259, lr: 0.03941144035272397
[batch_idx--350] train_loss: 0.001417790300762042, acc: 0.8388866631054132, lr: 0.03936396842825749
[batch_idx--400] train_loss: 0.001419040716155676, acc: 0.8387916926433915, lr: 0.03931641906146319
[batch_idx--450] train_loss: 0.0014288166647349369, acc: 0.8381382997188184, lr: 0.03926879250870011
train_loss:  0.0014288166647349369  acc:  0.8381382997188184
->>lr:0.039269
test_loss:  0.0014435065962685768  test_acc:  0.8425362948256607
best acc:  84.4025313314307

------Epoch: 46------
[batch_idx--0] train_loss: 0.0013202761765569448, acc: 0.84375, lr: 0.0392678391921108
[batch_idx--50] train_loss: 0.0014409790381643118, acc: 0.8361672794117647, lr: 0.03922013417419226
[batch_idx--100] train_loss: 0.0014314574297831053, acc: 0.8364789603960396, lr: 0.03917235248941817
[batch_idx--150] train_loss: 0.00142924823884587, acc: 0.8365583609271523, lr: 0.03912449439540013
[batch_idx--200] train_loss: 0.0014265050349377133, acc: 0.8367342972636815, lr: 0.0390765601501617
[batch_idx--250] train_loss: 0.001420458862224067, acc: 0.8370113296812749, lr: 0.03902855001213699
[batch_idx--300] train_loss: 0.0014231033852181562, acc: 0.836936773255814, lr: 0.0389804642401693
[batch_idx--350] train_loss: 0.001426107760258762, acc: 0.8362713675213675, lr: 0.03893230309350968
[batch_idx--400] train_loss: 0.0014253456869564411, acc: 0.8364732699501247, lr: 0.03888406683181559
[batch_idx--450] train_loss: 0.001429667875786783, acc: 0.8364807164925192, lr: 0.03883575571514944
train_loss:  0.001429667875786783  acc:  0.8364807164925192
->>lr:0.038836
test_loss:  0.0015569923459218828  test_acc:  0.8307482317905447
best acc:  84.4025313314307

------Epoch: 47------
[batch_idx--0] train_loss: 0.0012638759799301624, acc: 0.87109375, lr: 0.038834788731083605
[batch_idx--50] train_loss: 0.0014322548693812946, acc: 0.8381587009803921, lr: 0.038786401530680796
[batch_idx--100] train_loss: 0.0014235867039844542, acc: 0.8374845297029703, lr: 0.03873794000186154
[batch_idx--150] train_loss: 0.0014182918709671952, acc: 0.8364807533112583, lr: 0.03868940440590278
[batch_idx--200] train_loss: 0.0014182389847035358, acc: 0.8367148631840796, lr: 0.03864079500448077
[batch_idx--250] train_loss: 0.001409278349557602, acc: 0.8377583416334662, lr: 0.03859211205966969
[batch_idx--300] train_loss: 0.0014116631347401395, acc: 0.8375337416943521, lr: 0.038543355833940224
[batch_idx--350] train_loss: 0.0014135819833932652, acc: 0.8380519943019943, lr: 0.03849452659015813
[batch_idx--400] train_loss: 0.0014156272067012558, acc: 0.8378565305486284, lr: 0.03844562459158286
[batch_idx--450] train_loss: 0.0014171054571961133, acc: 0.838389974658937, lr: 0.0383966501018661
train_loss:  0.0014171054571961133  acc:  0.838389974658937
->>lr:0.038397
test_loss:  0.0013879898989639916  test_acc:  0.844893907432684
best acc:  84.4025313314307
Saving..

------Epoch: 48------
[batch_idx--0] train_loss: 0.0012633746955543756, acc: 0.84765625, lr: 0.03839566987447492
[batch_idx--50] train_loss: 0.0014212750692797057, acc: 0.8377757352941176, lr: 0.038346621715813135
[batch_idx--100] train_loss: 0.0014091373529598707, acc: 0.8377165841584159, lr: 0.038297501599776934
[batch_idx--150] train_loss: 0.0014187479098728359, acc: 0.8370757450331126, lr: 0.03824830979119395
[batch_idx--200] train_loss: 0.0014124851979648888, acc: 0.8380558146766169, lr: 0.038199046555278386
[batch_idx--250] train_loss: 0.0014114082156424504, acc: 0.8385520418326693, lr: 0.038149712157629516
[batch_idx--300] train_loss: 0.0014089494092742669, acc: 0.8388574543189369, lr: 0.038100306864230285
[batch_idx--350] train_loss: 0.0014114699872380435, acc: 0.8386084401709402, lr: 0.038050830941445866
[batch_idx--400] train_loss: 0.0014090213426470087, acc: 0.8390060006234414, lr: 0.03800128465602222
[batch_idx--450] train_loss: 0.001411844734840252, acc: 0.8390755719096054, lr: 0.03795166827508467
train_loss:  0.001411844734840252  acc:  0.8390755719096054
->>lr:0.037952
test_loss:  0.0014125584135991584  test_acc:  0.844893907432684
best acc:  84.4893907432684

------Epoch: 49------
[batch_idx--0] train_loss: 0.0012806594604626298, acc: 0.8515625, lr: 0.03795067523432826
[batch_idx--50] train_loss: 0.0013891532446495165, acc: 0.8409160539215687, lr: 0.037900987631550954
[batch_idx--100] train_loss: 0.001414503072305481, acc: 0.8382580445544554, lr: 0.03785123047400412
[batch_idx--150] train_loss: 0.0014066792190358635, acc: 0.8401800496688742, lr: 0.03780140402994997
[batch_idx--200] train_loss: 0.0013991006185527688, acc: 0.8409514925373134, lr: 0.03775150856802428
[batch_idx--250] train_loss: 0.0013966278151694345, acc: 0.8409798306772909, lr: 0.03770154435723495
[batch_idx--300] train_loss: 0.001395106112949575, acc: 0.8417384759136213, lr: 0.037651511666960506
[batch_idx--350] train_loss: 0.0013958256823697957, acc: 0.8417022792022792, lr: 0.03760141076694869
[batch_idx--400] train_loss: 0.0013925749765003003, acc: 0.8418796758104738, lr: 0.037551241927314974
[batch_idx--450] train_loss: 0.0013999864629738762, acc: 0.8410629360919221, lr: 0.03750100541854115
train_loss:  0.0013999864629738762  acc:  0.8410629360919221
->>lr:0.037501
test_loss:  0.0013807670977500522  test_acc:  0.8481201141580841
best acc:  84.4893907432684
Saving..

------Epoch: 50------
[batch_idx--0] train_loss: 0.0013867606176063418, acc: 0.84765625, lr: 0.037500000000000006
[batch_idx--50] train_loss: 0.0014032950013072468, acc: 0.8419883578431373, lr: 0.037449694747731944
[batch_idx--100] train_loss: 0.0013977214175341006, acc: 0.8411587252475248, lr: 0.03739932237380827
[batch_idx--150] train_loss: 0.001399345727581901, acc: 0.8409302566225165, lr: 0.03734888314980809
[batch_idx--200] train_loss: 0.0013966340385145734, acc: 0.841806592039801, lr: 0.037298377347670955
[batch_idx--250] train_loss: 0.0013967058964563437, acc: 0.8411977091633466, lr: 0.03724780523969534
[batch_idx--300] train_loss: 0.0013936825527459927, acc: 0.8416216777408638, lr: 0.03719716709853723
[batch_idx--350] train_loss: 0.0013873755044311795, acc: 0.8422253383190883, lr: 0.037146463197208594
[batch_idx--400] train_loss: 0.0013839260935133086, acc: 0.8428440617206983, lr: 0.03709569380907597
[batch_idx--450] train_loss: 0.0013910788087949629, acc: 0.8423299892387267, lr: 0.03704485920785895
train_loss:  0.0013910788087949629  acc:  0.8423299892387267
->>lr:0.037045
test_loss:  0.001378592256725182  test_acc:  0.8462588410472763
best acc:  84.81201141580841

------Epoch: 51------
[batch_idx--0] train_loss: 0.0013034194707870483, acc: 0.875, lr: 0.037043841852542884
[batch_idx--50] train_loss: 0.0014134855397666495, acc: 0.8427542892156863, lr: 0.03699294101633032
[batch_idx--100] train_loss: 0.0014000021250881623, acc: 0.8429378094059405, lr: 0.036941975521017835
[batch_idx--150] train_loss: 0.0013895672202474186, acc: 0.8440604304635762, lr: 0.0368909456413823
[batch_idx--200] train_loss: 0.0013907232020045654, acc: 0.8441775497512438, lr: 0.03683985165254776
[batch_idx--250] train_loss: 0.0013886884156803774, acc: 0.844746015936255, lr: 0.03678869382998386
[batch_idx--300] train_loss: 0.001390767128839532, acc: 0.8439187084717608, lr: 0.03673747244950439
[batch_idx--350] train_loss: 0.0013914344703945785, acc: 0.8434161324786325, lr: 0.03668618778726586
[batch_idx--400] train_loss: 0.0013885528560172757, acc: 0.8437987063591023, lr: 0.0366348401197659
[batch_idx--450] train_loss: 0.0013967138487907345, acc: 0.84384871732565, lr: 0.036583429723841876
train_loss:  0.0013967138487907345  acc:  0.84384871732565
->>lr:0.036583
test_loss:  0.00149455815941957  test_acc:  0.8307482317905447
best acc:  84.81201141580841

------Epoch: 52------
[batch_idx--0] train_loss: 0.0013689553597941995, acc: 0.84375, lr: 0.036582400877996545
[batch_idx--50] train_loss: 0.0014148498474455932, acc: 0.8394607843137255, lr: 0.03653092678462853
[batch_idx--100] train_loss: 0.0013982997440714574, acc: 0.8419709158415841, lr: 0.036479390523077894
[batch_idx--150] train_loss: 0.0013902984194870767, acc: 0.8431550082781457, lr: 0.036427792371198774
[batch_idx--200] train_loss: 0.001388797788725545, acc: 0.8424284825870647, lr: 0.036376132607179
[batch_idx--250] train_loss: 0.00139385478944885, acc: 0.8415712151394422, lr: 0.03632441150953856
[batch_idx--300] train_loss: 0.0013935871716616581, acc: 0.8417384759136213, lr: 0.036272629357128106
[batch_idx--350] train_loss: 0.001391187654323929, acc: 0.8422253383190883, lr: 0.0362207864291275
[batch_idx--400] train_loss: 0.0013913269254304654, acc: 0.8422401028678305, lr: 0.03616888300504424
[batch_idx--450] train_loss: 0.00139571572704222, acc: 0.8423820599159927, lr: 0.03611691936471199
train_loss:  0.00139571572704222  acc:  0.8423820599159927
->>lr:0.036117
test_loss:  0.0013826145606887653  test_acc:  0.8492368780245688
best acc:  84.81201141580841
Saving..

------Epoch: 53------
[batch_idx--0] train_loss: 0.0013552096206694841, acc: 0.85546875, lr: 0.036115879479623185
[batch_idx--50] train_loss: 0.0014322222982003701, acc: 0.8369332107843137, lr: 0.0360638547073383
[batch_idx--100] train_loss: 0.001419306441099558, acc: 0.8384127475247525, lr: 0.036011770285057136
[batch_idx--150] train_loss: 0.001402094581670171, acc: 0.8411889486754967, lr: 0.03595962649358918
[batch_idx--200] train_loss: 0.0013977061245886988, acc: 0.8412430037313433, lr: 0.03590742361406404
[batch_idx--250] train_loss: 0.0013904996518698703, acc: 0.8420225348605578, lr: 0.03585516192792988
[batch_idx--300] train_loss: 0.0013865699072637629, acc: 0.8422835340531561, lr: 0.03580284171695191
[batch_idx--350] train_loss: 0.0013834802636828942, acc: 0.8433493589743589, lr: 0.03575046326321087
[batch_idx--400] train_loss: 0.0013814688108029992, acc: 0.8435746571072319, lr: 0.035698026849101526
[batch_idx--450] train_loss: 0.001387301867741582, acc: 0.8433974381226785, lr: 0.03564553275733112
train_loss:  0.001387301867741582  acc:  0.8433974381226785
->>lr:0.035646
test_loss:  0.00157651787420141  test_acc:  0.819580593125698
best acc:  84.92368780245688

------Epoch: 54------
[batch_idx--0] train_loss: 0.0014347299002110958, acc: 0.83984375, lr: 0.03564448228912682
[batch_idx--50] train_loss: 0.0013919932260999784, acc: 0.8409926470588235, lr: 0.035591929657709624
[batch_idx--100] train_loss: 0.001370714762995001, acc: 0.8451810024752475, lr: 0.03553931992064693
[batch_idx--150] train_loss: 0.0013775278024671489, acc: 0.8435947847682119, lr: 0.035486653361580446
[batch_idx--200] train_loss: 0.001379346100854414, acc: 0.8433418843283582, lr: 0.03543393026445823
[batch_idx--250] train_loss: 0.0013759943839362953, acc: 0.8433920567729084, lr: 0.03538115091353316
[batch_idx--300] train_loss: 0.0013790603385204789, acc: 0.8430751661129569, lr: 0.035328315593361394
[batch_idx--350] train_loss: 0.0013778687241166152, acc: 0.8432714565527065, lr: 0.03527542458880086
[batch_idx--400] train_loss: 0.0013799809501216075, acc: 0.8428050966334164, lr: 0.035222478185009704
[batch_idx--450] train_loss: 0.0013796953179158683, acc: 0.8431978338598257, lr: 0.035169476667444736
train_loss:  0.0013796953179158683  acc:  0.8431978338598257
->>lr:0.035169
test_loss:  0.0015549191264157141  test_acc:  0.8189601687554288
best acc:  84.92368780245688

------Epoch: 55------
[batch_idx--0] train_loss: 0.001460712868720293, acc: 0.83984375, lr: 0.035168416076895005
[batch_idx--50] train_loss: 0.0014114032462969714, acc: 0.8388480392156863, lr: 0.03511535863766653
[batch_idx--100] train_loss: 0.00139291241008899, acc: 0.8410040222772277, lr: 0.03506224666219176
[batch_idx--150] train_loss: 0.0013864937173634372, acc: 0.8406974337748344, lr: 0.03500908043682018
[batch_idx--200] train_loss: 0.0013838076566702765, acc: 0.8422535758706468, lr: 0.034955860248193804
[batch_idx--250] train_loss: 0.0013825840426171087, acc: 0.8423493525896414, lr: 0.034902586383245504
[batch_idx--300] train_loss: 0.0013789505905864691, acc: 0.8434125830564784, lr: 0.03484925912919761
[batch_idx--350] train_loss: 0.0013746480866099185, acc: 0.8438724180911681, lr: 0.03479587877356025
[batch_idx--400] train_loss: 0.0013713314336726901, acc: 0.8440422381546134, lr: 0.034742445604129875
[batch_idx--450] train_loss: 0.0013745115674778932, acc: 0.8440309646960809, lr: 0.034688959908987675
train_loss:  0.0013745115674778932  acc:  0.8440309646960809
->>lr:0.034689
test_loss:  0.0013608762977998655  test_acc:  0.8457625015510609
best acc:  84.92368780245688

------Epoch: 56------
[batch_idx--0] train_loss: 0.0014719102764502168, acc: 0.83203125, lr: 0.034687889661302575
[batch_idx--50] train_loss: 0.001378502936431152, acc: 0.8421415441176471, lr: 0.034634350687009215
[batch_idx--100] train_loss: 0.0013734352236641294, acc: 0.8433632425742574, lr: 0.034580759769790186
[batch_idx--150] train_loss: 0.0013553769121853179, acc: 0.8460523592715232, lr: 0.034527117198577144
[batch_idx--200] train_loss: 0.0013521157210317455, acc: 0.8469177549751243, lr: 0.034473423262580266
[batch_idx--250] train_loss: 0.0013552923857019213, acc: 0.8460688496015937, lr: 0.03441967825128663
[batch_idx--300] train_loss: 0.001360241097647177, acc: 0.8456706810631229, lr: 0.0343658824544587
[batch_idx--350] train_loss: 0.0013604719127777783, acc: 0.8457754629629629, lr: 0.034312036162132735
[batch_idx--400] train_loss: 0.001363874178355388, acc: 0.8455716178304239, lr: 0.034258139664617236
[batch_idx--450] train_loss: 0.00136206328874556, acc: 0.8462873607109384, lr: 0.0342041932524914
train_loss:  0.00136206328874556  acc:  0.8462873607109384
->>lr:0.034204
test_loss:  0.0013625636217922234  test_acc:  0.8513463208834843
best acc:  84.92368780245688
Saving..

------Epoch: 57------
[batch_idx--0] train_loss: 0.0015615668380632997, acc: 0.80859375, lr: 0.03420311381711696
[batch_idx--50] train_loss: 0.0013749569919252512, acc: 0.8408394607843137, lr: 0.03414911679172232
[batch_idx--100] train_loss: 0.001356288313515263, acc: 0.8457998143564357, lr: 0.034095070439506506
[batch_idx--150] train_loss: 0.0013646476235956151, acc: 0.8434913079470199, lr: 0.03404097505185664
[batch_idx--200] train_loss: 0.0013624130567507957, acc: 0.8447411380597015, lr: 0.0339868309204242
[batch_idx--250] train_loss: 0.0013548697983155925, acc: 0.8461155378486056, lr: 0.03393263833712346
[batch_idx--300] train_loss: 0.0013542138945088532, acc: 0.8460600083056479, lr: 0.03387839759412996
[batch_idx--350] train_loss: 0.0013576774301400085, acc: 0.8458311075498576, lr: 0.03382410898387883
[batch_idx--400] train_loss: 0.001357367043929207, acc: 0.8460586814214464, lr: 0.033769772799063326
[batch_idx--450] train_loss: 0.0013636226056206167, acc: 0.8463828236192592, lr: 0.03371538933263315
train_loss:  0.0013636226056206167  acc:  0.8463828236192592
->>lr:0.033715
test_loss:  0.001347611383581771  test_acc:  0.8535798486164536
best acc:  85.13463208834843
Saving..

------Epoch: 58------
[batch_idx--0] train_loss: 0.0014555368106812239, acc: 0.83203125, lr: 0.03371430118304538
[batch_idx--50] train_loss: 0.0014060185058042407, acc: 0.8384650735294118, lr: 0.03365986979142943
[batch_idx--100] train_loss: 0.0013790184661912003, acc: 0.8419322400990099, lr: 0.03360539171073317
[batch_idx--150] train_loss: 0.0013673636285181077, acc: 0.843775869205298, lr: 0.03355086723467137
[batch_idx--200] train_loss: 0.0013710739865982488, acc: 0.843769434079602, lr: 0.03349629665720888
[batch_idx--250] train_loss: 0.0013609048706364347, acc: 0.8460844123505976, lr: 0.03344168027255918
[batch_idx--300] train_loss: 0.0013583611918959854, acc: 0.8460340531561462, lr: 0.033387018375182646
[batch_idx--350] train_loss: 0.0013583579307629003, acc: 0.8456530448717948, lr: 0.03333231125978507
[batch_idx--400] train_loss: 0.0013614466998544974, acc: 0.8452014495012469, lr: 0.03327755922131603
[batch_idx--450] train_loss: 0.001362773280297348, acc: 0.8459315444162877, lr: 0.033222762554967304
train_loss:  0.001362773280297348  acc:  0.8459315444162877
->>lr:0.033223
test_loss:  0.0013983214644081674  test_acc:  0.8471274351656533
best acc:  85.35798486164536

------Epoch: 59------
[batch_idx--0] train_loss: 0.00147815712261945, acc: 0.83984375, lr: 0.03322166616846458
[batch_idx--50] train_loss: 0.001372077954275643, acc: 0.8432904411764706, lr: 0.033166824286034645
[batch_idx--100] train_loss: 0.0013552077038508684, acc: 0.845993193069307, lr: 0.03311193837274467
[batch_idx--150] train_loss: 0.0013602191323278756, acc: 0.8458971440397351, lr: 0.03305700872450816
[batch_idx--200] train_loss: 0.0013594892946308227, acc: 0.845771144278607, lr: 0.03300203563747449
[batch_idx--250] train_loss: 0.0013539122609584871, acc: 0.846597983067729, lr: 0.0329470194080272
[batch_idx--300] train_loss: 0.0013554254898714812, acc: 0.8466959094684385, lr: 0.032891960332782424
[batch_idx--350] train_loss: 0.001356724777028092, acc: 0.8464988425925926, lr: 0.03283685870858731
[batch_idx--400] train_loss: 0.001351724023422101, acc: 0.847071773690773, lr: 0.03278171483251839
[batch_idx--450] train_loss: 0.0013556810993444262, acc: 0.8473808449335231, lr: 0.03272652900188
train_loss:  0.0013556810993444262  acc:  0.8473808449335231
->>lr:0.032727
test_loss:  0.0013492064346434652  test_acc:  0.8486164536542995
best acc:  85.35798486164536

------Epoch: 60------
[batch_idx--0] train_loss: 0.0013532688608393073, acc: 0.82421875, lr: 0.032725424859373686
[batch_idx--50] train_loss: 0.0013891003441576864, acc: 0.8424479166666666, lr: 0.032670196541591943
[batch_idx--100] train_loss: 0.0013642460753274437, acc: 0.8469987623762376, lr: 0.03261492687048379
[batch_idx--150] train_loss: 0.0013593941555352302, acc: 0.8482771109271523, lr: 0.03255961614403176
[batch_idx--200] train_loss: 0.0013497166430568369, acc: 0.8489388992537313, lr: 0.03250426466043975
[batch_idx--250] train_loss: 0.0013518123882841244, acc: 0.8482787599601593, lr: 0.03244887271813142
[batch_idx--300] train_loss: 0.0013526732765719838, acc: 0.8480974875415282, lr: 0.032393440615748524
[batch_idx--350] train_loss: 0.0013503858241175654, acc: 0.8483239850427351, lr: 0.03233796865214936
[batch_idx--400] train_loss: 0.0013470985253743288, acc: 0.8485914120947631, lr: 0.03228245712640712
[batch_idx--450] train_loss: 0.0013513432907293426, acc: 0.8484569722636859, lr: 0.0322269063378083
train_loss:  0.0013513432907293426  acc:  0.8484569722636859
->>lr:0.032227
test_loss:  0.0014145857961794064  test_acc:  0.8410472763370145
best acc:  85.35798486164536

------Epoch: 61------
[batch_idx--0] train_loss: 0.0013375487178564072, acc: 0.84375, lr: 0.03222579492361179
[batch_idx--50] train_loss: 0.0013458333548852335, acc: 0.8491881127450981, lr: 0.032170204395443734
[batch_idx--100] train_loss: 0.0013330025404765463, acc: 0.8498607673267327, lr: 0.03211457520962184
[batch_idx--150] train_loss: 0.0013477171228470787, acc: 0.8478114652317881, lr: 0.032058907666066935
[batch_idx--200] train_loss: 0.001348722656604959, acc: 0.8479671952736318, lr: 0.03200320206490668
[batch_idx--250] train_loss: 0.0013471496230236207, acc: 0.8480141932270916, lr: 0.03194745870647391
[batch_idx--300] train_loss: 0.001349607651180702, acc: 0.8476951827242525, lr: 0.03189167789130505
[batch_idx--350] train_loss: 0.0013447321033996777, acc: 0.848335113960114, lr: 0.031835859920138446
[batch_idx--400] train_loss: 0.0013471427658094358, acc: 0.8478900405236908, lr: 0.03178000509391275
[batch_idx--450] train_loss: 0.001351971370121637, acc: 0.8482313326622002, lr: 0.03172411371376536
train_loss:  0.001351971370121637  acc:  0.8482313326622002
->>lr:0.031724
test_loss:  0.0016260129455183468  test_acc:  0.8205732721181288
best acc:  85.35798486164536

------Epoch: 62------
[batch_idx--0] train_loss: 0.0016412121476605535, acc: 0.80859375, lr: 0.031722995515381644
[batch_idx--50] train_loss: 0.0013803833976899292, acc: 0.8413756127450981, lr: 0.03166706716067022
[batch_idx--100] train_loss: 0.0013611103248976097, acc: 0.8444461633663366, lr: 0.03161110286093404
[batch_idx--150] train_loss: 0.001350669768509387, acc: 0.8473199503311258, lr: 0.031555102917900694
[batch_idx--200] train_loss: 0.0013483578318368588, acc: 0.8472870024875622, lr: 0.03149906763348994
[batch_idx--250] train_loss: 0.0013399397327278951, acc: 0.8487456424302788, lr: 0.0314429973098121
[batch_idx--300] train_loss: 0.0013381585778909862, acc: 0.8494082225913622, lr: 0.031386892249166375
[batch_idx--350] train_loss: 0.0013389476777937932, acc: 0.8489360754985755, lr: 0.03133075275403927
[batch_idx--400] train_loss: 0.0013379500525657487, acc: 0.8492051122194514, lr: 0.03127457912710293
[batch_idx--450] train_loss: 0.0013400936237302529, acc: 0.8493768875620509, lr: 0.031218371671213524
train_loss:  0.0013400936237302529  acc:  0.8493768875620509
->>lr:0.031218
test_loss:  0.0013378498601268518  test_acc:  0.8533316788683459
best acc:  85.35798486164536

------Epoch: 63------
[batch_idx--0] train_loss: 0.0011784200323745608, acc: 0.8671875, lr: 0.031217247179121367
[batch_idx--50] train_loss: 0.0013563465669422464, acc: 0.8458180147058824, lr: 0.0311610055298914
[batch_idx--100] train_loss: 0.0013399244226183337, acc: 0.8483524133663366, lr: 0.031104730664032437
[batch_idx--150] train_loss: 0.0013280423120626353, acc: 0.8502949089403974, lr: 0.031048422884946483
[batch_idx--200] train_loss: 0.00132972995266411, acc: 0.8504547574626866, lr: 0.030992082496212964
[batch_idx--250] train_loss: 0.0013290517699099573, acc: 0.8507843625498008, lr: 0.030935709801587143
[batch_idx--300] train_loss: 0.0013272756957402607, acc: 0.850952553986711, lr: 0.03087930510499845
[batch_idx--350] train_loss: 0.0013290732028360805, acc: 0.8510616987179487, lr: 0.030822868710548842
[batch_idx--400] train_loss: 0.0013320530901786097, acc: 0.8503350997506235, lr: 0.030766400922511174
[batch_idx--450] train_loss: 0.0013307638195564353, acc: 0.8505137640156907, lr: 0.030709902045327583
train_loss:  0.0013307638195564353  acc:  0.8505137640156907
->>lr:0.030710
test_loss:  0.0013580858737012118  test_acc:  0.8479960292840303
best acc:  85.35798486164536

------Epoch: 64------
[batch_idx--0] train_loss: 0.00153858563862741, acc: 0.80078125, lr: 0.030708771752766397
[batch_idx--50] train_loss: 0.001322657292179179, acc: 0.8517156862745098, lr: 0.030652241478464032
[batch_idx--100] train_loss: 0.001320174385765844, acc: 0.8529548267326733, lr: 0.03059568073049836
[batch_idx--150] train_loss: 0.001316594598284423, acc: 0.8532698675496688, lr: 0.030539089813812693
[batch_idx--200] train_loss: 0.0013264914701196981, acc: 0.8514070273631841, lr: 0.03048246903351297
[batch_idx--250] train_loss: 0.0013205860541999875, acc: 0.8522161354581673, lr: 0.030425818694866172
[batch_idx--300] train_loss: 0.0013279941534790585, acc: 0.850874688538206, lr: 0.030369139103298623
[batch_idx--350] train_loss: 0.001326713822297805, acc: 0.8509504095441596, lr: 0.030312430564394355
[batch_idx--400] train_loss: 0.001319841411344456, acc: 0.851650171446384, lr: 0.03025569338389348
[batch_idx--450] train_loss: 0.0013224761865195866, acc: 0.851919672301871, lr: 0.03019892786769053
train_loss:  0.0013224761865195866  acc:  0.851919672301871
->>lr:0.030199
test_loss:  0.0013386895726404263  test_acc:  0.8514704057575382
best acc:  85.35798486164536

------Epoch: 65------
[batch_idx--0] train_loss: 0.0014232114190235734, acc: 0.83203125, lr: 0.03019779227044398
[batch_idx--50] train_loss: 0.001326155501837824, acc: 0.8481158088235294, lr: 0.03014099816711588
[batch_idx--100] train_loss: 0.0013039287157808569, acc: 0.8530708539603961, lr: 0.030084176346456905
[batch_idx--150] train_loss: 0.001320712523926543, acc: 0.851536630794702, lr: 0.030027327114817932
[batch_idx--200] train_loss: 0.0013146908210697052, acc: 0.8526702425373134, lr: 0.029970450778697578
[batch_idx--250] train_loss: 0.0013078755638339515, acc: 0.8528542081673307, lr: 0.02991354764474065
[batch_idx--300] train_loss: 0.0013104662652269772, acc: 0.852704526578073, lr: 0.02985661801973638
[batch_idx--350] train_loss: 0.00131130825838955, acc: 0.8524973290598291, lr: 0.02979966221061685
[batch_idx--400] train_loss: 0.0013162378377648085, acc: 0.8516209476309227, lr: 0.029742680524455323
[batch_idx--450] train_loss: 0.0013219125105387699, acc: 0.8514076439754227, lr: 0.02968567326846454
train_loss:  0.0013219125105387699  acc:  0.8514076439754227
->>lr:0.029686
test_loss:  0.0012661702362357099  test_acc:  0.8647474872813005
best acc:  85.35798486164536
Saving..

------Epoch: 66------
[batch_idx--0] train_loss: 0.0012381401611492038, acc: 0.859375, lr: 0.02968453286464312
[batch_idx--50] train_loss: 0.0013172513435539953, acc: 0.8553921568627451, lr: 0.029627499844060053
[batch_idx--100] train_loss: 0.0013101494693575372, acc: 0.8550819925742574, lr: 0.029570441874636297
[batch_idx--150] train_loss: 0.0013136849915978827, acc: 0.8537355132450332, lr: 0.029513359263995894
[batch_idx--200] train_loss: 0.0013181080593407822, acc: 0.8527091106965174, lr: 0.029456252319895716
[batch_idx--250] train_loss: 0.0013100891471450845, acc: 0.8549084910358565, lr: 0.029399121350223874
[batch_idx--300] train_loss: 0.0013106136603676945, acc: 0.8547420058139535, lr: 0.029341966662997956
[batch_idx--350] train_loss: 0.0013124538050563821, acc: 0.8542445690883191, lr: 0.02928478856636346
[batch_idx--400] train_loss: 0.0013143679864781252, acc: 0.8535984258104738, lr: 0.02922758736859208
[batch_idx--450] train_loss: 0.0013145212741383046, acc: 0.8536119693130142, lr: 0.02917036337808005
train_loss:  0.0013145212741383046  acc:  0.8536119693130142
->>lr:0.029170
test_loss:  0.0015459241538977324  test_acc:  0.8244199032137982
best acc:  86.47474872813004

------Epoch: 67------
[batch_idx--0] train_loss: 0.0015116551658138633, acc: 0.84765625, lr: 0.02916921866790256
[batch_idx--50] train_loss: 0.0013417253549228988, acc: 0.8498008578431373, lr: 0.029111971746632322
[batch_idx--100] train_loss: 0.0013179319448501682, acc: 0.8514077970297029, lr: 0.02905470265595496
[batch_idx--150] train_loss: 0.0013079702941275245, acc: 0.8530629139072847, lr: 0.028997411704632756
[batch_idx--200] train_loss: 0.0013121058179797669, acc: 0.8518734452736318, lr: 0.02894009920154584
[batch_idx--250] train_loss: 0.0013125060975878126, acc: 0.8509711155378487, lr: 0.02888276545569057
[batch_idx--300] train_loss: 0.0013154456380327724, acc: 0.8505762043189369, lr: 0.0288254107761778
[batch_idx--350] train_loss: 0.001315213322302491, acc: 0.8508613782051282, lr: 0.028768035472231265
[batch_idx--400] train_loss: 0.0013123921596531075, acc: 0.8509877649625935, lr: 0.0287106398531859
[batch_idx--450] train_loss: 0.001314157125104176, acc: 0.8515812128996425, lr: 0.02865322422848614
train_loss:  0.001314157125104176  acc:  0.8515812128996425
->>lr:0.028653
test_loss:  0.0013832894747540353  test_acc:  0.8497332175207842
best acc:  86.47474872813004

------Epoch: 68------
[batch_idx--0] train_loss: 0.0013234653742983937, acc: 0.84765625, lr: 0.028652075714060296
[batch_idx--50] train_loss: 0.0013032679003206831, acc: 0.854702818627451, lr: 0.028594640002494582
[batch_idx--100] train_loss: 0.0012987544443135582, acc: 0.856667698019802, lr: 0.028537184910679542
[batch_idx--150] train_loss: 0.0012898294949774563, acc: 0.8567880794701986, lr: 0.028479710748380266
[batch_idx--200] train_loss: 0.0012902950636917419, acc: 0.8562461131840796, lr: 0.028422217825464665
[batch_idx--250] train_loss: 0.001300543231984222, acc: 0.8549084910358565, lr: 0.028364706451901797
[batch_idx--300] train_loss: 0.001303107605936111, acc: 0.8544564991694352, lr: 0.028307176937760206
[batch_idx--350] train_loss: 0.001301934891028802, acc: 0.8547231125356125, lr: 0.028249629593206222
[batch_idx--400] train_loss: 0.0013052149630207429, acc: 0.8540173004987531, lr: 0.02819206472850232
[batch_idx--450] train_loss: 0.0013077166970540811, acc: 0.853733467559968, lr: 0.02813448265400542
train_loss:  0.0013077166970540811  acc:  0.853733467559968
->>lr:0.028134
test_loss:  0.001292895003811185  test_acc:  0.8636307234148157
best acc:  86.47474872813004

------Epoch: 69------
[batch_idx--0] train_loss: 0.0012232596054673195, acc: 0.8515625, lr: 0.028133330839107608
[batch_idx--50] train_loss: 0.0013353229276653305, acc: 0.8472732843137255, lr: 0.028075731530447786
[batch_idx--100] train_loss: 0.0013179111762414916, acc: 0.8504795792079208, lr: 0.028018115639197247
[batch_idx--150] train_loss: 0.0013112240634389862, acc: 0.8521316225165563, lr: 0.027960483475988025
[batch_idx--200] train_loss: 0.001309976531354831, acc: 0.8513487251243781, lr: 0.02790283535153988
[batch_idx--250] train_loss: 0.0013085903592437388, acc: 0.8513446215139442, lr: 0.027845171576658636
[batch_idx--300] train_loss: 0.00130483959538497, acc: 0.8521854235880398, lr: 0.027787492462234484
[batch_idx--350] train_loss: 0.0013018098535106392, acc: 0.8530315170940171, lr: 0.02772979831924033
[batch_idx--400] train_loss: 0.0013037204996164786, acc: 0.8531211034912718, lr: 0.02767208945873009
[batch_idx--450] train_loss: 0.0013077004452860252, acc: 0.8531693685562537, lr: 0.027614366191837037
train_loss:  0.0013077004452860252  acc:  0.8531693685562537
->>lr:0.027614
test_loss:  0.001433493999618945  test_acc:  0.839806427596476
best acc:  86.47474872813004

------Epoch: 70------
[batch_idx--0] train_loss: 0.0012484348844736814, acc: 0.859375, lr: 0.02761321158169134
[batch_idx--50] train_loss: 0.0013087828890146578, acc: 0.8543198529411765, lr: 0.027555473940897853
[batch_idx--100] train_loss: 0.0012978265467699212, acc: 0.8558555074257426, lr: 0.027497722522445946
[batch_idx--150] train_loss: 0.0012867573964014797, acc: 0.8568915562913907, lr: 0.027439957637698322
[batch_idx--200] train_loss: 0.0012923846156243698, acc: 0.8555659203980099, lr: 0.027382179598090313
[batch_idx--250] train_loss: 0.0012883628420327674, acc: 0.8561068227091634, lr: 0.027324388715128153
[batch_idx--300] train_loss: 0.001287056505022589, acc: 0.8561435838870431, lr: 0.027266585300387343
[batch_idx--350] train_loss: 0.0012918654990122466, acc: 0.8554242343304843, lr: 0.027208769665510946
[batch_idx--400] train_loss: 0.0012927620299799168, acc: 0.8556246103491272, lr: 0.027150942122207884
[batch_idx--450] train_loss: 0.0012971506252862622, acc: 0.855642725726386, lr: 0.027093102982251305
train_loss:  0.0012971506252862622  acc:  0.855642725726386
->>lr:0.027093
test_loss:  0.0013340748277960736  test_acc:  0.8514704057575382
best acc:  86.47474872813004

------Epoch: 71------
[batch_idx--0] train_loss: 0.0014311535051092505, acc: 0.83984375, lr: 0.02709194608330789
[batch_idx--50] train_loss: 0.0013400362941491254, acc: 0.8501838235294118, lr: 0.027034095436018247
[batch_idx--100] train_loss: 0.001317137228739955, acc: 0.851871905940594, lr: 0.026976233822045795
[batch_idx--150] train_loss: 0.0013076837493772833, acc: 0.8523385761589404, lr: 0.026918361553347366
[batch_idx--200] train_loss: 0.001295252870513469, acc: 0.8546719527363185, lr: 0.026860478941937244
[batch_idx--250] train_loss: 0.001292587408426763, acc: 0.8545972360557769, lr: 0.026802586299885446
[batch_idx--300] train_loss: 0.0012922958976378746, acc: 0.8549496470099668, lr: 0.026744683939316106
[batch_idx--350] train_loss: 0.0012904976813342857, acc: 0.8551460113960114, lr: 0.026686772172405743
[batch_idx--400] train_loss: 0.0012903798530064059, acc: 0.8550303927680798, lr: 0.02662885131138157
[batch_idx--450] train_loss: 0.0013026153375142633, acc: 0.8540024993925087, lr: 0.026570921668519862
train_loss:  0.0013026153375142633  acc:  0.8540024993925087
->>lr:0.026571
test_loss:  0.0014650433874704182  test_acc:  0.8365802208710759
best acc:  86.47474872813004

------Epoch: 72------
[batch_idx--0] train_loss: 0.0013982889940962195, acc: 0.8359375, lr: 0.026569762988232833
[batch_idx--50] train_loss: 0.0012865227364076703, acc: 0.8529411764705882, lr: 0.026511824709652916
[batch_idx--100] train_loss: 0.0012845374196583388, acc: 0.8543858292079208, lr: 0.02645387828017616
[batch_idx--150] train_loss: 0.001285571504742006, acc: 0.8544857201986755, lr: 0.026395924012216673
[batch_idx--200] train_loss: 0.0012761653879940027, acc: 0.8558379975124378, lr: 0.02633796221823084
[batch_idx--250] train_loss: 0.0012814749950131187, acc: 0.8557021912350598, lr: 0.026279993210715575
[batch_idx--300] train_loss: 0.0012791050504042188, acc: 0.8562993147840532, lr: 0.026222017302206753
[batch_idx--350] train_loss: 0.0012835939907325576, acc: 0.8555689102564102, lr: 0.02616403480527739
[batch_idx--400] train_loss: 0.0012883180251607482, acc: 0.8550401340399002, lr: 0.02610604603253605
[batch_idx--450] train_loss: 0.0012930015346247637, acc: 0.8549310931370847, lr: 0.026048051296625147
train_loss:  0.0012930015346247637  acc:  0.8549310931370847
->>lr:0.026048
test_loss:  0.0015286219069732597  test_acc:  0.82801836456136
best acc:  86.47474872813004

------Epoch: 73------
[batch_idx--0] train_loss: 0.001349621801637113, acc: 0.86328125, lr: 0.02604689134322999
[batch_idx--50] train_loss: 0.0012746262204303754, acc: 0.8579963235294118, lr: 0.025988890847003655
[batch_idx--100] train_loss: 0.001290064965522304, acc: 0.8559715346534653, lr: 0.025930885019241706
[batch_idx--150] train_loss: 0.0012861889673686827, acc: 0.8560378725165563, lr: 0.025872874172678507
[batch_idx--200] train_loss: 0.001289264698899745, acc: 0.855488184079602, lr: 0.02581485862007545
[batch_idx--250] train_loss: 0.0012825722882683682, acc: 0.8562624501992032, lr: 0.02575683867421932
[batch_idx--300] train_loss: 0.0012787856293866948, acc: 0.8565977990033222, lr: 0.02569881464792059
[batch_idx--350] train_loss: 0.0012727558959084444, acc: 0.8576166310541311, lr: 0.02564078685401172
[batch_idx--400] train_loss: 0.0012789814712066306, acc: 0.8567545978802993, lr: 0.025582755605345495
[batch_idx--450] train_loss: 0.0012848839072222412, acc: 0.8566233901482279, lr: 0.02552472121479332
train_loss:  0.0012848839072222412  acc:  0.8566233901482279
->>lr:0.025525
test_loss:  0.0014072593025570104  test_acc:  0.8390619183521528
best acc:  86.47474872813004

------Epoch: 74------
[batch_idx--0] train_loss: 0.0012281846720725298, acc: 0.86328125, lr: 0.025523560497083927
[batch_idx--50] train_loss: 0.0013258832474878314, acc: 0.8510263480392157, lr: 0.025465523224145754
[batch_idx--100] train_loss: 0.0013061924192058567, acc: 0.8545792079207921, lr: 0.02540748344137178
[batch_idx--150] train_loss: 0.0012960827451458338, acc: 0.8553135347682119, lr: 0.02534944146167944
[batch_idx--200] train_loss: 0.0012987329490223335, acc: 0.8547691231343284, lr: 0.025291397597997967
[batch_idx--250] train_loss: 0.0012961670629257165, acc: 0.8550329930278885, lr: 0.025233352163266793
[batch_idx--300] train_loss: 0.001296734841125847, acc: 0.8549756021594684, lr: 0.025175305470433812
[batch_idx--350] train_loss: 0.0012937534820723568, acc: 0.8552461716524217, lr: 0.025117257832453683
[batch_idx--400] train_loss: 0.0012914470406806566, acc: 0.8554492674563591, lr: 0.025059209562286185
[batch_idx--450] train_loss: 0.0012943587084312616, acc: 0.8558076162043947, lr: 0.02500116097289448
train_loss:  0.0012943587084312616  acc:  0.8558076162043947
->>lr:0.025001
test_loss:  0.0012968758012272463  test_acc:  0.8590395830748232
best acc:  86.47474872813004

------Epoch: 75------
[batch_idx--0] train_loss: 0.0012868472840636969, acc: 0.859375, lr: 0.025
[batch_idx--50] train_loss: 0.0012911945905572937, acc: 0.8568474264705882, lr: 0.024941951407416053
[batch_idx--100] train_loss: 0.001272034626753798, acc: 0.8584081064356436, lr: 0.024883903127797022
[batch_idx--150] train_loss: 0.0012792035130111637, acc: 0.8569950331125827, lr: 0.02482585547410613
[batch_idx--200] train_loss: 0.0012795115188019006, acc: 0.8565959266169154, lr: 0.024767808759303227
[batch_idx--250] train_loss: 0.0012759138462207914, acc: 0.8571184013944223, lr: 0.024709763296343102
[batch_idx--300] train_loss: 0.0012808895731372665, acc: 0.8571298795681063, lr: 0.024651719398173802
[batch_idx--350] train_loss: 0.0012769776827952907, acc: 0.8579504985754985, lr: 0.024593677377734924
[batch_idx--400] train_loss: 0.0012749862211798213, acc: 0.8581768235660848, lr: 0.02453563754795596
[batch_idx--450] train_loss: 0.0012762056394713687, acc: 0.8584111500676919, lr: 0.024477600221754565
train_loss:  0.0012762056394713687  acc:  0.8584111500676919
->>lr:0.024478
test_loss:  0.001293505772066288  test_acc:  0.8597840923191463
best acc:  86.47474872813004

------Epoch: 76------
[batch_idx--0] train_loss: 0.0013280266430228949, acc: 0.86328125, lr: 0.02447643950291608
[batch_idx--50] train_loss: 0.0013119887432777415, acc: 0.8524816176470589, lr: 0.024418405052717584
[batch_idx--100] train_loss: 0.0012903600115107723, acc: 0.855778155940594, lr: 0.024360373738147447
[batch_idx--150] train_loss: 0.0012742363576413384, acc: 0.8561672185430463, lr: 0.024302345872077406
[batch_idx--200] train_loss: 0.001262069152163073, acc: 0.8578785758706468, lr: 0.024244321767360646
[batch_idx--250] train_loss: 0.0012643215971427462, acc: 0.85746078187251, lr: 0.024186301736830045
[batch_idx--300] train_loss: 0.0012641053206281136, acc: 0.8574932516611296, lr: 0.024128286093296536
[batch_idx--350] train_loss: 0.0012699521355333597, acc: 0.8571158297720798, lr: 0.02407027514954738
[batch_idx--400] train_loss: 0.0012696353119723684, acc: 0.8573877805486284, lr: 0.024012269218344526
[batch_idx--450] train_loss: 0.001274746564161725, acc: 0.857395771861006, lr: 0.023954268612422863
train_loss:  0.001274746564161725  acc:  0.857395771861006
->>lr:0.023954
test_loss:  0.0013598680814245318  test_acc:  0.8488646234024072
best acc:  86.47474872813004

------Epoch: 77------
[batch_idx--0] train_loss: 0.0011919904500246048, acc: 0.859375, lr: 0.02395310865677001
[batch_idx--50] train_loss: 0.001321191630125338, acc: 0.8517156862745098, lr: 0.023895113804784887
[batch_idx--100] train_loss: 0.0012946984025253223, acc: 0.8561262376237624, lr: 0.02383712490971616
[batch_idx--150] train_loss: 0.0012861412952943946, acc: 0.8564776490066225, lr: 0.023779142284206876
[batch_idx--200] train_loss: 0.0012764816417993599, acc: 0.8578591417910447, lr: 0.023721166240866298
[batch_idx--250] train_loss: 0.0012719028576488691, acc: 0.858394546812749, lr: 0.0236631970922682
[batch_idx--300] train_loss: 0.0012713855687850634, acc: 0.8586742109634552, lr: 0.023605235150949158
[batch_idx--350] train_loss: 0.001270080320908558, acc: 0.8589854878917379, lr: 0.02354728072940694
[batch_idx--400] train_loss: 0.0012720432562145063, acc: 0.8585664744389028, lr: 0.02348933414009873
[batch_idx--450] train_loss: 0.0012726830767630108, acc: 0.8588016801471865, lr: 0.02343139569543949
train_loss:  0.0012726830767630108  acc:  0.8588016801471865
->>lr:0.023431
test_loss:  0.0013740590198827833  test_acc:  0.8529594242461844
best acc:  86.47474872813004

------Epoch: 78------
[batch_idx--0] train_loss: 0.0013621874386444688, acc: 0.83984375, lr: 0.023430237011767167
[batch_idx--50] train_loss: 0.0012642546321776714, acc: 0.8598345588235294, lr: 0.02337230719645424
[batch_idx--100] train_loss: 0.0012593507555792901, acc: 0.860728650990099, lr: 0.023314386156732855
[batch_idx--150] train_loss: 0.0012572691671778478, acc: 0.8601769453642384, lr: 0.023256474204880216
[batch_idx--200] train_loss: 0.0012614481149366432, acc: 0.8595304726368159, lr: 0.023198571653124563
[batch_idx--250] train_loss: 0.0012683327046809235, acc: 0.8587369272908366, lr: 0.02314067881364343
[batch_idx--300] train_loss: 0.001272534696111437, acc: 0.8585703903654485, lr: 0.02308279599856199
[batch_idx--350] train_loss: 0.0012739585850857452, acc: 0.8581842058404558, lr: 0.023024923519951404
[batch_idx--400] train_loss: 0.0012748509500596088, acc: 0.8582157886533666, lr: 0.02296706168982706
[batch_idx--450] train_loss: 0.0012780354405877558, acc: 0.8580206199881973, lr: 0.022909210820146964
train_loss:  0.0012780354405877558  acc:  0.8580206199881973
->>lr:0.022909
test_loss:  0.0012855625471356992  test_acc:  0.8582950738305001
best acc:  86.47474872813004

------Epoch: 79------
[batch_idx--0] train_loss: 0.0011741389753296971, acc: 0.8828125, lr: 0.022908053916692112
[batch_idx--50] train_loss: 0.0012684391843447206, acc: 0.8613664215686274, lr: 0.022850214547982983
[batch_idx--100] train_loss: 0.0012580197601864981, acc: 0.8609607054455446, lr: 0.022792386769691267
[batch_idx--150] train_loss: 0.001248449376253843, acc: 0.8613410596026491, lr: 0.022734570893591363
[batch_idx--200] train_loss: 0.0012501512951824574, acc: 0.8611435012437811, lr: 0.022676767231393526
[batch_idx--250] train_loss: 0.0012542956171972877, acc: 0.8605110806772909, lr: 0.02261897609474213
[batch_idx--300] train_loss: 0.0012526454849159376, acc: 0.8607246677740864, lr: 0.022561197795214035
[batch_idx--350] train_loss: 0.0012589424964308505, acc: 0.8598201566951567, lr: 0.022503432644316902
[batch_idx--400] train_loss: 0.0012596552099096601, acc: 0.8594334476309227, lr: 0.022445680953487472
[batch_idx--450] train_loss: 0.0012647708305749516, acc: 0.8592703162425799, lr: 0.022387943034089947
train_loss:  0.0012647708305749516  acc:  0.8592703162425799
->>lr:0.022388
test_loss:  0.0012412164510516763  test_acc:  0.8632584687926542
best acc:  86.47474872813004

------Epoch: 80------
[batch_idx--0] train_loss: 0.0012574252905324101, acc: 0.87890625, lr: 0.022386788418308663
[batch_idx--50] train_loss: 0.001260124519010823, acc: 0.8608302696078431, lr: 0.022329064866462054
[batch_idx--100] train_loss: 0.0012673835643557923, acc: 0.8592976485148515, lr: 0.022271355714774788
[batch_idx--150] train_loss: 0.0012570466200161159, acc: 0.8598923841059603, lr: 0.022213661274381714
[batch_idx--200] train_loss: 0.0012561181318061195, acc: 0.8608325559701493, lr: 0.022155981856338363
[batch_idx--250] train_loss: 0.0012507288430565115, acc: 0.8612269671314741, lr: 0.022098317771619257
[batch_idx--300] train_loss: 0.0012520652724989996, acc: 0.8609323089700996, lr: 0.022040669331116272
[batch_idx--350] train_loss: 0.0012593200498929198, acc: 0.8593527421652422, lr: 0.021983036845636923
[batch_idx--400] train_loss: 0.001260099825426063, acc: 0.8596282730673317, lr: 0.02192542062590272
[batch_idx--450] train_loss: 0.0012628892286936917, acc: 0.8593657791509008, lr: 0.02186782098254747
train_loss:  0.0012628892286936917  acc:  0.8593657791509008
->>lr:0.021868
test_loss:  0.001229388863754296  test_acc:  0.8672291847623774
best acc:  86.47474872813004
Saving..

------Epoch: 81------
[batch_idx--0] train_loss: 0.0008872527396306396, acc: 0.91796875, lr: 0.02186666916089239
[batch_idx--50] train_loss: 0.001252264165611682, acc: 0.8614430147058824, lr: 0.021809086745365924
[batch_idx--100] train_loss: 0.0012638205212845218, acc: 0.8589882425742574, lr: 0.021751521533424346
[batch_idx--150] train_loss: 0.0012524503272409175, acc: 0.8604873758278145, lr: 0.021693973835426473
[batch_idx--200] train_loss: 0.0012525766526131116, acc: 0.8605410447761194, lr: 0.021636443961636685
[batch_idx--250] train_loss: 0.0012481504767405916, acc: 0.8615382221115537, lr: 0.021578932222223245
[batch_idx--300] train_loss: 0.0012484666993114416, acc: 0.8608674210963455, lr: 0.021521438927256664
[batch_idx--350] train_loss: 0.0012503464743057187, acc: 0.860855146011396, lr: 0.021463964386708015
[batch_idx--400] train_loss: 0.0012453454404377246, acc: 0.8616057512468828, lr: 0.021406508910447243
[batch_idx--450] train_loss: 0.0012514640487551784, acc: 0.8611361821779429, lr: 0.021349072808241526
train_loss:  0.0012514640487551784  acc:  0.8611361821779429
->>lr:0.021349
test_loss:  0.0012612597350521587  test_acc:  0.8612731108077926
best acc:  86.72291847623775

------Epoch: 82------
[batch_idx--0] train_loss: 0.001200388534925878, acc: 0.84765625, lr: 0.021347924285939714
[batch_idx--50] train_loss: 0.0012565504134103072, acc: 0.8588388480392157, lr: 0.02129050826428394
[batch_idx--100] train_loss: 0.0012516113831477222, acc: 0.8601098391089109, lr: 0.021233112242092567
[batch_idx--150] train_loss: 0.0012557470006097282, acc: 0.8604097682119205, lr: 0.02117573652881221
[batch_idx--200] train_loss: 0.001245478006538504, acc: 0.8621540733830846, lr: 0.02111838143378
[batch_idx--250] train_loss: 0.0012424337077956364, acc: 0.862222983067729, lr: 0.021061047266221912
[batch_idx--300] train_loss: 0.0012451384486893557, acc: 0.8618926495016611, lr: 0.021003734335251087
[batch_idx--350] train_loss: 0.0012460559315679654, acc: 0.8617232015669516, lr: 0.020946442949866164
[batch_idx--400] train_loss: 0.0012430803105060912, acc: 0.8619759195760599, lr: 0.020889173418949643
[batch_idx--450] train_loss: 0.0012460919769847637, acc: 0.8614572846877495, lr: 0.020831926051266162
train_loss:  0.0012460919769847637  acc:  0.8614572846877495
->>lr:0.020832
test_loss:  0.0012153705043730005  test_acc:  0.8731852587169624
best acc:  86.72291847623775
Saving..

------Epoch: 83------
[batch_idx--0] train_loss: 0.0010406714864075184, acc: 0.8984375, lr: 0.020830781332097453
[batch_idx--50] train_loss: 0.0012270572627255437, acc: 0.8667279411764706, lr: 0.020773556888877096
[batch_idx--100] train_loss: 0.0012249649585326118, acc: 0.8654470915841584, lr: 0.020716355232228204
[batch_idx--150] train_loss: 0.001233781889907473, acc: 0.8634623344370861, lr: 0.020659176670549492
[batch_idx--200] train_loss: 0.0012301239309559081, acc: 0.8636504975124378, lr: 0.020602021512115154
[batch_idx--250] train_loss: 0.0012395177252193638, acc: 0.862222983067729, lr: 0.02054489006507322
[batch_idx--300] train_loss: 0.001244014376376839, acc: 0.8614643895348837, lr: 0.02048778263744388
[batch_idx--350] train_loss: 0.0012459532807825235, acc: 0.8614004629629629, lr: 0.02043069953711782
[batch_idx--400] train_loss: 0.0012451560231287989, acc: 0.8615180798004988, lr: 0.020373641071854572
[batch_idx--450] train_loss: 0.0012443937920156784, acc: 0.8620908112611518, lr: 0.020316607549280843
train_loss:  0.0012443937920156784  acc:  0.8620908112611518
->>lr:0.020317
test_loss:  0.001388158343657472  test_acc:  0.8458865864251148
best acc:  87.31852587169624

------Epoch: 84------
[batch_idx--0] train_loss: 0.0011519941035658121, acc: 0.859375, lr: 0.020315467135356893
[batch_idx--50] train_loss: 0.0012140522263578925, acc: 0.8651194852941176, lr: 0.02025845937110404
[batch_idx--100] train_loss: 0.0012371208441517509, acc: 0.861618193069307, lr: 0.020201477170534773
[batch_idx--150] train_loss: 0.0012386189789098028, acc: 0.8617032284768212, lr: 0.020144520840864624
[batch_idx--200] train_loss: 0.001243503362899507, acc: 0.861221237562189, lr: 0.02008759068916964
[batch_idx--250] train_loss: 0.00124218335989269, acc: 0.8616627241035857, lr: 0.020030687022384752
[batch_idx--300] train_loss: 0.0012424618256761015, acc: 0.8612567483388704, lr: 0.019973810147302068
[batch_idx--350] train_loss: 0.0012404035693604999, acc: 0.8613782051282052, lr: 0.01991696037056928
[batch_idx--400] train_loss: 0.0012427295958495552, acc: 0.8611673940149626, lr: 0.01986013799868798
[batch_idx--450] train_loss: 0.0012471680671828074, acc: 0.8611882528552088, lr: 0.01980334333801198
train_loss:  0.0012471680671828074  acc:  0.8611882528552088
->>lr:0.019803
test_loss:  0.001273016052308812  test_acc:  0.8627621292964388
best acc:  87.31852587169624

------Epoch: 85------
[batch_idx--0] train_loss: 0.0012612538412213326, acc: 0.86328125, lr: 0.019802207729556022
[batch_idx--50] train_loss: 0.0012649876869046221, acc: 0.8591452205882353, lr: 0.0197454416497602
[batch_idx--100] train_loss: 0.001253071016318506, acc: 0.8612314356435643, lr: 0.019688703899547008
[batch_idx--150] train_loss: 0.0012541398489292687, acc: 0.8601510761589404, lr: 0.01963199478481404
[batch_idx--200] train_loss: 0.001244009482855005, acc: 0.8614155783582089, lr: 0.019575314611304503
[batch_idx--250] train_loss: 0.001247927636943342, acc: 0.8604643924302788, lr: 0.019518663684605593
[batch_idx--300] train_loss: 0.0012455980420697393, acc: 0.8608025332225914, lr: 0.01946204231014678
[batch_idx--350] train_loss: 0.0012456135495308458, acc: 0.8606882122507122, lr: 0.019405450793198248
[batch_idx--400] train_loss: 0.0012457435846124356, acc: 0.860612141521197, lr: 0.01934888943886919
[batch_idx--450] train_loss: 0.0012457030651867897, acc: 0.8613531433332177, lr: 0.019292358552106172
train_loss:  0.0012457030651867897  acc:  0.8613531433332177
->>lr:0.019292
test_loss:  0.001312704854160048  test_acc:  0.8586673284526616
best acc:  87.31852587169624

------Epoch: 86------
[batch_idx--0] train_loss: 0.0011573460651561618, acc: 0.85546875, lr: 0.019291228247233613
[batch_idx--50] train_loss: 0.0012426173574674655, acc: 0.8639705882352942, lr: 0.019234728751373553
[batch_idx--100] train_loss: 0.0012345989154061616, acc: 0.8640160891089109, lr: 0.01917826033856888
[batch_idx--150] train_loss: 0.0012274181261344116, acc: 0.8639538493377483, lr: 0.019121823313265066
[batch_idx--200] train_loss: 0.001228526991326362, acc: 0.863397854477612, lr: 0.01906541797973838
[batch_idx--250] train_loss: 0.0012284565159950806, acc: 0.8632345617529881, lr: 0.01900904464209422
[batch_idx--300] train_loss: 0.0012289879329586122, acc: 0.8622430440199336, lr: 0.018952703604265476
[batch_idx--350] train_loss: 0.0012329591239505365, acc: 0.8615896545584045, lr: 0.018896395170010898
[batch_idx--400] train_loss: 0.0012344914830192237, acc: 0.8614791147132169, lr: 0.01884011964291346
[batch_idx--450] train_loss: 0.0012372302314125022, acc: 0.8618044225361892, lr: 0.018783877326378724
train_loss:  0.0012372302314125022  acc:  0.8618044225361892
->>lr:0.018784
test_loss:  0.0012196045475690088  test_acc:  0.8700831368656161
best acc:  87.31852587169624

------Epoch: 87------
[batch_idx--0] train_loss: 0.001383556635119021, acc: 0.84765625, lr: 0.018782752820878636
[batch_idx--50] train_loss: 0.001236278513519495, acc: 0.8648131127450981, lr: 0.018726544691500546
[batch_idx--100] train_loss: 0.0012429549178975348, acc: 0.862082301980198, lr: 0.01867037038501655
[batch_idx--150] train_loss: 0.001235565075817339, acc: 0.863229511589404, lr: 0.018614230204286455
[batch_idx--200] train_loss: 0.0012343756348330212, acc: 0.8627176616915423, lr: 0.018558124451986114
[batch_idx--250] train_loss: 0.0012352784494479815, acc: 0.8627832420318725, lr: 0.018502053430605753
[batch_idx--300] train_loss: 0.0012337441169007054, acc: 0.8629568106312292, lr: 0.01844601744244835
[batch_idx--350] train_loss: 0.0012290710697754517, acc: 0.8630364138176638, lr: 0.018390016789628004
[batch_idx--400] train_loss: 0.0012270797383994738, acc: 0.8630084943890274, lr: 0.018334051774068303
[batch_idx--450] train_loss: 0.0012280969665302275, acc: 0.8633752213003784, lr: 0.0182781226975007
train_loss:  0.0012280969665302275  acc:  0.8633752213003784
->>lr:0.018278
test_loss:  0.0012350858093594954  test_acc:  0.8659883360218389
best acc:  87.31852587169624

------Epoch: 88------
[batch_idx--0] train_loss: 0.0013078234624117613, acc: 0.84765625, lr: 0.01827700448461836
[batch_idx--50] train_loss: 0.0012287644306471681, acc: 0.8627450980392157, lr: 0.018221112376465492
[batch_idx--100] train_loss: 0.001211947614516104, acc: 0.8649829826732673, lr: 0.018165256816209563
[batch_idx--150] train_loss: 0.0012067093433725912, acc: 0.8654025248344371, lr: 0.018109438104991903
[batch_idx--200] train_loss: 0.0012083415301697357, acc: 0.8655356032338308, lr: 0.01805365654375518
[batch_idx--250] train_loss: 0.001218501206643256, acc: 0.8641216384462151, lr: 0.017997912433241763
[batch_idx--300] train_loss: 0.001217781497239819, acc: 0.8645790074750831, lr: 0.017942206073992117
[batch_idx--350] train_loss: 0.0012214680376762465, acc: 0.863960113960114, lr: 0.017886537766343167
[batch_idx--400] train_loss: 0.0012247661776617726, acc: 0.863583229426434, lr: 0.017830907810426684
[batch_idx--450] train_loss: 0.0012299050893977898, acc: 0.8636095393480752, lr: 0.017775316506167683
train_loss:  0.0012299050893977898  acc:  0.8636095393480752
->>lr:0.017775
test_loss:  0.0012280726820352518  test_acc:  0.8708276461099392
best acc:  87.31852587169624

------Epoch: 89------
[batch_idx--0] train_loss: 0.0012724822154268622, acc: 0.86328125, lr: 0.017774205076388213
[batch_idx--50] train_loss: 0.0012311635689590784, acc: 0.8639705882352942, lr: 0.017718653505586445
[batch_idx--100] train_loss: 0.0012154482566696214, acc: 0.8666847153465347, lr: 0.01766314119165336
[batch_idx--150] train_loss: 0.0012182250884145696, acc: 0.8656612168874173, lr: 0.017607668433879703
[batch_idx--200] train_loss: 0.001226214687652719, acc: 0.8647193718905473, lr: 0.017552235531342955
[batch_idx--250] train_loss: 0.0012263926713118902, acc: 0.8647130229083665, lr: 0.017496842782905715
[batch_idx--300] train_loss: 0.001233112161160691, acc: 0.8636316445182725, lr: 0.017441490487214106
[batch_idx--350] train_loss: 0.0012320104631213232, acc: 0.8637486645299145, lr: 0.017386178942696135
[batch_idx--400] train_loss: 0.0012303884295012774, acc: 0.8638754675810474, lr: 0.01733090844756013
[batch_idx--450] train_loss: 0.0012306702031417438, acc: 0.8640955323358905, lr: 0.017275679299793074
train_loss:  0.0012306702031417438  acc:  0.8640955323358905
->>lr:0.017276
test_loss:  0.0012301231716773486  test_acc:  0.8702072217396699
best acc:  87.31852587169624

------Epoch: 90------
[batch_idx--0] train_loss: 0.0011281383922323585, acc: 0.88671875, lr: 0.017274575140626316
[batch_idx--50] train_loss: 0.0012000308477082382, acc: 0.8645833333333334, lr: 0.017219388473930602
[batch_idx--100] train_loss: 0.0012088136172318591, acc: 0.8637840346534653, lr: 0.017164243755855953
[batch_idx--150] train_loss: 0.0012103427638645154, acc: 0.8648075331125827, lr: 0.017109141283711254
[batch_idx--200] train_loss: 0.0012060030924147041, acc: 0.8647582400497512, lr: 0.0170540813545776
[batch_idx--250] train_loss: 0.0012132950457354704, acc: 0.8640593874501992, lr: 0.016999064265306747
[batch_idx--300] train_loss: 0.0012171568327345127, acc: 0.863969061461794, lr: 0.016944090312519478
[batch_idx--350] train_loss: 0.0012160275694303893, acc: 0.8643607549857549, lr: 0.01688915979260398
[batch_idx--400] train_loss: 0.0012155462675054378, acc: 0.8643722724438903, lr: 0.016834273001714316
[batch_idx--450] train_loss: 0.0012173469729891472, acc: 0.864624917554761, lr: 0.016779430235768767
train_loss:  0.0012173469729891472  acc:  0.864624917554761
->>lr:0.016779
test_loss:  0.0012061717784912123  test_acc:  0.8702072217396699
best acc:  87.31852587169624

------Epoch: 91------
[batch_idx--0] train_loss: 0.001460848841816187, acc: 0.8125, lr: 0.01677833383153542
[batch_idx--50] train_loss: 0.0012649053424669831, acc: 0.8571537990196079, lr: 0.016723536275641708
[batch_idx--100] train_loss: 0.001230373080718414, acc: 0.8627011138613861, lr: 0.01666878334172139
[batch_idx--150] train_loss: 0.0012194182367575978, acc: 0.8645488410596026, lr: 0.01661407532497108
[batch_idx--200] train_loss: 0.0012256847790080999, acc: 0.8641169154228856, lr: 0.016559412520345193
[batch_idx--250] train_loss: 0.0012304351845500152, acc: 0.8632034362549801, lr: 0.016504795222554415
[batch_idx--300] train_loss: 0.0012258960166559292, acc: 0.8638003529900332, lr: 0.01645022372606408
[batch_idx--350] train_loss: 0.0012236302318860727, acc: 0.863960113960114, lr: 0.01639569832509256
[batch_idx--400] train_loss: 0.0012238378246939576, acc: 0.8637975374064838, lr: 0.016341219313609757
[batch_idx--450] train_loss: 0.0012257039119179546, acc: 0.863696323810185, lr: 0.01628678698533542
train_loss:  0.0012257039119179546  acc:  0.863696323810185
->>lr:0.016287
test_loss:  0.0013303935586969793  test_acc:  0.854076188112669
best acc:  87.31852587169624

------Epoch: 92------
[batch_idx--0] train_loss: 0.0012373097706586123, acc: 0.85546875, lr: 0.016285698816954627
[batch_idx--50] train_loss: 0.0012248326640795259, acc: 0.8636642156862745, lr: 0.016231314407882003
[batch_idx--100] train_loss: 0.001223646149288236, acc: 0.8638613861386139, lr: 0.016176977274562434
[batch_idx--150] train_loss: 0.0012178208419917436, acc: 0.8638503725165563, lr: 0.016122687709950775
[batch_idx--200] train_loss: 0.001216300155339642, acc: 0.8644667288557214, lr: 0.01606844600674539
[batch_idx--250] train_loss: 0.0012128933535079914, acc: 0.8648219621513944, lr: 0.016014252457386614
[batch_idx--300] train_loss: 0.0012115432344766896, acc: 0.8652019310631229, lr: 0.015960107354055172
[batch_idx--350] train_loss: 0.0012082640632875583, acc: 0.8657963853276354, lr: 0.01590601098867059
[batch_idx--400] train_loss: 0.00121223468343934, acc: 0.8652489869077307, lr: 0.015851963652889624
[batch_idx--450] train_loss: 0.001215996562303836, acc: 0.8656663311000798, lr: 0.015797965638104688
train_loss:  0.001215996562303836  acc:  0.8656663311000798
->>lr:0.015798
test_loss:  0.0012236055195191048  test_acc:  0.868470033502916
best acc:  87.31852587169624

------Epoch: 93------
[batch_idx--0] train_loss: 0.0010506976395845413, acc: 0.8984375, lr: 0.01579688618288306
[batch_idx--50] train_loss: 0.0012281948184667557, acc: 0.8646599264705882, lr: 0.01574293877543077
[batch_idx--100] train_loss: 0.0012137959503780791, acc: 0.8660272277227723, lr: 0.015689041276774486
[batch_idx--150] train_loss: 0.0012123363396300878, acc: 0.8649368791390728, lr: 0.015635193977498796
[batch_idx--200] train_loss: 0.0012093957563380326, acc: 0.8660020211442786, lr: 0.015581397167917649
[batch_idx--250] train_loss: 0.0012031018061923286, acc: 0.8667361802788844, lr: 0.01552765113807274
[batch_idx--300] train_loss: 0.001209830784237328, acc: 0.8660584509966778, lr: 0.015473956177732033
[batch_idx--350] train_loss: 0.0012038957275864151, acc: 0.8671986289173789, lr: 0.01542031257638813
[batch_idx--400] train_loss: 0.0012034919682513932, acc: 0.8671387936408977, lr: 0.015366720623256752
[batch_idx--450] train_loss: 0.0012084005743427577, acc: 0.8668292428923525, lr: 0.015313180607275165
train_loss:  0.0012084005743427577  acc:  0.8668292428923525
->>lr:0.015313
test_loss:  0.0012669777390175562  test_acc:  0.8625139595483311
best acc:  87.31852587169624

------Epoch: 94------
[batch_idx--0] train_loss: 0.000991289271041751, acc: 0.90234375, lr: 0.015312110338697428
[batch_idx--50] train_loss: 0.0012091847602277994, acc: 0.8658854166666666, lr: 0.015258623595981367
[batch_idx--100] train_loss: 0.001211521237636778, acc: 0.8665686881188119, lr: 0.015205189373212608
[batch_idx--150] train_loss: 0.0012056355774001718, acc: 0.8667994619205298, lr: 0.015151807958478031
[batch_idx--200] train_loss: 0.0012022646865691637, acc: 0.867304104477612, lr: 0.015098479639579785
[batch_idx--250] train_loss: 0.0012040712492191697, acc: 0.8670785607569721, lr: 0.015045204704033754
[batch_idx--300] train_loss: 0.0012039557701530573, acc: 0.8670966569767442, lr: 0.01499198343906803
[batch_idx--350] train_loss: 0.0012030624374919743, acc: 0.8671541132478633, lr: 0.014938816131621303
[batch_idx--400] train_loss: 0.0012032450869170824, acc: 0.8674212905236908, lr: 0.014885703068341395
[batch_idx--450] train_loss: 0.0012041827222219787, acc: 0.867393341896067, lr: 0.014832644535583656
train_loss:  0.0012041827222219787  acc:  0.867393341896067
->>lr:0.014833
test_loss:  0.0012341281208451797  test_acc:  0.8627621292964388
best acc:  87.31852587169624

------Epoch: 95------
[batch_idx--0] train_loss: 0.0011620809091255069, acc: 0.87109375, lr: 0.014831583923105
[batch_idx--50] train_loss: 0.0012166342976521335, acc: 0.8643535539215687, lr: 0.014778581306178257
[batch_idx--100] train_loss: 0.0011986342148099206, acc: 0.8669167698019802, lr: 0.014725633797313148
[batch_idx--150] train_loss: 0.0011999452346319098, acc: 0.8668253311258278, lr: 0.014672741681972468
[batch_idx--200] train_loss: 0.0011947955154299513, acc: 0.867226368159204, lr: 0.01461990524532032
[batch_idx--250] train_loss: 0.0012027342083485448, acc: 0.8664560507968128, lr: 0.014567124772220653
[batch_idx--300] train_loss: 0.001201592207278289, acc: 0.8670058139534884, lr: 0.014514400547235684
[batch_idx--350] train_loss: 0.0012017226357747962, acc: 0.8671875, lr: 0.014461732854624376
[batch_idx--400] train_loss: 0.0011967608057475764, acc: 0.8678888715710723, lr: 0.014409121978340905
[batch_idx--450] train_loss: 0.0012012169801688564, acc: 0.8674974832505988, lr: 0.014356568202033099
train_loss:  0.0012012169801688564  acc:  0.8674974832505988
->>lr:0.014357
test_loss:  0.0012335314346404122  test_acc:  0.8679736940067005
best acc:  87.31852587169624

------Epoch: 96------
[batch_idx--0] train_loss: 0.0010964947286993265, acc: 0.875, lr: 0.014355517710873192
[batch_idx--50] train_loss: 0.0011963439767486324, acc: 0.8681066176470589, lr: 0.014303022468435317
[batch_idx--100] train_loss: 0.0011760016135631663, acc: 0.8708230198019802, lr: 0.014250584898001182
[batch_idx--150] train_loss: 0.0011843259986767558, acc: 0.8692829056291391, lr: 0.014198205282284266
[batch_idx--200] train_loss: 0.0011891711605203092, acc: 0.869072605721393, lr: 0.014145883903685591
[batch_idx--250] train_loss: 0.0011898176039595054, acc: 0.8689149651394422, lr: 0.014093621044292189
[batch_idx--300] train_loss: 0.001189029167183486, acc: 0.8694585755813954, lr: 0.014041416985875627
[batch_idx--350] train_loss: 0.001185431354579667, acc: 0.8697582799145299, lr: 0.013989272009890398
[batch_idx--400] train_loss: 0.0011892851613092693, acc: 0.8689993765586035, lr: 0.013937186397472484
[batch_idx--450] train_loss: 0.0011971018247332987, acc: 0.8687385010587705, lr: 0.01388516042943782
train_loss:  0.0011971018247332987  acc:  0.8687385010587705
->>lr:0.013885
test_loss:  0.0012446066991228784  test_acc:  0.8652438267775158
best acc:  87.31852587169624

------Epoch: 97------
[batch_idx--0] train_loss: 0.0011543661821633577, acc: 0.88671875, lr: 0.01388412052037681
[batch_idx--50] train_loss: 0.00122357356617702, acc: 0.8655024509803921, lr: 0.01383215567857612
[batch_idx--100] train_loss: 0.0012095970692952154, acc: 0.8661045792079208, lr: 0.013780251047424422
[batch_idx--150] train_loss: 0.0011991434175496465, acc: 0.8667735927152318, lr: 0.013728406906761904
[batch_idx--200] train_loss: 0.0011956049766465651, acc: 0.8670903296019901, lr: 0.013676623536102595
[batch_idx--250] train_loss: 0.0011894920290321616, acc: 0.8676076942231076, lr: 0.013624901214632912
[batch_idx--300] train_loss: 0.001195090766601648, acc: 0.8670058139534884, lr: 0.01357324022121012
[batch_idx--350] train_loss: 0.001196420928125039, acc: 0.8672542735042735, lr: 0.013521640834360844
[batch_idx--400] train_loss: 0.0011973888668443014, acc: 0.8673725841645885, lr: 0.013470103332279566
[batch_idx--450] train_loss: 0.0012007709324244023, acc: 0.8675929461589197, lr: 0.013418627992827087
train_loss:  0.0012007709324244023  acc:  0.8675929461589197
->>lr:0.013419
test_loss:  0.001197043006155949  test_acc:  0.8739297679612855
best acc:  87.31852587169624
Saving..

------Epoch: 98------
[batch_idx--0] train_loss: 0.0012297570938244462, acc: 0.84375, lr: 0.013417599122003463
[batch_idx--50] train_loss: 0.0011994745080158406, acc: 0.866421568627451, lr: 0.013366187474337023
[batch_idx--100] train_loss: 0.0011919969652484327, acc: 0.8673808787128713, lr: 0.013314838549554453
[batch_idx--150] train_loss: 0.0011925188229351446, acc: 0.867161630794702, lr: 0.013263552624499875
[batch_idx--200] train_loss: 0.001191802215716102, acc: 0.8680814676616916, lr: 0.01321232997567776
[batch_idx--250] train_loss: 0.0011954640708047436, acc: 0.8677477589641435, lr: 0.01316117087925142
[batch_idx--300] train_loss: 0.0011948452900052441, acc: 0.8681348629568106, lr: 0.013110075611041534
[batch_idx--350] train_loss: 0.0011937474000116445, acc: 0.8684450676638177, lr: 0.01305904444652466
[batch_idx--400] train_loss: 0.0011934852577395218, acc: 0.8685902431421446, lr: 0.013008077660831736
[batch_idx--450] train_loss: 0.0011930745009017486, acc: 0.8688079286284583, lr: 0.01295717552874661
train_loss:  0.0011930745009017486  acc:  0.8688079286284583
->>lr:0.012957
test_loss:  0.0012056371228306476  test_acc:  0.8705794763618315
best acc:  87.39297679612855

------Epoch: 99------
[batch_idx--0] train_loss: 0.0011935135116800666, acc: 0.87109375, lr: 0.012956158147457115
[batch_idx--50] train_loss: 0.001214529058950789, acc: 0.8647365196078431, lr: 0.012905322244772761
[batch_idx--100] train_loss: 0.00119234475256451, acc: 0.8686958539603961, lr: 0.012854551549694835
[batch_idx--150] train_loss: 0.001191770767759137, acc: 0.8679635761589404, lr: 0.012803846335949954
[batch_idx--200] train_loss: 0.0011847878305880882, acc: 0.8689171330845771, lr: 0.012753206876911727
[batch_idx--250] train_loss: 0.001179408997926208, acc: 0.869210657370518, lr: 0.012702633445599246
[batch_idx--300] train_loss: 0.001179056195696921, acc: 0.8696143064784053, lr: 0.012652126314675622
[batch_idx--350] train_loss: 0.0011784156019589798, acc: 0.8693687678062678, lr: 0.012601685756446507
[batch_idx--400] train_loss: 0.0011801527152035358, acc: 0.8689214463840399, lr: 0.012551312042858621
[batch_idx--450] train_loss: 0.001182221967927261, acc: 0.8696237025722915, lr: 0.012501005445498313
train_loss:  0.001182221967927261  acc:  0.8696237025722915
->>lr:0.012501
test_loss:  0.0012098750433979207  test_acc:  0.8672291847623774
best acc:  87.39297679612855

------Epoch: 100------
[batch_idx--0] train_loss: 0.001117747975513339, acc: 0.890625, lr: 0.012500000000000006
[batch_idx--50] train_loss: 0.0011848409342871723, acc: 0.8694852941176471, lr: 0.012449762140604802
[batch_idx--100] train_loss: 0.0011993927447733902, acc: 0.867496905940594, lr: 0.012399591944936346
[batch_idx--150] train_loss: 0.00120412658233911, acc: 0.8667735927152318, lr: 0.012349489683483728
[batch_idx--200] train_loss: 0.0011948712137689935, acc: 0.8684507151741293, lr: 0.012299455626369788
[batch_idx--250] train_loss: 0.001185436087912802, acc: 0.8700821713147411, lr: 0.012249490043349598
[batch_idx--300] train_loss: 0.0011862216699494055, acc: 0.8699127906976745, lr: 0.01219959320380913
[batch_idx--350] train_loss: 0.001185063408864158, acc: 0.8698473112535613, lr: 0.012149765376763677
[batch_idx--400] train_loss: 0.001177712163929723, acc: 0.8706943578553616, lr: 0.012100006830856476
[batch_idx--450] train_loss: 0.0011855747246697142, acc: 0.8703266567153817, lr: 0.01205031783435723
train_loss:  0.0011855747246697142  acc:  0.8703266567153817
->>lr:0.012050
test_loss:  0.00123687404485121  test_acc:  0.8649956570294081
best acc:  87.39297679612855

------Epoch: 101------
[batch_idx--0] train_loss: 0.0011545221786946058, acc: 0.875, lr: 0.012049324765671741
[batch_idx--50] train_loss: 0.001222320402031947, acc: 0.8635110294117647, lr: 0.01199970698555119
[batch_idx--100] train_loss: 0.001205591974194429, acc: 0.8656404702970297, lr: 0.011950159295598187
[batch_idx--150] train_loss: 0.0011947947154931774, acc: 0.8683257450331126, lr: 0.011900681962945607
[batch_idx--200] train_loss: 0.0011929267639785412, acc: 0.8681592039800995, lr: 0.011851275254347019
[batch_idx--250] train_loss: 0.0011920691581454083, acc: 0.8690239043824701, lr: 0.011801939436175232
[batch_idx--300] train_loss: 0.001186483483560433, acc: 0.8689135174418605, lr: 0.01175267477442085
[batch_idx--350] train_loss: 0.001183900943668055, acc: 0.8692240918803419, lr: 0.011703481534690844
[batch_idx--400] train_loss: 0.001184090133959618, acc: 0.869466957605985, lr: 0.011654359982207095
[batch_idx--450] train_loss: 0.00118757656586577, acc: 0.8693112785086958, lr: 0.011605310381805019
train_loss:  0.00118757656586577  acc:  0.8693112785086958
->>lr:0.011605
test_loss:  0.001191537314615677  test_acc:  0.8734334284650701
best acc:  87.39297679612855

------Epoch: 102------
[batch_idx--0] train_loss: 0.0011893486371263862, acc: 0.8671875, lr: 0.01160433012552509
[batch_idx--50] train_loss: 0.0011991933730029153, acc: 0.8650428921568627, lr: 0.011555354188677439
[batch_idx--100] train_loss: 0.0011655620615099473, acc: 0.8713644801980198, lr: 0.011506450737694249
[batch_idx--150] train_loss: 0.001162684060377223, acc: 0.871145488410596, lr: 0.011457620036235043
[batch_idx--200] train_loss: 0.0011640588564112485, acc: 0.871210354477612, lr: 0.011408862347567132
[batch_idx--250] train_loss: 0.0011654352394500338, acc: 0.8712338147410359, lr: 0.011360177934564142
[batch_idx--300] train_loss: 0.001165704829525512, acc: 0.8713792566445183, lr: 0.011311567059704701
[batch_idx--350] train_loss: 0.0011666565125229692, acc: 0.8713719729344729, lr: 0.01126302998507092
[batch_idx--400] train_loss: 0.001170950583060344, acc: 0.8709086658354115, lr: 0.011214566972347026
[batch_idx--450] train_loss: 0.0011768624540591027, acc: 0.8707952928107752, lr: 0.01116617828281797
train_loss:  0.0011768624540591027  acc:  0.8707952928107752
->>lr:0.011166
test_loss:  0.0012233277115074543  test_acc:  0.8677255242585928
best acc:  87.39297679612855

------Epoch: 103------
[batch_idx--0] train_loss: 0.0011423600371927023, acc: 0.87890625, lr: 0.011165211268916392
[batch_idx--50] train_loss: 0.001171677390469567, acc: 0.8716299019607843, lr: 0.011116898657806221
[batch_idx--100] train_loss: 0.0011734361379951387, acc: 0.8693533415841584, lr: 0.01106866089646273
[batch_idx--150] train_loss: 0.0011715019154146511, acc: 0.8700072433774835, lr: 0.011020498244956429
[batch_idx--200] train_loss: 0.0011721037517056167, acc: 0.8699471393034826, lr: 0.010972410962952879
[batch_idx--250] train_loss: 0.0011738279901177759, acc: 0.8708758715139442, lr: 0.010924399309711284
[batch_idx--300] train_loss: 0.0011714646920323545, acc: 0.8712624584717608, lr: 0.010876463544083113
[batch_idx--350] train_loss: 0.0011708775574694311, acc: 0.8714832621082621, lr: 0.010828603924510663
[batch_idx--400] train_loss: 0.0011703627428766852, acc: 0.871658743765586, lr: 0.010780820709025727
[batch_idx--450] train_loss: 0.0011710909356537342, acc: 0.8717238865553512, lr: 0.010733114155248157
train_loss:  0.0011710909356537342  acc:  0.8717238865553512
->>lr:0.010733
test_loss:  0.0011929093437937977  test_acc:  0.8739297679612855
best acc:  87.39297679612855

------Epoch: 104------
[batch_idx--0] train_loss: 0.0010667709866538644, acc: 0.88671875, lr: 0.010732160807889211
[batch_idx--50] train_loss: 0.0011813760205518966, acc: 0.8694852941176471, lr: 0.010684532714024482
[batch_idx--100] train_loss: 0.001161653933456481, acc: 0.8715965346534653, lr: 0.010636981800997085
[batch_idx--150] train_loss: 0.0011683924899233, acc: 0.8708867963576159, lr: 0.01058950832517444
[batch_idx--200] train_loss: 0.0011708285707044438, acc: 0.8704912935323383, lr: 0.010542112542506449
[batch_idx--250] train_loss: 0.0011769103100133756, acc: 0.8698798555776892, lr: 0.01049479470852415
[batch_idx--300] train_loss: 0.001171201514505605, acc: 0.8703670058139535, lr: 0.010447555078338317
[batch_idx--350] train_loss: 0.0011685233615588216, acc: 0.870826655982906, lr: 0.010400393906638123
[batch_idx--400] train_loss: 0.0011699328772686514, acc: 0.8708794420199502, lr: 0.010353311447689712
[batch_idx--450] train_loss: 0.001174091831246398, acc: 0.8706390807789773, lr: 0.01030630795533484
train_loss:  0.001174091831246398  acc:  0.8706390807789773
->>lr:0.010306
test_loss:  0.0011584553758949244  test_acc:  0.8759151259461472
best acc:  87.39297679612855
Saving..

------Epoch: 105------
[batch_idx--0] train_loss: 0.0014181133592501283, acc: 0.8359375, lr: 0.010305368692688175
[batch_idx--50] train_loss: 0.001202816325102878, acc: 0.8655790441176471, lr: 0.010258446007324987
[batch_idx--100] train_loss: 0.001182694403606808, acc: 0.8689665841584159, lr: 0.010211602800015714
[batch_idx--150] train_loss: 0.0011739501523760623, acc: 0.8700848509933775, lr: 0.010164839323312217
[batch_idx--200] train_loss: 0.0011734949637422182, acc: 0.8705495957711443, lr: 0.010118155829336504
[batch_idx--250] train_loss: 0.0011725751196119594, acc: 0.8708291832669323, lr: 0.010071552569779364
[batch_idx--300] train_loss: 0.0011711369801940786, acc: 0.8708082433554817, lr: 0.010025029795898983
[batch_idx--350] train_loss: 0.0011709906654021147, acc: 0.8702590811965812, lr: 0.00997858775851964
[batch_idx--400] train_loss: 0.0011667048632912802, acc: 0.870704099127182, lr: 0.00993222670803032
[batch_idx--450] train_loss: 0.00117281852891044, acc: 0.8706477592251883, lr: 0.009885946894383374
train_loss:  0.00117281852891044  acc:  0.8706477592251883
->>lr:0.009886
test_loss:  0.0012259615800386328  test_acc:  0.8688422881250776
best acc:  87.59151259461471

------Epoch: 106------
[batch_idx--0] train_loss: 0.0010014529107138515, acc: 0.91015625, lr: 0.00988502212844063
[batch_idx--50] train_loss: 0.0011677466670745144, acc: 0.8707107843137255, lr: 0.009838825433419594
[batch_idx--100] train_loss: 0.001158882156682966, acc: 0.8736076732673267, lr: 0.009792710478807351
[batch_idx--150] train_loss: 0.001158173993578341, acc: 0.8730080711920529, lr: 0.009746677513229449
[batch_idx--200] train_loss: 0.0011563787317315848, acc: 0.8733675373134329, lr: 0.00970072678486938
[batch_idx--250] train_loss: 0.0011527698825158001, acc: 0.8738172310756972, lr: 0.009654858541467274
[batch_idx--300] train_loss: 0.0011511368503363995, acc: 0.8739877491694352, lr: 0.009609073030318542
[batch_idx--350] train_loss: 0.0011532910082269025, acc: 0.8739204950142451, lr: 0.009563370498272567
[batch_idx--400] train_loss: 0.0011550688000158999, acc: 0.8737141521197007, lr: 0.009517751191731351
[batch_idx--450] train_loss: 0.0011629673135430826, acc: 0.8733207206581733, lr: 0.00947221535664816
train_loss:  0.0011629673135430826  acc:  0.8733207206581733
->>lr:0.009472
test_loss:  0.0012099624737747729  test_acc:  0.8687182032510237
best acc:  87.59151259461471

------Epoch: 107------
[batch_idx--0] train_loss: 0.001201819977723062, acc: 0.85546875, lr: 0.009471305493042243
[batch_idx--50] train_loss: 0.001167093827069609, acc: 0.8706341911764706, lr: 0.00942585505176061
[batch_idx--100] train_loss: 0.0011668904923077784, acc: 0.8715191831683168, lr: 0.009380488577388602
[batch_idx--150] train_loss: 0.0011753653363472726, acc: 0.8692052980132451, lr: 0.009335206314516387
[batch_idx--200] train_loss: 0.0011775752530081096, acc: 0.8691697761194029, lr: 0.0092900085072801
[batch_idx--250] train_loss: 0.0011694555189850112, acc: 0.8702689243027888, lr: 0.009244895399360565
[batch_idx--300] train_loss: 0.0011612652064969399, acc: 0.8718204941860465, lr: 0.009199867233981912
[batch_idx--350] train_loss: 0.0011649693603205502, acc: 0.8715277777777778, lr: 0.009154924253910346
[batch_idx--400] train_loss: 0.0011684043038847794, acc: 0.8712496103491272, lr: 0.009110066701452797
[batch_idx--450] train_loss: 0.0011658725498571538, acc: 0.871880098587149, lr: 0.0090652948184556
train_loss:  0.0011658725498571538  acc:  0.871880098587149
->>lr:0.009065
test_loss:  0.001148400320458285  test_acc:  0.876907804938578
best acc:  87.59151259461471
Saving..

------Epoch: 108------
[batch_idx--0] train_loss: 0.0011419932125136256, acc: 0.875, lr: 0.009064400256282757
[batch_idx--50] train_loss: 0.0011564988846087135, acc: 0.8726256127450981, lr: 0.009019716004806225
[batch_idx--100] train_loss: 0.0011661662028567626, acc: 0.8714805074257426, lr: 0.0089751179099095
[batch_idx--150] train_loss: 0.00115926263681212, acc: 0.8715852649006622, lr: 0.008930606212040063
[batch_idx--200] train_loss: 0.0011542617613843535, acc: 0.8730177238805971, lr: 0.00888618115117962
[batch_idx--250] train_loss: 0.0011550344455341598, acc: 0.8734125996015937, lr: 0.00884184296684277
[batch_idx--300] train_loss: 0.0011548616397782441, acc: 0.8734037583056479, lr: 0.00879759189807571
[batch_idx--350] train_loss: 0.001155034305947034, acc: 0.8735977564102564, lr: 0.008753428183455004
[batch_idx--400] train_loss: 0.0011557124745555946, acc: 0.873373207605985, lr: 0.008709352061086187
[batch_idx--450] train_loss: 0.0011561959075392664, acc: 0.8733554344430173, lr: 0.008665363768602597
train_loss:  0.0011561959075392664  acc:  0.8733554344430173
->>lr:0.008665
test_loss:  0.0011421847574940715  test_acc:  0.878520908301278
best acc:  87.6907804938578
Saving..

------Epoch: 109------
[batch_idx--0] train_loss: 0.0010435383301228285, acc: 0.87890625, lr: 0.008664484900247363
[batch_idx--50] train_loss: 0.0011555677708968812, acc: 0.8743872549019608, lr: 0.00862058643856546
[batch_idx--100] train_loss: 0.0011568375065517012, acc: 0.8734916460396039, lr: 0.0085767762853424
[batch_idx--150] train_loss: 0.001147805065894191, acc: 0.8748447847682119, lr: 0.008533054676777554
[batch_idx--200] train_loss: 0.0011441669697914996, acc: 0.8761271766169154, lr: 0.008489421848592919
[batch_idx--250] train_loss: 0.0011446485022246185, acc: 0.875980453187251, lr: 0.008445878036031801
[batch_idx--300] train_loss: 0.0011473246211916258, acc: 0.8748832018272426, lr: 0.008402423473857626
[batch_idx--350] train_loss: 0.0011499755837731701, acc: 0.8746216168091168, lr: 0.008359058396352615
[batch_idx--400] train_loss: 0.0011488575503715496, acc: 0.8745811253117207, lr: 0.008315783037316537
[batch_idx--450] train_loss: 0.0011494217804066114, acc: 0.8747092720519318, lr: 0.008272597630065468
train_loss:  0.0011494217804066114  acc:  0.8747092720519318
->>lr:0.008273
test_loss:  0.0011557964921249085  test_acc:  0.8752947015758779
best acc:  87.8520908301278

------Epoch: 110------
[batch_idx--0] train_loss: 0.0011059138923883438, acc: 0.85546875, lr: 0.008271734841028545
[batch_idx--50] train_loss: 0.0011716729278822302, acc: 0.8736979166666666, lr: 0.008228641424457422
[batch_idx--100] train_loss: 0.0011527356242133986, acc: 0.8756574876237624, lr: 0.00818563842948918
[batch_idx--150] train_loss: 0.0011570263235361024, acc: 0.875051738410596, lr: 0.008142726087971458
[batch_idx--200] train_loss: 0.0011497960991772884, acc: 0.8752526430348259, lr: 0.00809990463126313
[batch_idx--250] train_loss: 0.0011509098727126195, acc: 0.8747976842629482, lr: 0.008057174290233077
[batch_idx--300] train_loss: 0.0011509890869983853, acc: 0.8744549418604651, lr: 0.008014535295258926
[batch_idx--350] train_loss: 0.0011519112336008872, acc: 0.8740429131054132, lr: 0.00797198787622585
[batch_idx--400] train_loss: 0.0011467664732488611, acc: 0.8744739713216958, lr: 0.007929532262525246
[batch_idx--450] train_loss: 0.0011505455494580838, acc: 0.8742666712951713, lr: 0.007887168683053591
train_loss:  0.0011505455494580838  acc:  0.8742666712951713
->>lr:0.007887
test_loss:  0.0011386411581874884  test_acc:  0.8782727385531703
best acc:  87.8520908301278

------Epoch: 111------
[batch_idx--0] train_loss: 0.0011509250616654754, acc: 0.86328125, lr: 0.00788632235178279
[batch_idx--50] train_loss: 0.001144486844685732, acc: 0.8755361519607843, lr: 0.00784405288251919
[batch_idx--100] train_loss: 0.0011387815834991266, acc: 0.8750386757425742, lr: 0.007801875908340622
[batch_idx--150] train_loss: 0.0011455207960587611, acc: 0.8744567466887417, lr: 0.007759791656641275
[batch_idx--200] train_loss: 0.0011326515898270654, acc: 0.8759911380597015, lr: 0.007717800354315446
[batch_idx--250] train_loss: 0.001131991489161459, acc: 0.8755291334661355, lr: 0.007675902227756282
[batch_idx--300] train_loss: 0.0011365763184404082, acc: 0.8748053363787376, lr: 0.007634097502854598
[batch_idx--350] train_loss: 0.0011372357799918351, acc: 0.8745771011396012, lr: 0.007592386404997634
[batch_idx--400] train_loss: 0.0011382674427640315, acc: 0.8745421602244389, lr: 0.007550769159067847
[batch_idx--450] train_loss: 0.001137134762633516, acc: 0.8752039434859583, lr: 0.00750924598944171
train_loss:  0.001137134762633516  acc:  0.8752039434859583
->>lr:0.007509
test_loss:  0.0011439892624134254  test_acc:  0.8806303511601936
best acc:  87.8520908301278
Saving..

------Epoch: 112------
[batch_idx--0] train_loss: 0.0009059116127900779, acc: 0.9140625, lr: 0.007508416487165862
[batch_idx--50] train_loss: 0.001104828521527131, acc: 0.8795189950980392, lr: 0.007466989505996053
[batch_idx--100] train_loss: 0.001115379406088539, acc: 0.877552599009901, lr: 0.007425657052821996
[batch_idx--150] train_loss: 0.0011314226893418228, acc: 0.8754915149006622, lr: 0.007384419350484717
[batch_idx--200] train_loss: 0.0011316252958066576, acc: 0.8759717039800995, lr: 0.0073432766213143985
[batch_idx--250] train_loss: 0.0011394602094454119, acc: 0.8750933764940239, lr: 0.007302229087129178
[batch_idx--300] train_loss: 0.0011397770049431024, acc: 0.8751687084717608, lr: 0.007261276969233954
[batch_idx--350] train_loss: 0.0011384726148443642, acc: 0.8756120904558404, lr: 0.007220420488419194
[batch_idx--400] train_loss: 0.0011391854389777357, acc: 0.8754091334164589, lr: 0.007179659864959754
[batch_idx--450] train_loss: 0.0011437260206489663, acc: 0.8757333287048287, lr: 0.007138995318613667
train_loss:  0.0011437260206489663  acc:  0.8757333287048287
->>lr:0.007139
test_loss:  0.0011433487047789958  test_acc:  0.8808785209083013
best acc:  88.06303511601936
Saving..

------Epoch: 113------
[batch_idx--0] train_loss: 0.0008633069810457528, acc: 0.90234375, lr: 0.007138183009179922
[batch_idx--50] train_loss: 0.0011305328134410813, acc: 0.8778339460784313, lr: 0.007097616687346986
[batch_idx--100] train_loss: 0.0011325523529275513, acc: 0.8774365717821783, lr: 0.00705714688495745
[batch_idx--150] train_loss: 0.0011351161219321487, acc: 0.8776386589403974, lr: 0.007016773820201414
[batch_idx--200] train_loss: 0.0011348887355372643, acc: 0.8771766169154229, lr: 0.00697649771074741
[batch_idx--250] train_loss: 0.0011346236497916994, acc: 0.8771943476095617, lr: 0.006936318773741257
[batch_idx--300] train_loss: 0.0011374871250384323, acc: 0.8762069144518272, lr: 0.00689623722580488
[batch_idx--350] train_loss: 0.0011372807060243651, acc: 0.8760683760683761, lr: 0.006856253283035133
[batch_idx--400] train_loss: 0.0011395975120569999, acc: 0.8757305953865336, lr: 0.00681636716100264
[batch_idx--450] train_loss: 0.0011445106640077909, acc: 0.8758635053979935, lr: 0.006776579074750619
train_loss:  0.0011445106640077909  acc:  0.8758635053979935
->>lr:0.006777
test_loss:  0.0011551290665449139  test_acc:  0.8792654175456012
best acc:  88.08785209083013

------Epoch: 114------
[batch_idx--0] train_loss: 0.0011419266229495406, acc: 0.85546875, lr: 0.006775784314464717
[batch_idx--50] train_loss: 0.0011259411568936035, acc: 0.8724724264705882, lr: 0.006736096445698165
[batch_idx--100] train_loss: 0.0011431621450995382, acc: 0.8728341584158416, lr: 0.0066965070454860194
[batch_idx--150] train_loss: 0.0011458089679251395, acc: 0.8741204470198676, lr: 0.006657016327271753
[batch_idx--200] train_loss: 0.0011468801195084566, acc: 0.8738339552238806, lr: 0.006617624503966805
[batch_idx--250] train_loss: 0.0011438665078095794, acc: 0.8747043077689243, lr: 0.006578331787949427
[batch_idx--300] train_loss: 0.0011492656482742524, acc: 0.8741694352159468, lr: 0.006539138391063526
[batch_idx--350] train_loss: 0.0011491892677585878, acc: 0.8739872685185185, lr: 0.006500044524617568
[batch_idx--400] train_loss: 0.001148694270479718, acc: 0.8738602711970075, lr: 0.0064610503993833834
[batch_idx--450] train_loss: 0.0011493450575569765, acc: 0.8739802825702087, lr: 0.006422156225595066
train_loss:  0.0011493450575569765  acc:  0.8739802825702087
->>lr:0.006422
test_loss:  0.0011673791486345106  test_acc:  0.876907804938578
best acc:  88.08785209083013

------Epoch: 115------
[batch_idx--0] train_loss: 0.0012024207971990108, acc: 0.85546875, lr: 0.00642137936306515
[batch_idx--50] train_loss: 0.0011495376649477025, acc: 0.8746936274509803, lr: 0.006382587355775901
[batch_idx--100] train_loss: 0.0011378565698281153, acc: 0.8758895420792079, lr: 0.006343895722960522
[batch_idx--150] train_loss: 0.0011399109128535732, acc: 0.875025869205298, lr: 0.006305304673222229
[batch_idx--200] train_loss: 0.001134694431704557, acc: 0.8756024564676617, lr: 0.0062668144146219495
[batch_idx--250] train_loss: 0.001137286153542197, acc: 0.8764628984063745, lr: 0.006228425154677217
[batch_idx--300] train_loss: 0.0011337712767574182, acc: 0.8769855689368771, lr: 0.0061901371003610295
[batch_idx--350] train_loss: 0.0011343244317934937, acc: 0.8767917556980057, lr: 0.00615195045810075
[batch_idx--400] train_loss: 0.0011337453386504807, acc: 0.8769190305486284, lr: 0.006113865433776986
[batch_idx--450] train_loss: 0.0011374938978435686, acc: 0.877208664560697, lr: 0.006075882232722457
train_loss:  0.0011374938978435686  acc:  0.877208664560697
->>lr:0.006076
test_loss:  0.0011430862657277427  test_acc:  0.8813748604045167
best acc:  88.08785209083013
Saving..

------Epoch: 116------
[batch_idx--0] train_loss: 0.0012321497779339552, acc: 0.87109375, lr: 0.006075123608706093
[batch_idx--50] train_loss: 0.001151169472209671, acc: 0.8733915441176471, lr: 0.006037244478350651
[batch_idx--100] train_loss: 0.0011343472403252848, acc: 0.8765083539603961, lr: 0.00599946758436096
[batch_idx--150] train_loss: 0.0011294820135465471, acc: 0.8761899834437086, lr: 0.005961793130408489
[batch_idx--200] train_loss: 0.0011251233700565549, acc: 0.8762437810945274, lr: 0.005924221319612417
[batch_idx--250] train_loss: 0.0011262954808715954, acc: 0.8756536354581673, lr: 0.005886752354538527
[batch_idx--300] train_loss: 0.0011218197852995036, acc: 0.8767519725913622, lr: 0.0058493864371981065
[batch_idx--350] train_loss: 0.0011203078680109243, acc: 0.8770477207977208, lr: 0.005812123769046874
[batch_idx--400] train_loss: 0.0011271443821842832, acc: 0.8764124844139651, lr: 0.005774964550983902
[batch_idx--450] train_loss: 0.0011290292133600349, acc: 0.8762193216926442, lr: 0.005737908983350504
train_loss:  0.0011290292133600349  acc:  0.8762193216926442
->>lr:0.005738
test_loss:  0.0011675352900240347  test_acc:  0.8775282293088472
best acc:  88.13748604045166

------Epoch: 117------
[batch_idx--0] train_loss: 0.0012127758236601949, acc: 0.8671875, lr: 0.005737168930605272
[batch_idx--50] train_loss: 0.0010817524572998723, acc: 0.8831954656862745, lr: 0.005700219292222111
[batch_idx--100] train_loss: 0.0011027584798717041, acc: 0.8795250618811881, lr: 0.005663373707252357
[batch_idx--150] train_loss: 0.0011135491450773613, acc: 0.8783112582781457, lr: 0.005626632374346372
[batch_idx--200] train_loss: 0.0011208118148036857, acc: 0.8784981343283582, lr: 0.005589995491592479
[batch_idx--250] train_loss: 0.0011195637822796801, acc: 0.8782214890438247, lr: 0.005553463256515865
[batch_idx--300] train_loss: 0.0011203770943069551, acc: 0.877906976744186, lr: 0.005517035866077508
[batch_idx--350] train_loss: 0.0011158808938126767, acc: 0.8782941595441596, lr: 0.0054807135166731365
[batch_idx--400] train_loss: 0.0011160876257914093, acc: 0.8783802213216958, lr: 0.005444496404132135
[batch_idx--450] train_loss: 0.0011188410960682794, acc: 0.8780244385045302, lr: 0.005408384723716528
train_loss:  0.0011188410960682794  acc:  0.8780244385045302
->>lr:0.005408
test_loss:  0.0011349224503600342  test_acc:  0.8806303511601936
best acc:  88.13748604045166

------Epoch: 118------
[batch_idx--0] train_loss: 0.0012088780058547854, acc: 0.86328125, lr: 0.005407663566854007
[batch_idx--50] train_loss: 0.0011275007843733857, acc: 0.8780637254901961, lr: 0.005371659627775735
[batch_idx--100] train_loss: 0.0011255737292663296, acc: 0.8774752475247525, lr: 0.005335761513517229
[batch_idx--150] train_loss: 0.0011263540718513263, acc: 0.8765521523178808, lr: 0.0052999694176206515
[batch_idx--200] train_loss: 0.0011198128167011622, acc: 0.8780705845771144, lr: 0.005264283533056586
[batch_idx--250] train_loss: 0.001124690246464039, acc: 0.8774277888446215, lr: 0.0052287040522229575
[batch_idx--300] train_loss: 0.0011219244929528687, acc: 0.8777123131229236, lr: 0.005193231166944048
[batch_idx--350] train_loss: 0.0011216235817065117, acc: 0.8780604522792023, lr: 0.005157865068469426
[batch_idx--400] train_loss: 0.001119350576268142, acc: 0.87803927680798, lr: 0.005122605947472936
[batch_idx--450] train_loss: 0.0011233249353647405, acc: 0.8784323254764467, lr: 0.0050874539940516635
train_loss:  0.0011233249353647405  acc:  0.8784323254764467
->>lr:0.005087
test_loss:  0.0011672449438843216  test_acc:  0.8767837200645241
best acc:  88.13748604045166

------Epoch: 119------
[batch_idx--0] train_loss: 0.0013686342863366008, acc: 0.8515625, lr: 0.005086752049395094
[batch_idx--50] train_loss: 0.0011430470268333367, acc: 0.8747702205882353, lr: 0.005051709602139323
[batch_idx--100] train_loss: 0.0011178643372380128, acc: 0.8789449257425742, lr: 0.005016774704691432
[batch_idx--150] train_loss: 0.0011113512939588469, acc: 0.8800186258278145, lr: 0.004981947545400465
[batch_idx--200] train_loss: 0.0011115689450337445, acc: 0.8794115360696517, lr: 0.004947228312034608
[batch_idx--250] train_loss: 0.001114471847761007, acc: 0.8790151892430279, lr: 0.004912617191780173
[batch_idx--300] train_loss: 0.0011181129696485013, acc: 0.8784390573089701, lr: 0.004878114371240583
[batch_idx--350] train_loss: 0.001120547242953503, acc: 0.8778378739316239, lr: 0.004843720036435387
[batch_idx--400] train_loss: 0.001117231271042723, acc: 0.8780587593516209, lr: 0.004809434372799207
[batch_idx--450] train_loss: 0.0011212376183675503, acc: 0.8778595480265213, lr: 0.004775257565180805
train_loss:  0.0011212376183675503  acc:  0.8778595480265213
->>lr:0.004775
test_loss:  0.0011333580540137428  test_acc:  0.8810026057823551
best acc:  88.13748604045166

------Epoch: 120------
[batch_idx--0] train_loss: 0.0010261454153805971, acc: 0.88671875, lr: 0.004774575140626317
[batch_idx--50] train_loss: 0.0011059971453220236, acc: 0.8794424019607843, lr: 0.004740509555968578
[batch_idx--100] train_loss: 0.0011109457037752808, acc: 0.8774365717821783, lr: 0.004706553198931926
[batch_idx--150] train_loss: 0.0011078518482282855, acc: 0.8780784354304636, lr: 0.004672706252589681
[batch_idx--200] train_loss: 0.0011138542652574938, acc: 0.8773126554726368, lr: 0.004638968899425284
[batch_idx--250] train_loss: 0.0011152590831845584, acc: 0.8775522908366534, lr: 0.004605341321331294
[batch_idx--300] train_loss: 0.0011154587662537455, acc: 0.8773359634551495, lr: 0.004571823699608443
[batch_idx--350] train_loss: 0.0011155655832451761, acc: 0.8771367521367521, lr: 0.0045384162149646355
[batch_idx--400] train_loss: 0.0011146608663158506, acc: 0.8772015274314214, lr: 0.004505119047513989
[batch_idx--450] train_loss: 0.0011153747404085567, acc: 0.8775384455167147, lr: 0.0044719323767758445
train_loss:  0.0011153747404085567  acc:  0.8775384455167147
->>lr:0.004472
test_loss:  0.0011181523888204896  test_acc:  0.880258096538032
best acc:  88.13748604045166

------Epoch: 121------
[batch_idx--0] train_loss: 0.0011257854057475924, acc: 0.8828125, lr: 0.004471269771657399
[batch_idx--50] train_loss: 0.0011165557228320954, acc: 0.8784466911764706, lr: 0.004438195991889007
[batch_idx--100] train_loss: 0.001108311991539258, acc: 0.8793703589108911, lr: 0.004405233069644058
[batch_idx--150] train_loss: 0.0011185435678452156, acc: 0.8779490894039735, lr: 0.004372381182639854
[batch_idx--200] train_loss: 0.001117541481664321, acc: 0.8780122823383084, lr: 0.004339640507995033
[batch_idx--250] train_loss: 0.001111168976951079, acc: 0.8786728087649402, lr: 0.00430701122222864
[batch_idx--300] train_loss: 0.0011095115700287999, acc: 0.8791268687707641, lr: 0.004274493501259191
[batch_idx--350] train_loss: 0.0011089390962639363, acc: 0.8792289886039886, lr: 0.004242087520403681
[batch_idx--400] train_loss: 0.0011070253495289537, acc: 0.8794322786783042, lr: 0.004209793454376692
[batch_idx--450] train_loss: 0.001113484064347585, acc: 0.8789790675877391, lr: 0.0041776114772894115
train_loss:  0.001113484064347585  acc:  0.8789790675877391
->>lr:0.004178
test_loss:  0.0011286245528869733  test_acc:  0.8822434545228937
best acc:  88.13748604045166
Saving..

------Epoch: 122------
[batch_idx--0] train_loss: 0.0010275199310854077, acc: 0.8984375, lr: 0.004176968982247515
[batch_idx--50] train_loss: 0.0011074606399964907, acc: 0.8794424019607843, lr: 0.004144901514621361
[batch_idx--100] train_loss: 0.0011027766752407177, acc: 0.8795637376237624, lr: 0.0041129464857952575
[batch_idx--150] train_loss: 0.0011086553163265206, acc: 0.8783888658940397, lr: 0.004081104068052505
[batch_idx--200] train_loss: 0.0011102647534502086, acc: 0.8781677549751243, lr: 0.00404937443306925
[batch_idx--250] train_loss: 0.0011176742676857012, acc: 0.8772565986055777, lr: 0.004017757751913604
[batch_idx--300] train_loss: 0.0011126673409090288, acc: 0.877906976744186, lr: 0.003986254195044678
[batch_idx--350] train_loss: 0.001111489339722347, acc: 0.8780381944444444, lr: 0.003954863932311695
[batch_idx--400] train_loss: 0.0011121662835267714, acc: 0.8782146197007481, lr: 0.003923587132953052
[batch_idx--450] train_loss: 0.0011147159087170448, acc: 0.8786926788627764, lr: 0.003892423965595415
train_loss:  0.0011147159087170448  acc:  0.8786926788627764
->>lr:0.003892
test_loss:  0.0011210660937258736  test_acc:  0.8817471150266782
best acc:  88.22434545228937

------Epoch: 123------
[batch_idx--0] train_loss: 0.0011259119492024183, acc: 0.88671875, lr: 0.003891801862449629
[batch_idx--50] train_loss: 0.0011439023359094327, acc: 0.8753063725490197, lr: 0.003860754772816863
[batch_idx--100] train_loss: 0.0011282086903736512, acc: 0.876044245049505, lr: 0.0038298216539413596
[batch_idx--150] train_loss: 0.0011249836453310653, acc: 0.8765521523178808, lr: 0.0037990026725968685
[batch_idx--200] train_loss: 0.0011184893499837437, acc: 0.8767101990049752, lr: 0.003768297994941761
[batch_idx--250] train_loss: 0.0011222187225128045, acc: 0.8765562749003984, lr: 0.0037377077865181603
[batch_idx--300] train_loss: 0.0011248153926761344, acc: 0.8764924210963455, lr: 0.003707232212251013
[batch_idx--350] train_loss: 0.0011180704796190594, acc: 0.8773927172364673, lr: 0.003676871436447249
[batch_idx--400] train_loss: 0.001115485714740373, acc: 0.8776204021197007, lr: 0.0036466256227948553
[batch_idx--450] train_loss: 0.0011189562990575057, acc: 0.878206685874961, lr: 0.003616494934362016
train_loss:  0.0011189562990575057  acc:  0.878206685874961
->>lr:0.003616
test_loss:  0.0011568366168435505  test_acc:  0.8787690780493858
best acc:  88.22434545228937

------Epoch: 124------
[batch_idx--0] train_loss: 0.0011377374175935984, acc: 0.8671875, lr: 0.003615893495987335
[batch_idx--50] train_loss: 0.0011124615762930583, acc: 0.8782169117647058, lr: 0.003585880402627581
[batch_idx--100] train_loss: 0.0011002431481864563, acc: 0.8803372524752475, lr: 0.0035559827619909834
[batch_idx--150] train_loss: 0.0010974062028470516, acc: 0.8804842715231788, lr: 0.0035262007352685563
[batch_idx--200] train_loss: 0.0010982458065588268, acc: 0.8800334266169154, lr: 0.003496534483028016
[batch_idx--250] train_loss: 0.0011084581798580717, acc: 0.8789529382470119, lr: 0.0034669841652128805
[batch_idx--300] train_loss: 0.0011102805351931291, acc: 0.878374169435216, lr: 0.00343754994114161
[batch_idx--350] train_loss: 0.0011063966216211688, acc: 0.8790286680911681, lr: 0.0034082319695067616
[batch_idx--400] train_loss: 0.0011044452901688077, acc: 0.8793056421446384, lr: 0.003379030408374123
[batch_idx--450] train_loss: 0.0011109948749510289, acc: 0.8792480994202798, lr: 0.00334994541518186
train_loss:  0.0011109948749510289  acc:  0.8792480994202798
->>lr:0.003350
test_loss:  0.0011238130088005724  test_acc:  0.8793895024196551
best acc:  88.22434545228937

------Epoch: 125------
[batch_idx--0] train_loss: 0.0011736255837604403, acc: 0.875, lr: 0.003349364905389038
[batch_idx--50] train_loss: 0.0011109252341602947, acc: 0.8778339460784313, lr: 0.003320398973036992
[batch_idx--100] train_loss: 0.0011115932507477333, acc: 0.8780940594059405, lr: 0.0032915499247325843
[batch_idx--150] train_loss: 0.0011194961674400414, acc: 0.8772506208609272, lr: 0.003262817916013447
[batch_idx--200] train_loss: 0.0011165014529526604, acc: 0.8780122823383084, lr: 0.0032342031017861847
[batch_idx--250] train_loss: 0.0011142631475553955, acc: 0.8785794322709163, lr: 0.003205705636325565
[batch_idx--300] train_loss: 0.001114048737107445, acc: 0.8787115863787376, lr: 0.0031773256732736594
[batch_idx--350] train_loss: 0.001109804177857097, acc: 0.8793068910256411, lr: 0.0031490633656390573
[batch_idx--400] train_loss: 0.0011094581776187103, acc: 0.8796368453865336, lr: 0.0031209188657960046
[batch_idx--450] train_loss: 0.00111036405826503, acc: 0.8796993786232513, lr: 0.0030928923254835983
train_loss:  0.00111036405826503  acc:  0.8796993786232513
->>lr:0.003093
test_loss:  0.001152949144444109  test_acc:  0.8775282293088472
best acc:  88.22434545228937

------Epoch: 126------
[batch_idx--0] train_loss: 0.001394940889440477, acc: 0.84375, lr: 0.0030923329989034107
[batch_idx--50] train_loss: 0.0010877206074713054, acc: 0.8797487745098039, lr: 0.003064426932974329
[batch_idx--100] train_loss: 0.0010930483412726017, acc: 0.8790222772277227, lr: 0.003036639131148192
[batch_idx--150] train_loss: 0.0010878657182024805, acc: 0.8803807947019867, lr: 0.003008969743240972
[batch_idx--200] train_loss: 0.001089700688237315, acc: 0.8804221082089553, lr: 0.002981418918430234
[batch_idx--250] train_loss: 0.0010950153993175501, acc: 0.879840014940239, lr: 0.002953986805254319
[batch_idx--300] train_loss: 0.0010947112123206556, acc: 0.8803467607973422, lr: 0.002926673551611542
[batch_idx--350] train_loss: 0.0010972693325365308, acc: 0.8797743055555556, lr: 0.002899479304759398
[batch_idx--400] train_loss: 0.001100230285609861, acc: 0.8791595230673317, lr: 0.0028724042113137424
[batch_idx--450] train_loss: 0.0011033738092052017, acc: 0.879091887388482, lr: 0.002845448417248059
train_loss:  0.0011033738092052017  acc:  0.879091887388482
->>lr:0.002845
test_loss:  0.0011122443788710444  test_acc:  0.8847251520039707
best acc:  88.22434545228937
Saving..

------Epoch: 127------
[batch_idx--0] train_loss: 0.0011639234144240618, acc: 0.875, lr: 0.0028449105192196373
[batch_idx--50] train_loss: 0.001111647091331143, acc: 0.8760723039215687, lr: 0.0028180765602363435
[batch_idx--100] train_loss: 0.0010962851770858436, acc: 0.8796410891089109, lr: 0.0027913621935367313
[batch_idx--150] train_loss: 0.0011003048379677317, acc: 0.8793460264900662, lr: 0.002764767563149445
[batch_idx--200] train_loss: 0.0011060638619651694, acc: 0.8785758706467661, lr: 0.002738292812457563
[batch_idx--250] train_loss: 0.001103823940293767, acc: 0.8790774402390438, lr: 0.00271193808419784
[batch_idx--300] train_loss: 0.0011068100194397237, acc: 0.8788413621262459, lr: 0.002685703520459948
[batch_idx--350] train_loss: 0.0011081624539977634, acc: 0.8788172186609686, lr: 0.0026595892626856965
[batch_idx--400] train_loss: 0.0011067013484147143, acc: 0.8792861596009975, lr: 0.0026335954516682747
[batch_idx--450] train_loss: 0.00110870355592572, acc: 0.8801246224875898, lr: 0.0026077222275514957
train_loss:  0.00110870355592572  acc:  0.8801246224875898
->>lr:0.002608
test_loss:  0.0011144339786834376  test_acc:  0.8842288125077553
best acc:  88.47251520039707

------Epoch: 128------
[batch_idx--0] train_loss: 0.0011206433409824967, acc: 0.88671875, lr: 0.0026072059940146775
[batch_idx--50] train_loss: 0.0010755647569620872, acc: 0.8822763480392157, lr: 0.002581455912238459
[batch_idx--100] train_loss: 0.0010839412876377159, acc: 0.8812267945544554, lr: 0.0025558266984695406
[batch_idx--150] train_loss: 0.001094222426513173, acc: 0.8793977649006622, lr: 0.002530318490886033
[batch_idx--200] train_loss: 0.0010917091001385814, acc: 0.8796253109452736, lr: 0.0025049314270136516
[batch_idx--250] train_loss: 0.0010892566621852704, acc: 0.8796532619521913, lr: 0.0024796656437249655
[batch_idx--300] train_loss: 0.00108805755532325, acc: 0.8802299626245847, lr: 0.0024545212772386807
[batch_idx--350] train_loss: 0.001084574276350566, acc: 0.8809539707977208, lr: 0.0024294984631188735
[batch_idx--400] train_loss: 0.0010860426199884895, acc: 0.8808642456359103, lr: 0.0024045973362742893
[batch_idx--450] train_loss: 0.001090082965684408, acc: 0.880844933523102, lr: 0.0023798180309576172
train_loss:  0.001090082965684408  acc:  0.880844933523102
->>lr:0.002380
test_loss:  0.0011173837503527188  test_acc:  0.8834843032634322
best acc:  88.47251520039707

------Epoch: 129------
[batch_idx--0] train_loss: 0.0009599446784704924, acc: 0.890625, lr: 0.0023793236883495163
[batch_idx--50] train_loss: 0.0010689862766394428, acc: 0.8838848039215687, lr: 0.002354668778617275
[batch_idx--100] train_loss: 0.0010756972421071317, acc: 0.8838954207920792, lr: 0.0023301359595992556
[batch_idx--150] train_loss: 0.0010840946119216991, acc: 0.882786630794702, lr: 0.0023057253635624278
[batch_idx--200] train_loss: 0.001091079054625047, acc: 0.8814715485074627, lr: 0.0022814371221148186
[batch_idx--250] train_loss: 0.001088336358019392, acc: 0.8815363545816733, lr: 0.0022572713662047635
[batch_idx--300] train_loss: 0.0010928417853020948, acc: 0.8810086171096345, lr: 0.0022332282261202457
[batch_idx--350] train_loss: 0.0010923421616663366, acc: 0.8811320334757835, lr: 0.0022093078314881667
[batch_idx--400] train_loss: 0.0010943200733591605, acc: 0.8808837281795511, lr: 0.002185510311273653
[batch_idx--450] train_loss: 0.0010992744663706276, acc: 0.8806626861526712, lr: 0.0021618357937793764
train_loss:  0.0010992744663706276  acc:  0.8806626861526712
->>lr:0.002162
test_loss:  0.0011088233152206815  test_acc:  0.8848492368780245
best acc:  88.47251520039707
Saving..

------Epoch: 130------
[batch_idx--0] train_loss: 0.001110445591621101, acc: 0.859375, lr: 0.0021613635589349755
[batch_idx--50] train_loss: 0.0010911472650774407, acc: 0.8818167892156863, lr: 0.0021378146357049848
[batch_idx--100] train_loss: 0.0010892476201850442, acc: 0.8813428217821783, lr: 0.0021143889723431067
[batch_idx--150] train_loss: 0.0010909874532649307, acc: 0.8810533940397351, lr: 0.0020910866951471623
[batch_idx--200] train_loss: 0.0010900562495892098, acc: 0.8815104166666666, lr: 0.0020679079297497515
[batch_idx--250] train_loss: 0.0010875922025262983, acc: 0.8813651643426295, lr: 0.0020448528011175527
[batch_idx--300] train_loss: 0.0010935787985215824, acc: 0.8808009759136213, lr: 0.0020219214335506857
[batch_idx--350] train_loss: 0.0010914640893172631, acc: 0.8811654202279202, lr: 0.0019991139506819965
[batch_idx--400] train_loss: 0.0010911442572831588, acc: 0.8809811408977556, lr: 0.001976430475476429
[batch_idx--450] train_loss: 0.0010972009831362167, acc: 0.8807060783837262, lr: 0.0019538711302303584
train_loss:  0.0010972009831362167  acc:  0.8807060783837262
->>lr:0.001954
test_loss:  0.0011226001885174255  test_acc:  0.8824916242710014
best acc:  88.48492368780245

------Epoch: 131------
[batch_idx--0] train_loss: 0.0011018470395356417, acc: 0.87109375, lr: 0.00195342121028749
[batch_idx--50] train_loss: 0.0010973387443478785, acc: 0.8804381127450981, lr: 0.001930988602895767
[batch_idx--100] train_loss: 0.0011014188628293352, acc: 0.8796410891089109, lr: 0.001908680370460203
[batch_idx--150] train_loss: 0.0010896318238259397, acc: 0.8814673013245033, lr: 0.0018864966332540673
[batch_idx--200] train_loss: 0.0010950164232328904, acc: 0.8805970149253731, lr: 0.0018644375108794243
[batch_idx--250] train_loss: 0.0010913169531864058, acc: 0.8811472858565738, lr: 0.0018425031222664813
[batch_idx--300] train_loss: 0.0010947686735311665, acc: 0.8804765365448505, lr: 0.0018206935856729584
[batch_idx--350] train_loss: 0.0010955838393353647, acc: 0.8806423611111112, lr: 0.0017990090186834396
[batch_idx--400] train_loss: 0.0010916968411882118, acc: 0.8811370012468828, lr: 0.0017774495382087503
[batch_idx--450] train_loss: 0.0010951152349069166, acc: 0.8813048911722845, lr: 0.001756015260485311
train_loss:  0.0010951152349069166  acc:  0.8813048911722845
->>lr:0.001756
test_loss:  0.0011235191939205193  test_acc:  0.8816230301526244
best acc:  88.48492368780245

------Epoch: 132------
[batch_idx--0] train_loss: 0.0010679379338398576, acc: 0.890625, lr: 0.0017555878527937164
[batch_idx--50] train_loss: 0.0010933377608821234, acc: 0.8826593137254902, lr: 0.0017342814009232738
[batch_idx--100] train_loss: 0.001087019296644237, acc: 0.8830445544554455, lr: 0.0017131003845420657
[batch_idx--150] train_loss: 0.0010833645016664257, acc: 0.8825279387417219, lr: 0.0016920449178460556
[batch_idx--200] train_loss: 0.0010902110761754325, acc: 0.8814326803482587, lr: 0.0016711151143543159
[batch_idx--250] train_loss: 0.001086727000144032, acc: 0.8813807270916335, lr: 0.0016503110869084154
[batch_idx--300] train_loss: 0.0010847970207091854, acc: 0.8815017649501661, lr: 0.0016296329476718054
[batch_idx--350] train_loss: 0.0010853833754406314, acc: 0.8813879985754985, lr: 0.0016090808081292208
[batch_idx--400] train_loss: 0.0010868185929933149, acc: 0.8813707917705735, lr: 0.0015886547790860768
[batch_idx--450] train_loss: 0.0010896873461137475, acc: 0.8814784600965043, lr: 0.0015683549706678873
train_loss:  0.0010896873461137475  acc:  0.8814784600965043
->>lr:0.001568
test_loss:  0.0011091112114238538  test_acc:  0.8838565578855937
best acc:  88.48492368780245

------Epoch: 133------
[batch_idx--0] train_loss: 0.0011261377949267626, acc: 0.859375, lr: 0.0015679502627027138
[batch_idx--50] train_loss: 0.0011002578722386091, acc: 0.8785998774509803, lr: 0.0015477793120675237
[batch_idx--100] train_loss: 0.001087856181035058, acc: 0.8819616336633663, lr: 0.0015277348024344995
[batch_idx--150] train_loss: 0.0010907155182520976, acc: 0.8807429635761589, lr: 0.001507816841872195
[batch_idx--200] train_loss: 0.0010875666618870748, acc: 0.8814326803482587, lr: 0.0014880255377669072
[batch_idx--250] train_loss: 0.0010817860307884347, acc: 0.8823923057768924, lr: 0.0014683609968220602
[batch_idx--300] train_loss: 0.0010799988872213814, acc: 0.8828254775747508, lr: 0.0014488233250576467
[batch_idx--350] train_loss: 0.0010811311376554378, acc: 0.8827012108262108, lr: 0.0014294126278096538
[batch_idx--400] train_loss: 0.001085162809306751, acc: 0.8822864713216958, lr: 0.0014101290097294967
[batch_idx--450] train_loss: 0.0010900863744019759, acc: 0.882736834797098, lr: 0.0013909725747834447
train_loss:  0.0010900863744019759  acc:  0.882736834797098
->>lr:0.001391
test_loss:  0.0011065104623366887  test_acc:  0.8833602183893783
best acc:  88.48492368780245

------Epoch: 134------
[batch_idx--0] train_loss: 0.0009525324567221105, acc: 0.89453125, lr: 0.001390590744062975
[batch_idx--50] train_loss: 0.0011023070261466737, acc: 0.8771446078431373, lr: 0.0013715641423086456
[batch_idx--100] train_loss: 0.0011019558019908422, acc: 0.8777846534653465, lr: 0.0013526649316081968
[batch_idx--150] train_loss: 0.001103769265815295, acc: 0.8782077814569537, lr: 0.0013338932138554
[batch_idx--200] train_loss: 0.0010954904594732012, acc: 0.8800528606965174, lr: 0.0013152490902566538
[batch_idx--250] train_loss: 0.001090184160446993, acc: 0.8807737798804781, lr: 0.0012967326613304436
[batch_idx--300] train_loss: 0.0010924926588460307, acc: 0.8804116486710963, lr: 0.0012783440269067943
[batch_idx--350] train_loss: 0.001091799374896651, acc: 0.8807536502849003, lr: 0.0012600832861267342
[batch_idx--400] train_loss: 0.0010900642109414715, acc: 0.8807083852867831, lr: 0.0012419505374417718
[batch_idx--450] train_loss: 0.0010949710304674109, acc: 0.8802634776269657, lr: 0.0012239458786133446
train_loss:  0.0010949710304674109  acc:  0.8802634776269657
->>lr:0.001224
test_loss:  0.001102987786412668  test_acc:  0.8852214915001861
best acc:  88.48492368780245
Saving..

------Epoch: 135------
[batch_idx--0] train_loss: 0.0009900344302877784, acc: 0.90234375, lr: 0.0012235870926211618
[batch_idx--50] train_loss: 0.001116681232244945, acc: 0.8783700980392157, lr: 0.0012057131854440546
[batch_idx--100] train_loss: 0.00110061711166054, acc: 0.881458849009901, lr: 0.0011879675634946297
[batch_idx--150] train_loss: 0.0010906583895530576, acc: 0.8822692466887417, lr: 0.0011703503224471529
[batch_idx--200] train_loss: 0.0010933231073547507, acc: 0.8806941853233831, lr: 0.001152861557283752
[batch_idx--250] train_loss: 0.0010956768470582556, acc: 0.8805870268924303, lr: 0.0011355013622938726
[batch_idx--300] train_loss: 0.0010913183148393401, acc: 0.8810605274086378, lr: 0.0011182698310737928
[batch_idx--350] train_loss: 0.0010900602090912752, acc: 0.8814325142450142, lr: 0.0011011670565261005
[batch_idx--400] train_loss: 0.0010894702720174803, acc: 0.8815266521197007, lr: 0.0010841931308592041
[batch_idx--450] train_loss: 0.001087441564549773, acc: 0.8822248064706495, lr: 0.00106734814558683
train_loss:  0.001087441564549773  acc:  0.8822248064706495
->>lr:0.001067
test_loss:  0.0011037368216900956  test_acc:  0.8850974066261322
best acc:  88.5221491500186

------Epoch: 136------
[batch_idx--0] train_loss: 0.001030297251418233, acc: 0.90625, lr: 0.001067012561698319
[batch_idx--50] train_loss: 0.001083800036177112, acc: 0.8838848039215687, lr: 0.0010502991891848613
[batch_idx--100] train_loss: 0.0010824176358372563, acc: 0.8825804455445545, lr: 0.0010337149398027378
[batch_idx--150] train_loss: 0.0010772387513317265, acc: 0.8836920529801324, lr: 0.001017259902964765
[batch_idx--200] train_loss: 0.0010769433244618017, acc: 0.8833566542288557, lr: 0.0010009341673871241
[batch_idx--250] train_loss: 0.0010821430325189019, acc: 0.8829058764940239, lr: 0.000984737821088863
[batch_idx--300] train_loss: 0.0010785367531296595, acc: 0.8831758720930233, lr: 0.000968670951391451
[batch_idx--350] train_loss: 0.0010823388764716981, acc: 0.8825676638176638, lr: 0.0009527336449182884
[batch_idx--400] train_loss: 0.0010803516796253567, acc: 0.882724828553616, lr: 0.0009369259875942449
[batch_idx--450] train_loss: 0.0010818220187717895, acc: 0.8823983753948693, lr: 0.0009212480646451971
train_loss:  0.0010818220187717895  acc:  0.8823983753948693
->>lr:0.000921
test_loss:  0.0011028894968076622  test_acc:  0.8855937461223476
best acc:  88.5221491500186
Saving..

------Epoch: 137------
[batch_idx--0] train_loss: 0.0011650128290057182, acc: 0.88671875, lr: 0.0009209358300585474
[batch_idx--50] train_loss: 0.0010410985480263537, acc: 0.8891697303921569, lr: 0.0009053903232463079
[batch_idx--100] train_loss: 0.0010582690564039541, acc: 0.8864866955445545, lr: 0.0008899747208313852
[batch_idx--150] train_loss: 0.0010587562786088332, acc: 0.8864859271523179, lr: 0.0008746891059259105
[batch_idx--200] train_loss: 0.001064062064087287, acc: 0.8852223258706468, lr: 0.0008595335609412142
[batch_idx--250] train_loss: 0.0010673496459399265, acc: 0.8846489043824701, lr: 0.0008445081675873518
[batch_idx--300] train_loss: 0.001071136820546206, acc: 0.8843178986710963, lr: 0.0008296130068726787
[batch_idx--350] train_loss: 0.0010758337448351085, acc: 0.8837473290598291, lr: 0.0008148481591034013
[batch_idx--400] train_loss: 0.0010787896580292734, acc: 0.8834164588528678, lr: 0.0008002137038831625
[batch_idx--450] train_loss: 0.0010802882594546967, acc: 0.8837435345575728, lr: 0.000785709720112604
train_loss:  0.0010802882594546967  acc:  0.8837435345575728
->>lr:0.000786
test_loss:  0.0010967452020351667  test_acc:  0.8867105099888324
best acc:  88.55937461223476
Saving..

------Epoch: 138------
[batch_idx--0] train_loss: 0.0008552289800718427, acc: 0.8984375, lr: 0.0007854209717842232
[batch_idx--50] train_loss: 0.0011019451799345952, acc: 0.8772977941176471, lr: 0.0007710501494462701
[batch_idx--100] train_loss: 0.0010809063626739132, acc: 0.8805306311881188, lr: 0.0007568099557912517
[batch_idx--150] train_loss: 0.0010760658580982528, acc: 0.8820622930463576, lr: 0.000742700467594179
[batch_idx--200] train_loss: 0.0010786527991920377, acc: 0.8819768345771144, lr: 0.0007287217609253605
[batch_idx--250] train_loss: 0.00107678547881885, acc: 0.8822833665338645, lr: 0.0007148739111500131
[batch_idx--300] train_loss: 0.001085508570687666, acc: 0.8814887873754153, lr: 0.0007011569929278511
[batch_idx--350] train_loss: 0.0010845118535263332, acc: 0.8814992877492878, lr: 0.0006875710802126706
[batch_idx--400] train_loss: 0.001083711977407076, acc: 0.8818481140897756, lr: 0.0006741162462519713
[batch_idx--450] train_loss: 0.0010854456872839422, acc: 0.8819384177456868, lr: 0.0006607925635865458
train_loss:  0.0010854456872839422  acc:  0.8819384177456868
->>lr:0.000661
test_loss:  0.0010998953193452018  test_acc:  0.8853455763742399
best acc:  88.67105099888323

------Epoch: 139------
[batch_idx--0] train_loss: 0.0012493375688791275, acc: 0.86328125, lr: 0.0006605274281709927
[batch_idx--50] train_loss: 0.0010834479574388003, acc: 0.8821997549019608, lr: 0.0006473375938249676
[batch_idx--100] train_loss: 0.0010717880743841576, acc: 0.8828898514851485, lr: 0.0006342790551494393
[batch_idx--150] train_loss: 0.0010639727821061696, acc: 0.8839248758278145, lr: 0.0006213518825486042
[batch_idx--200] train_loss: 0.0010656727302995562, acc: 0.8837453358208955, lr: 0.0006085561457184031
[batch_idx--250] train_loss: 0.0010671111967496248, acc: 0.8833571962151394, lr: 0.0005958919136461599
[batch_idx--300] train_loss: 0.0010726287242789378, acc: 0.8828514327242525, lr: 0.000583359254610194
[batch_idx--350] train_loss: 0.0010709409367165759, acc: 0.8826678240740741, lr: 0.0005709582361794502
[batch_idx--400] train_loss: 0.0010707918303424116, acc: 0.8827735349127181, lr: 0.0005586889252131494
[batch_idx--450] train_loss: 0.0010721932108779988, acc: 0.8830752941993265, lr: 0.0005465513878604278
train_loss:  0.0010721932108779988  acc:  0.8830752941993265
->>lr:0.000547
test_loss:  0.0010995121381761773  test_acc:  0.8852214915001861
best acc:  88.67105099888323

------Epoch: 140------
[batch_idx--0] train_loss: 0.000936618132982403, acc: 0.91015625, lr: 0.0005463099816548578
[batch_idx--50] train_loss: 0.0010808642380688265, acc: 0.8822763480392157, lr: 0.0005343069207980278
[batch_idx--100] train_loss: 0.0010809910882467238, acc: 0.8816522277227723, lr: 0.0005224357650086414
[batch_idx--150] train_loss: 0.0010817070851651396, acc: 0.8814931705298014, lr: 0.0005106965782891965
[batch_idx--200] train_loss: 0.0010827260499644041, acc: 0.8820740049751243, lr: 0.0004990894239306987
[batch_idx--250] train_loss: 0.0010786494428006211, acc: 0.8821277390438247, lr: 0.0004876143645123038
[batch_idx--300] train_loss: 0.0010777139108770362, acc: 0.8821376661129569, lr: 0.00047627146190098725
[batch_idx--350] train_loss: 0.0010789423974355593, acc: 0.8819555733618234, lr: 0.00046506077725121944
[batch_idx--400] train_loss: 0.0010784002981308913, acc: 0.8820331982543641, lr: 0.00045398237100461594
[batch_idx--450] train_loss: 0.0010812294700130246, acc: 0.8818863470684208, lr: 0.0004430363028896239
train_loss:  0.0010812294700130246  acc:  0.8818863470684208
->>lr:0.000443
test_loss:  0.0010975115218460152  test_acc:  0.8858419158704554
best acc:  88.67105099888323

------Epoch: 141------
[batch_idx--0] train_loss: 0.0009357110830023885, acc: 0.8984375, lr: 0.00044281873178278475
[batch_idx--50] train_loss: 0.0010770314176767773, acc: 0.8842677696078431, lr: 0.0004320077093543484
[batch_idx--100] train_loss: 0.001086538900174016, acc: 0.8829285272277227, lr: 0.0004213291435323757
[batch_idx--150] train_loss: 0.0010795182161440222, acc: 0.8838213990066225, lr: 0.0004107830918895994
[batch_idx--200] train_loss: 0.0010729550360238634, acc: 0.8838425062189055, lr: 0.00040036961128431806
[batch_idx--250] train_loss: 0.0010725284622336527, acc: 0.8839797061752988, lr: 0.00039008875786008537
[batch_idx--300] train_loss: 0.0010751664444751592, acc: 0.8836430647840532, lr: 0.00037994058704539927
[batch_idx--350] train_loss: 0.001076425927355514, acc: 0.8834134615384616, lr: 0.0003699251535534104
[batch_idx--400] train_loss: 0.001073443796775975, acc: 0.8835918017456359, lr: 0.0003600425113816253
[batch_idx--450] train_loss: 0.001079521351669284, acc: 0.8830579373069045, lr: 0.0003502927138116147
train_loss:  0.001079521351669284  acc:  0.8830579373069045
->>lr:0.000350
test_loss:  0.0010991721184174703  test_acc:  0.8860900856185631
best acc:  88.67105099888323

------Epoch: 142------
[batch_idx--0] train_loss: 0.0011144123272970319, acc: 0.87109375, lr: 0.00035009907323737546
[batch_idx--50] train_loss: 0.0010948336270053451, acc: 0.8790594362745098, lr: 0.00034048483130907914
[batch_idx--100] train_loss: 0.0011042012962153053, acc: 0.8785581683168316, lr: 0.00033100353942642217
[batch_idx--150] train_loss: 0.0011071600721507574, acc: 0.8792684188741722, lr: 0.0003216552487071145
[batch_idx--200] train_loss: 0.0010979595291196595, acc: 0.8798779539800995, lr: 0.00031244000955181765
[batch_idx--250] train_loss: 0.001093507765938873, acc: 0.8805714641434262, lr: 0.00030335787164384453
[batch_idx--300] train_loss: 0.0010897633374012709, acc: 0.8813719892026578, lr: 0.00029440888394890454
[batch_idx--350] train_loss: 0.0010881421248762845, acc: 0.8811988069800569, lr: 0.00028559309471483943
[batch_idx--400] train_loss: 0.0010816641343511324, acc: 0.8819650093516209, lr: 0.00027691055147135437
[batch_idx--450] train_loss: 0.001086389772343355, acc: 0.8818689901759988, lr: 0.0002683613010297709
train_loss:  0.001086389772343355  acc:  0.8818689901759988
->>lr:0.000268
test_loss:  0.001098399471298816  test_acc:  0.8859660007445093
best acc:  88.67105099888323

------Epoch: 143------
[batch_idx--0] train_loss: 0.0011467168806120753, acc: 0.8828125, lr: 0.0002681916759252945
[batch_idx--50] train_loss: 0.001082113988505786, acc: 0.8808210784313726, lr: 0.00025977843162139916
[batch_idx--100] train_loss: 0.0010650761939566768, acc: 0.8850170173267327, lr: 0.0002514985724860308
[batch_idx--150] train_loss: 0.0010735628893598914, acc: 0.8840024834437086, lr: 0.00024335214315946986
[batch_idx--200] train_loss: 0.0010707920480320294, acc: 0.8840757151741293, lr: 0.00023533918756261376
[batch_idx--250] train_loss: 0.0010696238983307494, acc: 0.8840730826693227, lr: 0.00022745974889675503
[batch_idx--300] train_loss: 0.0010719528395459798, acc: 0.8838247508305648, lr: 0.0002197138696433315
[batch_idx--350] train_loss: 0.001073945121656562, acc: 0.8832020121082621, lr: 0.0002121015915637098
[batch_idx--400] train_loss: 0.0010776842682752434, acc: 0.8825981920199502, lr: 0.0002046229556989493
[batch_idx--450] train_loss: 0.0010798055060849525, acc: 0.8825893012115111, lr: 0.00019727800236959416
train_loss:  0.0010798055060849525  acc:  0.8825893012115111
->>lr:0.000197
test_loss:  0.001096506580065107  test_acc:  0.8850974066261322
best acc:  88.67105099888323

------Epoch: 144------
[batch_idx--0] train_loss: 0.000981133314780891, acc: 0.890625, lr: 0.0001971324671380531
[batch_idx--50] train_loss: 0.0010752280233610495, acc: 0.8861060049019608, lr: 0.00018992391078557493
[batch_idx--100] train_loss: 0.0010807031147772133, acc: 0.8832379331683168, lr: 0.00018284911621737067
[batch_idx--150] train_loss: 0.0010775914107332167, acc: 0.8836661837748344, lr: 0.00017590812157669555
[batch_idx--200] train_loss: 0.0010767263682925174, acc: 0.8838230721393034, lr: 0.00016910096428543743
[batch_idx--250] train_loss: 0.0010764528746787176, acc: 0.883761827689243, lr: 0.00016242768104390848
[batch_idx--300] train_loss: 0.0010754687213060444, acc: 0.8836690199335548, lr: 0.00015588830783064834
[batch_idx--350] train_loss: 0.0010767157268816569, acc: 0.8834245904558404, lr: 0.0001494828799022241
[batch_idx--400] train_loss: 0.0010759902200917074, acc: 0.8831437032418953, lr: 0.00014321143179305552
[batch_idx--450] train_loss: 0.0010763097502847015, acc: 0.88354393029472, lr: 0.00013707399731520964
train_loss:  0.0010763097502847015  acc:  0.88354393029472
->>lr:0.000137
test_loss:  0.0010949924878497798  test_acc:  0.8867105099888324
best acc:  88.67105099888323

------Epoch: 145------
[batch_idx--0] train_loss: 0.000889859686139971, acc: 0.8984375, lr: 0.00013695261579316776
[batch_idx--50] train_loss: 0.0010580367764786763, acc: 0.8845741421568627, lr: 0.0001309519093031647
[batch_idx--100] train_loss: 0.001074290739275552, acc: 0.8810720915841584, lr: 0.0001250852825408405
[batch_idx--150] train_loss: 0.0010731270500545125, acc: 0.8823727235099338, lr: 0.0001193527671357053
[batch_idx--200] train_loss: 0.0010732173950374312, acc: 0.8824821206467661, lr: 0.00011375439399421428
[batch_idx--250] train_loss: 0.0010699584409483223, acc: 0.8832793824701195, lr: 0.00010829019329959833
[batch_idx--300] train_loss: 0.001071625434409864, acc: 0.8829163205980066, lr: 0.00010296019451171701
[batch_idx--350] train_loss: 0.0010708316646853297, acc: 0.8832910434472935, lr: 9.776442636688366e-05
[batch_idx--400] train_loss: 0.001074311620453628, acc: 0.8829196539900249, lr: 9.270291687771548e-05
[batch_idx--450] train_loss: 0.0010782658449107192, acc: 0.882641371888777, lr: 8.77756933329893e-05
train_loss:  0.0010782658449107192  acc:  0.882641371888777
->>lr:0.000088
test_loss:  0.0010977187994204232  test_acc:  0.8853455763742399
best acc:  88.67105099888323

------Epoch: 146------
[batch_idx--0] train_loss: 0.0011339223710820079, acc: 0.859375, lr: 8.767851876239075e-05
[batch_idx--50] train_loss: 0.0010737810162918678, acc: 0.8826593137254902, lr: 8.288829424305533e-05
[batch_idx--100] train_loss: 0.0010705737052439243, acc: 0.8837793935643564, lr: 7.82324085830094e-05
[batch_idx--150] train_loss: 0.0010748130939314943, acc: 0.8829677152317881, lr: 7.371088688413186e-05
[batch_idx--200] train_loss: 0.0010728496306376596, acc: 0.8826764614427861, lr: 6.93237535238922e-05
[batch_idx--250] train_loss: 0.001075106654622299, acc: 0.8822522410358565, lr: 6.507103215520893e-05
[batch_idx--300] train_loss: 0.0010757314902188819, acc: 0.8819040697674418, lr: 6.09527457063358e-05
[batch_idx--350] train_loss: 0.0010696432547925034, acc: 0.8830016915954416, lr: 5.6968916380720215e-05
[batch_idx--400] train_loss: 0.0010671269253391, acc: 0.883396976309227, lr: 5.311956565690057e-05
[batch_idx--450] train_loss: 0.001073218232773566, acc: 0.8830839726455375, lr: 4.9404714288381335e-05
train_loss:  0.001073218232773566  acc:  0.8830839726455375
->>lr:0.000049
test_loss:  0.0010985653144370416  test_acc:  0.8850974066261322
best acc:  88.67105099888323

------Epoch: 147------
[batch_idx--0] train_loss: 0.0007492130971513689, acc: 0.921875, lr: 4.933178929321103e-05
[batch_idx--50] train_loss: 0.0010799395249170415, acc: 0.8834252450980392, lr: 4.57541478953355e-05
[batch_idx--100] train_loss: 0.0010737904190722077, acc: 0.884475556930693, lr: 4.231104556289278e-05
[batch_idx--150] train_loss: 0.0010701764701859446, acc: 0.8845974751655629, lr: 3.9002500859128264e-05
[batch_idx--200] train_loss: 0.0010742351250023355, acc: 0.8838425062189055, lr: 3.582853162182598e-05
[batch_idx--250] train_loss: 0.0010744398093385347, acc: 0.8833416334661355, lr: 3.2789154963222546e-05
[batch_idx--300] train_loss: 0.0010728423812030583, acc: 0.8829552533222591, lr: 2.9884387269901725e-05
[batch_idx--350] train_loss: 0.0010760283793131702, acc: 0.8826789529914529, lr: 2.7114244202716688e-05
[batch_idx--400] train_loss: 0.0010686194037314866, acc: 0.8834749064837906, lr: 2.4478740696695647e-05
[batch_idx--450] train_loss: 0.001071367159857738, acc: 0.883509216509876, lr: 2.1977890960975244e-05
train_loss:  0.001071367159857738  acc:  0.883509216509876
->>lr:0.000022
test_loss:  0.0010986032209646052  test_acc:  0.8854696612482938
best acc:  88.67105099888323

------Epoch: 148------
[batch_idx--0] train_loss: 0.0008881075773388147, acc: 0.90234375, lr: 2.192924752854042e-05
[batch_idx--50] train_loss: 0.0010638631053962837, acc: 0.8828125, lr: 1.9565758523884538e-05
[batch_idx--100] train_loss: 0.001061491448648901, acc: 0.8835086633663366, lr: 1.7336949777524425e-05
[batch_idx--150] train_loss: 0.0010589368291359568, acc: 0.8841576986754967, lr: 1.524283330592846e-05
[batch_idx--200] train_loss: 0.0010667087215425178, acc: 0.8831040111940298, lr: 1.3283420399376467e-05
[batch_idx--250] train_loss: 0.0010656561914449223, acc: 0.8830303784860558, lr: 1.14587216219042e-05
[batch_idx--300] train_loss: 0.001066336847425584, acc: 0.8835262666112956, lr: 9.768746811253394e-06
[batch_idx--350] train_loss: 0.0010662834996495534, acc: 0.8836471688034188, lr: 8.213505078799587e-06
[batch_idx--400] train_loss: 0.0010707962184162007, acc: 0.8829488778054863, lr: 6.793004809518832e-06
[batch_idx--450] train_loss: 0.0010746014561965407, acc: 0.882624014996355, lr: 5.507253661940492e-06
train_loss:  0.0010746014561965407  acc:  0.882624014996355
->>lr:0.000006
test_loss:  0.0010973670643425295  test_acc:  0.8847251520039707
best acc:  88.67105099888323

------Epoch: 149------
[batch_idx--0] train_loss: 0.000951683905441314, acc: 0.90234375, lr: 5.482913128862511e-06
[batch_idx--50] train_loss: 0.0010829452692311915, acc: 0.8818167892156863, lr: 4.334613221818051e-06
[batch_idx--100] train_loss: 0.001085052010257461, acc: 0.8812654702970297, lr: 3.321075690718156e-06
[batch_idx--150] train_loss: 0.0010726993497991502, acc: 0.8826314155629139, lr: 2.4423059999861833e-06
[batch_idx--200] train_loss: 0.0010763229558994965, acc: 0.8817047574626866, lr: 1.6983088874406029e-06
[batch_idx--250] train_loss: 0.001080853199220793, acc: 0.8817697958167331, lr: 1.089088364294999e-06
[batch_idx--300] train_loss: 0.0010772675573222487, acc: 0.8824750830564784, lr: 6.146477151164343e-07
[batch_idx--350] train_loss: 0.001077877000230298, acc: 0.8826789529914529, lr: 2.749894978198997e-07
[batch_idx--400] train_loss: 0.0010795049757483018, acc: 0.8822572475062345, lr: 7.011554364610984e-08
[batch_idx--450] train_loss: 0.0010824511475871676, acc: 0.8822508418092825, lr: 2.6957161503027296e-11
train_loss:  0.0010824511475871676  acc:  0.8822508418092825
->>lr:0.000000
test_loss:  0.0010979920699646993  test_acc:  0.8852214915001861
best acc:  88.67105099888323